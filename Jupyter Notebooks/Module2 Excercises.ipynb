{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb696c68-5487-46df-82c5-4e8db0dfb9a3",
   "metadata": {},
   "source": [
    "# A: Where Data Comes From"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8e759-0b72-4241-822b-ec46203fe341",
   "metadata": {},
   "source": [
    "This section will cover the main places data comes from, what assumptions each source bakes in, and how to do a quick *source audit* before cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5ca67-4ce9-4595-9cd7-682db17e8930",
   "metadata": {},
   "source": [
    "## A.1 Files: CSV, JSON, Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd08049-78c9-4d9b-ac5a-552a20e6c864",
   "metadata": {},
   "source": [
    "Flat files are common, portable, and simple\n",
    "\n",
    "**What these formats are good at:**\n",
    "<table style=\"text-align=left\";>\n",
    "    <tr>\n",
    "        <th>Format</th>\n",
    "        <th>Best suited for</th>\n",
    "        <th>Common misuse</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>CSV</th>\n",
    "        <th>Simple rectangular tables; system exports; quick sharing</th>\n",
    "        <th>Encoding complex structure or multiple tables in one file</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Excel</th>\n",
    "        <th>Human-facing analysis, manual edits, reporting</th>\n",
    "        <th>Using as a source of truth or automated data pipeline input</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>JSON</th>\n",
    "        <th>Nested or semi-structured records; API responses</th>\n",
    "        <th>Assuming fields are stable or consistently present</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90a741-d994-449a-a0ef-e0b5b7a14286",
   "metadata": {},
   "source": [
    "### Hidden assumptions\n",
    "* Someone chose column names, encodings, delimiters, and header rows\n",
    "* Missing values may be implicit(\"\", NA, -1).\n",
    "* Types are inferred later, not enforced at creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c2c84-ceff-4192-8772-ff899b8f2c2b",
   "metadata": {},
   "source": [
    "### Microlab: File sanity check\n",
    "\n",
    "Simulate loading messy exports and practice the first questions that should be asked:\n",
    "* What are the columns?\n",
    "* What looks like missing data?\n",
    "* What types are being inferred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6608396c-9659-4554-8afc-6e2c50f6b713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CSV export (note blanks, NA, weird spacing) ===\n",
      "   user_id   age   income              city\n",
      "0      101  34.0  72000.0            Denver\n",
      "1      102   NaN      NaN           Boulder\n",
      "2      103  29.0  65000.0            Denver\n",
      "3      104  -1.0   5400.0  Colorado Springs\n",
      "\n",
      "Dtypes: {'user_id': dtype('int64'), 'age': dtype('float64'), 'income': dtype('float64'), 'city': dtype('O')}\n",
      "Missing by column: {'user_id': 0, 'age': 1, 'income': 1, 'city': 0}\n",
      "\n",
      "=== JSON export (nested fields) ===\n",
      "   user_id income  profile.age profile.city\n",
      "0      201  80000         31.0       Denver\n",
      "1      202  79000          NaN      Boulder\n",
      "2      203  61000          NaN       Denver\n",
      "\n",
      "Dtypes: {'user_id': dtype('int64'), 'income': dtype('O'), 'profile.age': dtype('float64'), 'profile.city': dtype('O')}\n",
      "\n",
      "Try:\n",
      "- Replace -1 with a real age and re-check missingness\n",
      "- Make income consistently numeric\n",
      "- Rename columns to something consistent (snake_case)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import json\n",
    "\n",
    "print(\"=== CSV export (note blanks, NA, weird spacing) ===\")\n",
    "csv_text = \"\"\"\n",
    "user_id,age,income,city\n",
    "101,34,72000,Denver\n",
    "102,,NA, Boulder\n",
    "103,29,65000,Denver\n",
    "104, -1,5400, \"Colorado Springs\"\n",
    "\"\"\"\n",
    "df_csv = pd.read_csv(StringIO(csv_text),\n",
    "                     skipinitialspace=True)\n",
    "print(df_csv)\n",
    "print(\"\\nDtypes:\", df_csv.dtypes.to_dict())\n",
    "print(\"Missing by column:\", df_csv.isna().sum().to_dict())\n",
    "\n",
    "print(\"\\n=== JSON export (nested fields) ===\")\n",
    "json_text = json.dumps([\n",
    "      {\"user_id\": 201, \"profile\": {\"age\": 31, \"city\": \"Denver\"}, \"income\": 80000},\n",
    "      {\"user_id\": 202, \"profile\": {\"age\": None, \"city\": \"Boulder\"}, \"income\": \"79000\"},\n",
    "      {\"user_id\": 203, \"profile\": {\"city\": \"Denver\"}, \"income\": 61000}\n",
    "])\n",
    "\n",
    "records = json.loads(json_text)\n",
    "df_json = pd.json_normalize(records)\n",
    "print(df_json)\n",
    "print(\"\\nDtypes:\", df_json.dtypes.to_dict())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Replace -1 with a real age and re-check missingness\")\n",
    "print(\"- Make income consistently numeric\")\n",
    "print(\"- Rename columns to something consistent (snake_case)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd55c4-9b9d-4eca-ba27-f62302e68d2a",
   "metadata": {},
   "source": [
    "#### Example with cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be4648a-1491-40eb-9b19-5eebfce68ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CSV export (note blanks, NA, weird spacing) ===\n",
      "   user_id  age  income              city\n",
      "0      101   34   72000            Denver\n",
      "1      102   27   99000           Boulder\n",
      "2      103   29   65000            Denver\n",
      "3      104   44    5400  Colorado Springs\n",
      "\n",
      "Dtypes: {'user_id': dtype('int64'), 'age': dtype('int64'), 'income': dtype('int64'), 'city': dtype('O')}\n",
      "Missing by column: {'user_id': 0, 'age': 0, 'income': 0, 'city': 0}\n",
      "\n",
      "=== JSON export (nested fields) ===\n",
      "   user_id  income  profile.age profile.city\n",
      "0      201   80000           31       Denver\n",
      "1      202   79000           27      Boulder\n",
      "2      203   61000           44       Denver\n",
      "\n",
      "Dtypes: {'user_id': dtype('int64'), 'income': dtype('int64'), 'profile.age': dtype('int64'), 'profile.city': dtype('O')}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import json\n",
    "\n",
    "print(\"=== CSV export (note blanks, NA, weird spacing) ===\")\n",
    "csv_text = \"\"\"\n",
    "user_id,age,income,city\n",
    "101,34,72000,Denver\n",
    "102,27,99000,Boulder\n",
    "103,29,65000,Denver\n",
    "104,44,5400,Colorado Springs\n",
    "\"\"\"\n",
    "df_csv = pd.read_csv(StringIO(csv_text),\n",
    "                     skipinitialspace=True)\n",
    "print(df_csv)\n",
    "print(\"\\nDtypes:\", df_csv.dtypes.to_dict())\n",
    "print(\"Missing by column:\", df_csv.isna().sum().to_dict())\n",
    "\n",
    "print(\"\\n=== JSON export (nested fields) ===\")\n",
    "json_text = json.dumps([\n",
    "      {\"user_id\": 201, \"profile\": {\"age\": 31, \"city\": \"Denver\"}, \"income\": 80000},\n",
    "      {\"user_id\": 202, \"profile\": {\"age\": 27, \"city\": \"Boulder\"}, \"income\": 79000},\n",
    "      {\"user_id\": 203, \"profile\": {\"age\": 44, \"city\": \"Denver\"}, \"income\": 61000}\n",
    "])\n",
    "\n",
    "records = json.loads(json_text)\n",
    "df_json = pd.json_normalize(records)\n",
    "print(df_json)\n",
    "print(\"\\nDtypes:\", df_json.dtypes.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046c3c1-abaf-4668-a633-cbe5c7088016",
   "metadata": {},
   "source": [
    "## A.2 SQL databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb2683-69cb-47c6-94a1-741c5411b15a",
   "metadata": {},
   "source": [
    "Databases reflect how applications think about the world.  \n",
    "\n",
    "**Why databases exist**  \n",
    "\n",
    "Databases are optimized for transactions, consistency, and multi-user access, not analysis. Their schemas encode business logic: users, orders, events, states.  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <tr>\n",
    "        <th>Type</th>\n",
    "        <th>What a row represents</th>\n",
    "        <th>Typical pitfall</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Events</th>\n",
    "        <th>Something that happened at a time</th>\n",
    "        <th>Accidentally double counting or missing time windows</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>State</th>\n",
    "        <th>Current snapshot of something</th>\n",
    "        <th>Assuming it contains historical truth</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed7b1f-9d0a-4325-bfc9-890ce276f3aa",
   "metadata": {},
   "source": [
    "### Microlab: Join logic and granularity traps\n",
    "\n",
    "This microlab creates two tables (users and orders) and shows how a join can silently change your \"row meaning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6ac734-3d32-4568-b929-d3cede53f537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== users (state table) ===\n",
      "(1, 'free')\n",
      "(2, 'paid')\n",
      "(3, 'paid')\n",
      "\n",
      "=== orders (event table) ====\n",
      "(101, 2, 20.0)\n",
      "(102, 2, 35.0)\n",
      "(103, 3, 15.0)\n",
      "(104, 3, 60.0)\n",
      "\n",
      "=== joined view ===\n",
      "(1, 'free', None, None)\n",
      "(2, 'paid', 101, 20.0)\n",
      "(2, 'paid', 102, 35.0)\n",
      "(3, 'paid', 103, 15.0)\n",
      "(3, 'paid', 104, 60.0)\n",
      "\n",
      "Question: How many paid users are there?\n",
      "Correct (from users table): 2\n",
      "WRONG (after join, counting rows): 4\n",
      "\n",
      "Fix: define unite of analysis explicitly.\n",
      "Paid users (DISTINCT user_id): 2\n",
      "\n",
      "Try:\n",
      "- Insert another order for user 2\n",
      "- Compute total revenue by plan using GROUP BY\n",
      "- Ask: what does ONE ROW represent after each query?\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE users (\n",
    "    user_id INTEGER PRIMARY KEY,\n",
    "    plan TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE orders (\n",
    "    order_id INTEGER PRIMARY KEY,\n",
    "    user_id INTEGER,\n",
    "    amount REAL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert data\n",
    "cur.executemany(\n",
    "    \"INSERT INTO users (user_id, plan) VALUES (?, ?)\",\n",
    "    [(1, \"free\"), (2, \"paid\"), (3, \"paid\")]\n",
    ")\n",
    "\n",
    "cur.executemany(\n",
    "    \"INSERT INTO orders (order_id, user_id, amount) VALUES (?, ?, ?)\",\n",
    "    [(101, 2, 20.0), (102, 2, 35.0), (103, 3, 15.0), (104, 3, 60.0)]\n",
    ")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "print(\"=== users (state table) ===\")\n",
    "for row in cur.execute(\"SELECT * FROM users\"):\n",
    "    print(row)\n",
    "\n",
    "print(\"\\n=== orders (event table) ====\")\n",
    "for row in cur.execute(\"SELECT * FROM orders\"):\n",
    "    print(row)\n",
    "\n",
    "print(\"\\n=== joined view ===\")\n",
    "for row in cur.execute(\"\"\"\n",
    "SELECT u.user_id, u.plan, o.order_id, o.amount\n",
    "FROM users u\n",
    "LEFT JOIN orders o\n",
    "ON u.user_id = o.user_id\n",
    "\"\"\"):\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nQuestion: How many paid users are there?\")\n",
    "\n",
    "# Correct answer\n",
    "cur.execute(\"SELECT COUNT(*) FROM users WHERE plan = 'paid'\")\n",
    "print(\"Correct (from users table):\", cur.fetchone()[0])\n",
    "\n",
    "# WRONG answer: counting rows after join\n",
    "cur.execute(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM users u\n",
    "LEFT JOIN orders o\n",
    "ON u.user_id = o.user_id\n",
    "WHERE u.plan = 'paid'\n",
    "\"\"\")\n",
    "print(\"WRONG (after join, counting rows):\", cur.fetchone()[0])\n",
    "\n",
    "print(\"\\nFix: define unite of analysis explicitly.\")\n",
    "\n",
    "#Correct fix using DISTINCT\n",
    "cur.execute(\"\"\"\n",
    "SELECT COUNT(DISTINCT u.user_id)\n",
    "FROM users u\n",
    "LEFT JOIN orders o\n",
    "ON u.user_id = o.user_id\n",
    "WHERE u.plan = 'paid'\n",
    "\"\"\")\n",
    "print(\"Paid users (DISTINCT user_id):\", cur.fetchone()[0])\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Insert another order for user 2\")\n",
    "print(\"- Compute total revenue by plan using GROUP BY\")\n",
    "print(\"- Ask: what does ONE ROW represent after each query?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba7c4f-97c1-4444-97ec-a2bcaf7967ea",
   "metadata": {},
   "source": [
    "## A.3 APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3c8de-d278-4175-a078-1206fac37b32",
   "metadata": {},
   "source": [
    "APIs give you data through someone else's interface and rules  \n",
    "\n",
    "**What APIs provide**  \n",
    "\n",
    "* Programmatic access to live or regularly updated data.\n",
    "* Structured responses (often JSON).\n",
    "* Authentication, quotas, and versioning\n",
    "\n",
    "**Common constraints**\n",
    "* Rate limits: you cannot pull everything at once\n",
    "* Partial views: pagination, filters, or redacted fields\n",
    "* Instability: fields can change or disappear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb0da0-39fb-4436-9dd5-0e65873bf876",
   "metadata": {},
   "source": [
    "### Microlab: Pagination + schema drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfa7669-1f42-47c8-8df1-8eb87279d378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id name score country\n",
      "0   1  Ava  0.91     NaN\n",
      "1   2  Ben  0.74     NaN\n",
      "2   3  Cam  0.88      US\n",
      "3   4  Dee  0.67    None\n",
      "\n",
      "Dtypes: {'id': dtype('int64'), 'name': dtype('O'), 'score': dtype('O'), 'country': dtype('O')}\n",
      "\n",
      "Common API tasks:\n",
      "1) Combine pages (done)\n",
      "2) Normalize fields (types + names)\n",
      "3) Decide how to handle missing optional fields\n",
      "\n",
      "After forcing score numeric:\n",
      "   id name  score country\n",
      "0   1  Ava   0.91     NaN\n",
      "1   2  Ben   0.74     NaN\n",
      "2   3  Cam   0.88      US\n",
      "3   4  Dee   0.67    None\n",
      "\n",
      "Dtypes: {'id': dtype('int64'), 'name': dtype('O'), 'score': dtype('float64'), 'country': dtype('O')}\n",
      "\n",
      "Try:\n",
      "- Add a page3 missing 'name'\n",
      "- Rename 'id' to 'user_id'\n",
      "- Drop rows with missing critical fields vs keep and flag\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simulated API responses (page 1 vs page 2)\n",
    "page1 = [\n",
    "    {\"id\": 1, \"name\": \"Ava\", \"score\": 0.91},\n",
    "    {\"id\": 2, \"name\": \"Ben\", \"score\": 0.74},\n",
    "]\n",
    "\n",
    "page2 = [\n",
    "    {\"id\": 3, \"name\": \"Cam\", \"score\": \"0.88\", \"country\": \"US\"},\n",
    "    {\"id\": 4, \"name\": \"Dee\", \"score\": 0.67, \"country\": None},\n",
    "]\n",
    "\n",
    "df = pd.json_normalize(page1 + page2)\n",
    "\n",
    "print(df)\n",
    "print(\"\\nDtypes:\", df.dtypes.to_dict())\n",
    "\n",
    "print(\"\\nCommon API tasks:\")\n",
    "print(\"1) Combine pages (done)\")\n",
    "print(\"2) Normalize fields (types + names)\")\n",
    "print(\"3) Decide how to handle missing optional fields\")\n",
    "\n",
    "# Example: force score numeric\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "print(\"\\nAfter forcing score numeric:\")\n",
    "print(df)\n",
    "print(\"\\nDtypes:\", df.dtypes.to_dict())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Add a page3 missing 'name'\")\n",
    "print(\"- Rename 'id' to 'user_id'\")\n",
    "print(\"- Drop rows with missing critical fields vs keep and flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993b141-710c-4e61-8fed-9c6d193465a7",
   "metadata": {},
   "source": [
    "## A.4 Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47289689-47cb-4111-8f15-0333c652963f",
   "metadata": {},
   "source": [
    "Scraping is extracting structure from pages built for humans.  \n",
    "\n",
    "**Why scraping exists**  \n",
    "\n",
    "Sometimes the data is visible but not downloadable. Scraping turns HTML pages into rows and columns.  \n",
    "\n",
    "**Why scraping is fragile**  \n",
    "* HTML structure changes without notice\n",
    "* Content may be dynamically loaded\n",
    "* Legal and ethical constraints apply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616baae1-8439-47d4-a8ef-ae56dc89b309",
   "metadata": {},
   "source": [
    "### Microlab: parse a tiny HTML snippet into a table\n",
    "\n",
    "This is a minimal example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff32694-ad0c-429c-8a23-1f9520506aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name price\n",
      "0  Widget A   $10\n",
      "1  Widget B   $12\n",
      "\n",
      "Try:\n",
      "- Change <td>$10</td> to <td>10 usd</td> and clean price\n",
      "- Add a new column in the HTML and update your parser\n",
      "- Remove the header row and see how the assumptions break\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<table>\n",
    "  <tr><th>Name</th><th>Price</th></tr>\n",
    "  <tr><td>Widget A</td><td>$10</td></tr>\n",
    "  <tr><td>Widget B</td><td>$12</td></tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "rows = []\n",
    "for tr in soup.find_all(\"tr\")[1:]:\n",
    "    tds = tr.find_all(\"td\")\n",
    "    rows.append({\n",
    "        \"name\": tds[0].get_text(strip=True),\n",
    "        \"price\": tds[1].get_text(strip=True),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Change <td>$10</td> to <td>10 usd</td> and clean price\")\n",
    "print(\"- Add a new column in the HTML and update your parser\")\n",
    "print(\"- Remove the header row and see how the assumptions break\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0bd2c1-dfa3-47df-88a1-f5ac59734af1",
   "metadata": {},
   "source": [
    "## A.5 Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412e436-1d44-4c16-9520-e4f7cccd4962",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "* **Files:** clean a messy CSV, normalize a nested JSON, handle Excel-style human edits\n",
    "* **SQL:** use SQLite to see how joins change row meaning and how to aggregate safely\n",
    "* **APIs:** combine paginated responses and handle schema drift (simulated)\n",
    "* **Scraping:** parse a tiny HTML table and clean extracted fields.\n",
    "* **MiniProject:** merge sources into one analysis-ready table + write a data quality report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e996c7-c46c-4264-ac5d-81b3f5e552fb",
   "metadata": {},
   "source": [
    "# B. Data Quality and Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed0d92-bae9-4d1b-8396-fdc3af9d6287",
   "metadata": {},
   "source": [
    "\"Messy data\" is not random. It has structure and causes. Quality problems usually come from how systems record events, how humans edit files, and how pipelines evolve over time.  \n",
    "\n",
    "This section focuses on auditing what the data means before it is trusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650e628-f15d-4e57-ae90-0c77bd1327db",
   "metadata": {},
   "source": [
    "## B.1 Missing Data: types and causes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ff162-651c-4073-a598-2cb7e8300f5e",
   "metadata": {},
   "source": [
    "**Three patterns of missingness**\n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <tr>\n",
    "        <th>Pattern</th>\n",
    "        <th>What is means (plain language)</th>\n",
    "        <th>Example</th>\n",
    "        <th>Why it matters</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MCAR (Missing Completely at Random)</th>\n",
    "        <th>Values are missing for reasons unrelated to the data itself (pure noise or chance)</th>\n",
    "        <th>A sensor randomly drops readings due to network glitches</th>\n",
    "        <th>Dropping or simple imputation usually does not bias results (but reduces sample size)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MAR (Missing At Random, conditional on observed data)</th>\n",
    "        <th>Missingness depends on other columns you can see, but not on the missing value itself once you account for them</th>\n",
    "        <th>Income is missing more often for younger users, but within each age group it is random.</th>\n",
    "        <th>Imputation using observed features can be reasonable, but assumptions must be documented.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MNAR (Missing Not At Random)</th>\n",
    "        <th>Missingness depends on the value that is missing, even after accounting for obserced data</th>\n",
    "        <th>People with very low income skip the income question because it is low</th>\n",
    "        <th>Naive dropping or imputation can bias conclusions; this is the hardest case to fix.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Missing is often encoded**\n",
    "* **Sentinels:** -1, 999, \"unknown\", \"N/A\"\n",
    "* **Empty strings:** \"\"\n",
    "* **Whitespace:** \" \"\n",
    "* **Type coercion:** numbers stored as strings, turning failures into nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebba490-c865-499a-8b11-cdea9ad8c58d",
   "metadata": {},
   "source": [
    "### Microlab: detect missingness + missingness as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7928dfd2-8745-41f7-9f01-eeb452397aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data:\n",
      "   user_id age income              city  plan\n",
      "0      101  34  72000            Denver  paid\n",
      "1      102         NA           Boulder  free\n",
      "2      103  29  65000                    paid\n",
      "3      104  -1  54000  Colorado Springs  free\n",
      "4      105  41   None            Denver  paid\n",
      "5      106        999           unknown  free\n",
      "\n",
      "After normalizing missing encodings:\n",
      "   user_id   age   income              city  plan\n",
      "0      101  34.0  72000.0            Denver  paid\n",
      "1      102   NaN      NaN           Boulder  free\n",
      "2      103  29.0  65000.0               NaN  paid\n",
      "3      104   NaN  54000.0  Colorado Springs  free\n",
      "4      105  41.0      NaN            Denver  paid\n",
      "5      106   NaN      NaN               NaN  free\n",
      "\n",
      "Missing counts: {'user_id': 0, 'age': 3, 'income': 3, 'city': 2, 'plan': 0}\n",
      "\n",
      "Income missing rate by plan:\n",
      "plan\n",
      "free    0.666667\n",
      "paid    0.333333\n",
      "Name: income_missing, dtype: float64\n",
      "\n",
      "Try:\n",
      "- Add more rows where only 'free' users hide income\n",
      "- Ask: does dropping missing-income rows change the plan mix?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23972/4204433707.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_clean = df.replace(missing_tokens, np.nan)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"user_id\": [101, 102, 103, 104, 105, 106],\n",
    "  \"age\": [34, \"\", 29, -1, 41, \"  \"],\n",
    "  \"income\": [\"72000\", \"NA\", \"65000\", \"54000\", None, \"999\"],\n",
    "  \"city\": [\"Denver\", \"Boulder\", \"\", \"Colorado Springs\", \"Denver\", \"unknown\"],\n",
    "  \"plan\": [\"paid\", \"free\", \"paid\", \"free\", \"paid\", \"free\"]\n",
    "})\n",
    "\n",
    "print(\"Raw data:\")\n",
    "print(df)\n",
    "\n",
    "# Step 1: standardize common missing encodings\n",
    "missing_tokens = [\"\", \"  \", \"NA\", \"N/A\", \"unknown\", \"999\"]\n",
    "df_clean = df.replace(missing_tokens, np.nan)\n",
    "\n",
    "# Step 2: coerce numeric columns\n",
    "df_clean[\"age\"] = pd.to_numeric(df_clean[\"age\"], errors=\"coerce\")\n",
    "df_clean.loc[df_clean[\"age\"] < 0, \"age\"] = np.nan\n",
    "df_clean[\"income\"] = pd.to_numeric(df_clean[\"income\"], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nAfter normalizing missing encodings:\")\n",
    "print(df_clean)\n",
    "print(\"\\nMissing counts:\", df_clean.isna().sum().to_dict())\n",
    "\n",
    "# Missingness can be informative:\n",
    "df_clean[\"income_missing\"] = df_clean[\"income\"].isna().astype(int)\n",
    "\n",
    "print(\"\\nIncome missing rate by plan:\")\n",
    "print(df_clean.groupby(\"plan\")[\"income_missing\"].mean())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Add more rows where only 'free' users hide income\")\n",
    "print(\"- Ask: does dropping missing-income rows change the plan mix?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47763dc2-4f18-487f-b9fd-ae1cc787fd3e",
   "metadata": {},
   "source": [
    "## B2. Duplicates and inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cde25c-5846-49a6-b619-dc4bf2f7fdaa",
   "metadata": {},
   "source": [
    "Duplication is usually a modeling bug waiting to happen: double-counting, label leakage, or identity confusion\n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <tr>\n",
    "        <th>Type</th>\n",
    "        <th>What it looks like</th>\n",
    "        <th>Common cause</th>\n",
    "        <th>What to do</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Exact duplicates</th>\n",
    "        <th>Same value across all columns</th>\n",
    "        <th>Export glitch, retry logic, copy/paste</th>\n",
    "        <th>Usually safe to drop</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Entity duplicates</th>\n",
    "        <th>Same real-world entity appears multiple times</th>\n",
    "        <th>Multiple IDs, casing\\typos, merges</th>\n",
    "        <th>Requires a dedupe rule</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Inconsistency is often \"almost the same\"**  \n",
    "\n",
    "* **Strings:** casing, whitespace, punctuation (\"NY\" vs \"New York\")\n",
    "* **Categories:** synonyms (\"M\", \"male\", \"man\")\n",
    "* **Units:** kg vs lb, dollars vs cents\n",
    "* **Timezones:** mixing UTC and local time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60eb38-e889-4a85-ab64-885fba54a9b5",
   "metadata": {},
   "source": [
    "### Microlab: Exact duplicates vs entity duplicates \n",
    "\n",
    "Remove exact duplicates and then face the harder case: multiple records that probably refer to the same customer. Practice dedupe rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b6eaad-1b19-4a49-b667-86909359b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:\n",
      "  customer_id            email      name updated_at       city\n",
      "0        C001  ava@example.com    Ava Li 2025-01-01     Denver\n",
      "1        C001  ava@example.com    Ava Li 2025-01-01     Denver\n",
      "2        C009  ava@example.com    Ava L. 2025-02-10     Denver\n",
      "3        C010  ben@example.com       Ben 2025-02-09   Boulder \n",
      "4        C011  ben@example.com  Benjamin 2025-02-12    Boulder\n",
      "\n",
      "1) Drop exact duplicates:\n",
      "  customer_id            email      name updated_at       city\n",
      "0        C001  ava@example.com    Ava Li 2025-01-01     Denver\n",
      "2        C009  ava@example.com    Ava L. 2025-02-10     Denver\n",
      "3        C010  ben@example.com       Ben 2025-02-09   Boulder \n",
      "4        C011  ben@example.com  Benjamin 2025-02-12    Boulder\n",
      "\n",
      "2) Normalize strings (city):\n",
      "  customer_id            email     city\n",
      "0        C001  ava@example.com   Denver\n",
      "2        C009  ava@example.com   Denver\n",
      "3        C010  ben@example.com  Boulder\n",
      "4        C011  ben@example.com  Boulder\n",
      "\n",
      "3) Entity dedupe rule: keep the most recent row per email\n",
      "  customer_id            email      name updated_at     city\n",
      "2        C009  ava@example.com    Ava L. 2025-02-10   Denver\n",
      "4        C011  ben@example.com  Benjamin 2025-02-12  Boulder\n",
      "\n",
      "Try:\n",
      "- Change the rule: keep row with longest name, or fewest missing fields\n",
      "- Ask: what is your unit of analysis — customer, email, or account?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23972/2530805920.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[\"city\"] = df1[\"city\"].str.strip()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "  {\"customer_id\": \"C001\", \"email\": \"ava@example.com\", \"name\": \"Ava Li\", \"updated_at\": \"2025-01-01\", \"city\": \"Denver\"},\n",
    "  {\"customer_id\": \"C001\", \"email\": \"ava@example.com\", \"name\": \"Ava Li\", \"updated_at\": \"2025-01-01\", \"city\": \"Denver\"},  # exact dup\n",
    "  {\"customer_id\": \"C009\", \"email\": \"ava@example.com\", \"name\": \"Ava L.\", \"updated_at\": \"2025-02-10\", \"city\": \"Denver\"},   # entity dup (same email)\n",
    "  {\"customer_id\": \"C010\", \"email\": \"ben@example.com\", \"name\": \"Ben\", \"updated_at\": \"2025-02-09\", \"city\": \" Boulder \"},    # whitespace\n",
    "  {\"customer_id\": \"C011\", \"email\": \"ben@example.com\", \"name\": \"Benjamin\", \"updated_at\": \"2025-02-12\", \"city\": \"Boulder\"}, # entity dup\n",
    "])\n",
    "\n",
    "df[\"updated_at\"] = pd.to_datetime(df[\"updated_at\"])\n",
    "print(\"Raw:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\n1) Drop exact duplicates:\")\n",
    "df1 = df.drop_duplicates()\n",
    "print(df1)\n",
    "\n",
    "print(\"\\n2) Normalize strings (city):\")\n",
    "df1[\"city\"] = df1[\"city\"].str.strip()\n",
    "print(df1[[\"customer_id\",\"email\",\"city\"]])\n",
    "\n",
    "print(\"\\n3) Entity dedupe rule: keep the most recent row per email\")\n",
    "df2 = df1.sort_values(\"updated_at\").drop_duplicates(subset=[\"email\"], keep=\"last\")\n",
    "print(df2)\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Change the rule: keep row with longest name, or fewest missing fields\")\n",
    "print(\"- Ask: what is your unit of analysis — customer, email, or account?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d5397-a05c-4119-90aa-782e33c19ae5",
   "metadata": {},
   "source": [
    "## B.3 Schema drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a13ae-bfdf-4a2c-888c-f949ca21be56",
   "metadata": {},
   "source": [
    "Over time, the meaning of columns change, sometimes quietly.  \n",
    "\n",
    "**What \"drift\" looks like in raw data**.\n",
    "* New columns appear (a new feature launches)\n",
    "* Columns disappear (a field is deprecated)\n",
    "* Type changes (integer becomes string; cents becomes dollars)\n",
    "* Semantic changes (same name, different meaning)\n",
    "\n",
    "**Why this matters**\n",
    "Drift can break pipelines, but worse: It can create silent correctness failures. Your model may keep training, on a different problem than intended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864010ff-c962-4fd5-9f53-114a07143d73",
   "metadata": {},
   "source": [
    "### Microlab: Compare \"yesterday vs today\" schemas\n",
    "\n",
    "Simulate two daily exports, detect differences, and choose a conservative strategy:\n",
    "* Align columns\n",
    "* Coerce types\n",
    "* Log drift for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15e1d15-af87-48ca-bdcd-8df3416c3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Day 1 ===\n",
      "   id  score country\n",
      "0   1   0.91      US\n",
      "1   2   0.74      US\n",
      "Dtypes: {'id': dtype('int64'), 'score': dtype('float64'), 'country': dtype('O')}\n",
      "\n",
      "=== Day 2 ===\n",
      "   id score country  tier\n",
      "0   3  0.88      CA   pro\n",
      "1   4  0.67    None  free\n",
      "Dtypes: {'id': dtype('int64'), 'score': dtype('O'), 'country': dtype('O'), 'tier': dtype('O')}\n",
      "\n",
      "Schema drift:\n",
      "Added columns: ['tier']\n",
      "Removed columns: []\n",
      "\n",
      "=== Combined (aligned + normalized) ===\n",
      "  country  id  score  tier\n",
      "0      US   1   0.91   NaN\n",
      "1      US   2   0.74   NaN\n",
      "2      CA   3   0.88   pro\n",
      "3    None   4   0.67  free\n",
      "Dtypes: {'country': dtype('O'), 'id': dtype('int64'), 'score': dtype('float64'), 'tier': dtype('O')}\n",
      "\n",
      "Try:\n",
      "- Rename 'score' to 'risk_score' on day2 and detect semantic drift\n",
      "- Decide: should missing 'tier' on day1 be 'unknown' or null?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "day1 = pd.DataFrame([\n",
    "  {\"id\": 1, \"score\": 0.91, \"country\": \"US\"},\n",
    "  {\"id\": 2, \"score\": 0.74, \"country\": \"US\"},\n",
    "])\n",
    "\n",
    "day2 = pd.DataFrame([\n",
    "  {\"id\": 3, \"score\": \"0.88\", \"country\": \"CA\", \"tier\": \"pro\"},  # score becomes string; new column tier\n",
    "  {\"id\": 4, \"score\": \"0.67\", \"country\": None, \"tier\": \"free\"},\n",
    "])\n",
    "\n",
    "print(\"=== Day 1 ===\")\n",
    "print(day1)\n",
    "print(\"Dtypes:\", day1.dtypes.to_dict())\n",
    "\n",
    "print(\"\\n=== Day 2 ===\")\n",
    "print(day2)\n",
    "print(\"Dtypes:\", day2.dtypes.to_dict())\n",
    "\n",
    "# Detect drift\n",
    "cols1, cols2 = set(day1.columns), set(day2.columns)\n",
    "print(\"\\nSchema drift:\")\n",
    "print(\"Added columns:\", sorted(list(cols2 - cols1)))\n",
    "print(\"Removed columns:\", sorted(list(cols1 - cols2)))\n",
    "\n",
    "# Align columns conservatively\n",
    "all_cols = sorted(list(cols1 | cols2))\n",
    "aligned = pd.concat([day1.reindex(columns=all_cols), day2.reindex(columns=all_cols)], ignore_index=True)\n",
    "\n",
    "# Coerce score numeric as a normalization step\n",
    "aligned[\"score\"] = pd.to_numeric(aligned[\"score\"], errors=\"coerce\")\n",
    "\n",
    "print(\"\\n=== Combined (aligned + normalized) ===\")\n",
    "print(aligned)\n",
    "print(\"Dtypes:\", aligned.dtypes.to_dict())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Rename 'score' to 'risk_score' on day2 and detect semantic drift\")\n",
    "print(\"- Decide: should missing 'tier' on day1 be 'unknown' or null?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce56c60-b47d-425f-937d-e970f5966cc7",
   "metadata": {},
   "source": [
    "## B.4 A practical quality checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca61fa-b4bc-414f-81ae-79ad45121960",
   "metadata": {},
   "source": [
    "A small set of checks catches most downstream bugs.  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Check</th>\n",
    "            <th>Question</th>\n",
    "            <th>Quick Method</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Row meaning</td>\n",
    "            <td>What does one row represent?</td>\n",
    "            <td>Write a one-line row definition</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Missingness</td>\n",
    "            <td>What is missing, and for whom?</td>\n",
    "            <td>Missing rate by column and by group</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Uniqueness</td>\n",
    "            <td>What should be unique?</td>\n",
    "            <td>Check duplicates on key fields</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Ranges</td>\n",
    "            <td>Are numeric values plausible?</td>\n",
    "            <td>Min/max, quantiles, unit sanity check</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Categories</td>\n",
    "            <td>Do labels look consistent?</td>\n",
    "            <td>Top values + \"other\" bucket</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>Are timestamps consistent?</td>\n",
    "            <td>Timezone check; gaps; ordering</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Schema stability</td>\n",
    "            <td>Did fields/types change over time?</td>\n",
    "            <td>Compare scchema snapshots (diff)</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f59e5-834a-4782-9949-8e818d839e06",
   "metadata": {},
   "source": [
    "## B.5 - Lab in other notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca99e2-d5af-4811-947c-511bb941414f",
   "metadata": {},
   "source": [
    "# C - Data Wrangling and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9b71b-abdf-4578-91f2-c48dbcf3103d",
   "metadata": {},
   "source": [
    "Cleaning is only step one. To build models, dashboards, or decisions, you typically need to **reshape**, **combine**, and **encode** raw data into analysis-ready form.  \n",
    "\n",
    "This section will cover the pandas patterns that show up everywhere:  \n",
    "* selecting\n",
    "* filtering\n",
    "* creating columns\n",
    "* grouping / aggregating\n",
    "* joining tables\n",
    "* extracting structure\n",
    "\n",
    "The goal is learning how to preserve meaning while tranforming data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077d316-65fb-430c-8b81-4fe1325ef78b",
   "metadata": {},
   "source": [
    "## C.1 - Pandas Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433b549-f141-4856-8266-fe69d7fd213b",
   "metadata": {},
   "source": [
    "**Three fundamental questions**\n",
    "* What does one row represent right now?\n",
    "* What columns are inputs vs derived outputs?\n",
    "* What assumptions am I encoding when I transform?\n",
    "\n",
    "Rule of Thumb:  \n",
    "Transformations should be reversible in your head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0adf7c5-71de-4070-9946-d911bae1e24a",
   "metadata": {},
   "source": [
    "### Microlab - Pandas Basics (columns, filters, summaries)  \n",
    "\n",
    "Create a few clean derived columns without changing row meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8accd263-f0ec-48fd-aac1-fc4e5ca73323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:\n",
      "   unique_key        created_date         closed_date    borough  status\n",
      "0           1 2025-02-01 10:00:00 2025-02-01 14:00:00  MANHATTAN  Closed\n",
      "1           2 2025-02-01 11:30:00                 NaT   BROOKLYN    Open\n",
      "2           3 2025-02-02 09:15:00 2025-02-02 10:00:00  MANHATTAN  Closed\n",
      "3           4 2025-02-02 12:00:00 2025-02-03 12:00:00     QUEENS  Closed\n",
      "\n",
      "With derived column:\n",
      "   unique_key        created_date         closed_date    borough  status  \\\n",
      "0           1 2025-02-01 10:00:00 2025-02-01 14:00:00  MANHATTAN  Closed   \n",
      "1           2 2025-02-01 11:30:00                 NaT   BROOKLYN    Open   \n",
      "2           3 2025-02-02 09:15:00 2025-02-02 10:00:00  MANHATTAN  Closed   \n",
      "3           4 2025-02-02 12:00:00 2025-02-03 12:00:00     QUEENS  Closed   \n",
      "\n",
      "   is_closed  resolution_hours  \n",
      "0          1              4.00  \n",
      "1          0               NaN  \n",
      "2          1              0.75  \n",
      "3          1             24.00  \n",
      "\n",
      "Summary:\n",
      "Closed rate: 0.75\n",
      "Median resolution (hrs): 4.0\n",
      "\n",
      "Try:\n",
      "- Filter to MANHATTEN and compute the same stats\n",
      "- Decide: should negative or huge durations be flagged?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"unique_key\": [1, 2, 3, 4],\n",
    "  \"created_date\": pd.to_datetime([\"2025-02-01 10:00\", \"2025-02-01 11:30\", \"2025-02-02 09:15\", \"2025-02-02 12:00\"]),\n",
    "  \"closed_date\":  pd.to_datetime([\"2025-02-01 14:00\", None, \"2025-02-02 10:00\", \"2025-02-03 12:00\"]),\n",
    "  \"borough\": [\"MANHATTAN\", \"BROOKLYN\", \"MANHATTAN\", \"QUEENS\"],\n",
    "  \"status\": [\"Closed\", \"Open\", \"Closed\", \"Closed\"],\n",
    "})\n",
    "\n",
    "print(\"Raw:\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Derived columns\n",
    "df[\"is_closed\"] = (df[\"status\"] == \"Closed\").astype(int)\n",
    "\n",
    "# Duration in hours (only when closed)\n",
    "df[\"resolution_hours\"] = (df[\"closed_date\"] - df[\"created_date\"]).dt.total_seconds() / 3600\n",
    "df.loc[df[\"closed_date\"].isna(), \"resolution_hours\"] = np.nan\n",
    "\n",
    "print(\"\\nWith derived column:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Closed rate:\", df[\"is_closed\"].mean())\n",
    "print(\"Median resolution (hrs):\", df[\"resolution_hours\"].median())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Filter to MANHATTEN and compute the same stats\")\n",
    "print(\"- Decide: should negative or huge durations be flagged?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21c7dd-5883-4db4-9d5a-27649253841e",
   "metadata": {},
   "source": [
    "## C.2 - groupby, joins/merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95a1e8-d7ad-4aab-9754-18c7bb967a5a",
   "metadata": {},
   "source": [
    "Aggregation changes what a row means. Joins can multiply rows.\n",
    "\n",
    "**groupby: choosing the unit of analysis**  \n",
    "\n",
    "When you group and aggregate, you are redefining the dataset. A dataset of requests can become a dataset of boroughs, agencies, or days.  \n",
    "\n",
    "**Joins: the most ocmmon silent bug**  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Operation</th>\n",
    "            <th>Risk</th>\n",
    "            <th>Common Symptom</th>\n",
    "            <th>Guardrail</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>groupby + agg</td>\n",
    "            <td>Unit-of-analysis shift</td>\n",
    "            <td>Metrics no longer comparable</td>\n",
    "            <td>Write the new row definition</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>merge/join</td>\n",
    "            <td>Row multiplication</td>\n",
    "            <td>Counts and totals inflate</td>\n",
    "            <td>Check keys + row counts before/after</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6592eb7-9fbd-4594-a6f4-2512bf479304",
   "metadata": {},
   "source": [
    "### Microlab - aggregate, then join back safely  \n",
    "\n",
    "Complete an agency-level metric (like average resolution time) and then merge it back into the request-level table. This is a standard pattern for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9010f1e7-997b-4a63-9e4b-6b40364da79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request-level rows: 5\n",
      "   unique_key agency  resolution_hours  status\n",
      "0           1   NYPD               4.0  Closed\n",
      "1           2   NYPD               NaN    Open\n",
      "2           3    DOT               1.0  Closed\n",
      "3           4    DOT              10.0  Closed\n",
      "4           5    DOT               2.0  Closed\n",
      "\n",
      "Agency-level table:\n",
      "  agency  n_requests  closed_rate  median_resolution\n",
      "0    DOT           3          1.0                2.0\n",
      "1   NYPD           2          0.5                4.0\n",
      "\n",
      "After merge back (rows should match original): 5\n",
      "   unique_key agency  resolution_hours  status  n_requests  closed_rate  \\\n",
      "0           1   NYPD               4.0  Closed           2          0.5   \n",
      "1           2   NYPD               NaN    Open           2          0.5   \n",
      "2           3    DOT               1.0  Closed           3          1.0   \n",
      "3           4    DOT              10.0  Closed           3          1.0   \n",
      "4           5    DOT               2.0  Closed           3          1.0   \n",
      "\n",
      "   median_resolution  \n",
      "0                4.0  \n",
      "1                4.0  \n",
      "2                2.0  \n",
      "3                2.0  \n",
      "4                2.0  \n",
      "\n",
      "Try:\n",
      "- Change validate to see what happens if the keys are not unique\n",
      "- Add another table that has multiple rows per agency and observe row multiplication\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"unique_key\": [1, 2, 3, 4, 5],\n",
    "  \"agency\": [\"NYPD\", \"NYPD\", \"DOT\", \"DOT\", \"DOT\"],\n",
    "  \"resolution_hours\": [4.0, np.nan, 1.0, 10.0, 2.0],\n",
    "  \"status\": [\"Closed\", \"Open\", \"Closed\", \"Closed\", \"Closed\"],\n",
    "})\n",
    "\n",
    "print(\"Request-level rows:\", len(df))\n",
    "print(df)\n",
    "\n",
    "# Aggregate to agency-legel (new unit of analysis)\n",
    "agency_stats = (\n",
    "    df.groupby(\"agency\", dropna=False)\n",
    "      .agg(\n",
    "          n_requests=(\"unique_key\", \"count\"),\n",
    "          closed_rate=(\"status\", lambda s: (s == \"Closed\").mean()),\n",
    "          median_resolution=(\"resolution_hours\", \"median\"),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nAgency-level table:\")\n",
    "print(agency_stats)\n",
    "\n",
    "# Merge back: add columns, should not change row count\n",
    "df2 = df.merge(agency_stats, on=\"agency\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "print(\"\\nAfter merge back (rows should match original):\", len(df2))\n",
    "print(df2)\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Change validate to see what happens if the keys are not unique\")\n",
    "print(\"- Add another table that has multiple rows per agency and observe row multiplication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3edfa1-e7bf-421f-a328-6cec4090d880",
   "metadata": {},
   "source": [
    "## C.3 - String handling and regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3a471-e7fa-4bec-a04c-e4bcbf6673df",
   "metadata": {},
   "source": [
    "Real datasets hide structure inside messy text. Extract it carefully.  \n",
    "\n",
    "**What text cleaning is (and is not)**  \n",
    "* **Cleaning:** removing noise (whitespace, casing), normalizing formats.\n",
    "* **Extraction:** pulling a structured field out of text (zip code, street number, category).\n",
    "* **Not magic:** extraction always has errors, you must measure and handle them  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917af54-1a72-4037-9460-fc3ad1685cff",
   "metadata": {},
   "source": [
    "### Microlab - extract a number and normalize a label  \n",
    "\n",
    "Pull a street number from an address-like string, and normalize a messy category label. Then measure how many rows failed exctraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a51301-0552-4fb1-a57f-15269b9ed63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    incident_address           complaint_type           complaint_norm  \\\n",
      "0        123 Main St  Noise - Street/Sidewalk  noise - street/sidewalk   \n",
      "1    55-01 31st Ave   noise - street/sidewalk  noise - street/sidewalk   \n",
      "2               None  NOISE - Street/Sidewalk  noise - street/sidewalk   \n",
      "3  Broadway & W 50th                   Rodent                   rodent   \n",
      "4     12B Elm Street                  Rodents                  rodents   \n",
      "\n",
      "   street_number  \n",
      "0            123  \n",
      "1             55  \n",
      "2           <NA>  \n",
      "3           <NA>  \n",
      "4             12  \n",
      "\n",
      "Street number extraction fail rate: 0.4\n",
      "\n",
      "Try:\n",
      "- Improve the regex to capture patterns like '55-01' or '12B'\n",
      "- Decide whether you want a strict or lenient extractor\n",
      "- Measure: how many rows become null after extraction?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"incident_address\": [\"123 Main St\", \" 55-01 31st Ave \", None, \"Broadway & W 50th\", \"12B Elm Street\"],\n",
    "  \"complaint_type\": [\"Noise - Street/Sidewalk\", \"noise - street/sidewalk\", \"NOISE - Street/Sidewalk\", \"Rodent\", \"Rodents\"],\n",
    "})\n",
    "\n",
    "# Normalize labels (a light touch)\n",
    "df[\"complaint_norm\"] = (\n",
    "  df[\"complaint_type\"]\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# Regex: try to extract leading street number (very imperfect)\n",
    "df[\"street_number\"] = (\n",
    "  df[\"incident_address\"]\n",
    "    .astype(\"string\")\n",
    "    .str.extract(r\"^\\s*(\\d+)\", expand=False)\n",
    ")\n",
    "\n",
    "df[\"street_number\"] = pd.to_numeric(df[\"street_number\"], errors=\"coerce\")\n",
    "\n",
    "print(df)\n",
    "\n",
    "fail_rate = df[\"street_number\"].isna().mean()\n",
    "print(\"\\nStreet number extraction fail rate:\", round(fail_rate, 3))\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Improve the regex to capture patterns like '55-01' or '12B'\")\n",
    "print(\"- Decide whether you want a strict or lenient extractor\")\n",
    "print(\"- Measure: how many rows become null after extraction?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad38a84-12aa-45c0-9405-b0a03f8d8c5d",
   "metadata": {},
   "source": [
    "## C.4 - Feature construction from raw fields  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416f58f-7b21-415a-a0b1-7c8f7f7d671c",
   "metadata": {},
   "source": [
    "Features are not \"more columns\". They are structured signals that match your decision.  \n",
    "\n",
    "**Good features are predictable and auditable**  \n",
    "* **Stable:** defined the same way over time.\n",
    "* **Non-leaky:** available at prediction time\n",
    "* **Interpretable:** you can explain what it means and why it helps\n",
    "* **Measurable quality:** you can quantify missingness and noise\n",
    "\n",
    "**Common \"first-pass\" features for event-like data**  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Raw field</th>\n",
    "            <th>Feature idea</th>\n",
    "            <th>Why it helps</th>\n",
    "            <th>Risk</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td><code>created_date</code></td>\n",
    "            <td>hour/day-of=week, <br>is_weekend</td>\n",
    "            <td>captures seasonality and staffing effects</td>\n",
    "            <td>timezones issues</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>complaint_type</code></td>\n",
    "            <td>top-k categories + \"Other\"</td>\n",
    "            <td>reduces high-cardinality noise</td>\n",
    "            <td>rare categories get hidden</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>borough</code></td>\n",
    "            <td>one-hot or grouping</td>\n",
    "            <td>location differences</td>\n",
    "            <td>encodes demographic proxies</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>text fields</code></td>\n",
    "            <td>keyword flags / extracted tokens</td>\n",
    "            <td>adds signal from descriptions</td>\n",
    "            <td>high error rate; drift</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715c887-383a-4d3e-9372-0cf4955cc232",
   "metadata": {},
   "source": [
    "### Microlab - Create a few \"safe\" features  \n",
    "\n",
    "Create time-based features and simple top-k category encoding, the kind of features that are often good enough for a baseline model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67299c1-de82-46ef-b743-74fb68d7b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unique_key        created_date     complaint_type  hour  dayofweek  \\\n",
      "0           1 2025-02-01 10:00:00              Noise    10          5   \n",
      "1           2 2025-02-01 23:30:00              Noise    23          5   \n",
      "2           3 2025-02-02 09:15:00             Rodent     9          6   \n",
      "3           4 2025-02-02 12:00:00         Water Leak    12          6   \n",
      "4           5 2025-02-03 08:05:00              Noise     8          0   \n",
      "5           6 2025-02-03 17:45:00  Other Weird Thing    17          0   \n",
      "\n",
      "   is_weekend complaint_topk  \n",
      "0           1          Noise  \n",
      "1           1          Noise  \n",
      "2           1         Rodent  \n",
      "3           1          Other  \n",
      "4           0          Noise  \n",
      "5           0          Other  \n",
      "\n",
      "Try:\n",
      "- Change k to 3 or 4\n",
      "- Add a rare category and see how it collapses into 'Other'\n",
      "- Decide: is 'Other' acceptable for your decision?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"unique_key\": [1, 2, 3, 4, 5, 6],\n",
    "  \"created_date\": pd.to_datetime([\n",
    "    \"2025-02-01 10:00\", \"2025-02-01 23:30\", \"2025-02-02 09:15\",\n",
    "    \"2025-02-02 12:00\", \"2025-02-03 08:05\", \"2025-02-03 17:45\"\n",
    "  ]),\n",
    "  \"complaint_type\": [\"Noise\", \"Noise\", \"Rodent\", \"Water Leak\", \"Noise\", \"Other Weird Thing\"],\n",
    "})\n",
    "\n",
    "df[\"hour\"] = df[\"created_date\"].dt.hour\n",
    "df[\"dayofweek\"] = df[\"created_date\"].dt.dayofweek  # 0=Mon\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5,6]).astype(int)\n",
    "\n",
    "# Top-k category encoding\n",
    "k = 2\n",
    "topk = df[\"complaint_type\"].value_counts().head(k).index\n",
    "df[\"complaint_topk\"] = df[\"complaint_type\"].where(df[\"complaint_type\"].isin(topk), other=\"Other\")\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Change k to 3 or 4\")\n",
    "print(\"- Add a rare category and see how it collapses into 'Other'\")\n",
    "print(\"- Decide: is 'Other' acceptable for your decision?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50df236-3017-4e10-a05b-dc03c6e20a54",
   "metadata": {},
   "source": [
    "## C.5 - Lab in other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb971c1-c8d9-4b49-82f3-a2248f1b2274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
