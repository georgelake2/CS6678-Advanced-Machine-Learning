{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb696c68-5487-46df-82c5-4e8db0dfb9a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# A - Where Data Comes From"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8e759-0b72-4241-822b-ec46203fe341",
   "metadata": {},
   "source": [
    "This section will cover the main places data comes from, what assumptions each source bakes in, and how to do a quick *source audit* before cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5ca67-4ce9-4595-9cd7-682db17e8930",
   "metadata": {},
   "source": [
    "## A.1 Files: CSV, JSON, Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd08049-78c9-4d9b-ac5a-552a20e6c864",
   "metadata": {},
   "source": [
    "Flat files are common, portable, and simple\n",
    "\n",
    "**What these formats are good at:**\n",
    "<table style=\"text-align=left\";>\n",
    "    <tr>\n",
    "        <th>Format</th>\n",
    "        <th>Best suited for</th>\n",
    "        <th>Common misuse</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>CSV</th>\n",
    "        <th>Simple rectangular tables; system exports; quick sharing</th>\n",
    "        <th>Encoding complex structure or multiple tables in one file</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Excel</th>\n",
    "        <th>Human-facing analysis, manual edits, reporting</th>\n",
    "        <th>Using as a source of truth or automated data pipeline input</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>JSON</th>\n",
    "        <th>Nested or semi-structured records; API responses</th>\n",
    "        <th>Assuming fields are stable or consistently present</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90a741-d994-449a-a0ef-e0b5b7a14286",
   "metadata": {},
   "source": [
    "### Hidden assumptions\n",
    "* Someone chose column names, encodings, delimiters, and header rows\n",
    "* Missing values may be implicit(\"\", NA, -1).\n",
    "* Types are inferred later, not enforced at creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c2c84-ceff-4192-8772-ff899b8f2c2b",
   "metadata": {},
   "source": [
    "### Microlab: File sanity check\n",
    "\n",
    "Simulate loading messy exports and practice the first questions that should be asked:\n",
    "* What are the columns?\n",
    "* What looks like missing data?\n",
    "* What types are being inferred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6608396c-9659-4554-8afc-6e2c50f6b713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CSV export (note blanks, NA, weird spacing) ===\n",
      "   user_id   age   income              city\n",
      "0      101  34.0  72000.0            Denver\n",
      "1      102   NaN      NaN           Boulder\n",
      "2      103  29.0  65000.0            Denver\n",
      "3      104  -1.0   5400.0  Colorado Springs\n",
      "\n",
      "Dtypes: {'user_id': dtype('int64'), 'age': dtype('float64'), 'income': dtype('float64'), 'city': dtype('O')}\n",
      "Missing by column: {'user_id': 0, 'age': 1, 'income': 1, 'city': 0}\n",
      "\n",
      "=== JSON export (nested fields) ===\n",
      "   user_id income  profile.age profile.city\n",
      "0      201  80000         31.0       Denver\n",
      "1      202  79000          NaN      Boulder\n",
      "2      203  61000          NaN       Denver\n",
      "\n",
      "Dtypes: {'user_id': dtype('int64'), 'income': dtype('O'), 'profile.age': dtype('float64'), 'profile.city': dtype('O')}\n",
      "\n",
      "Try:\n",
      "- Replace -1 with a real age and re-check missingness\n",
      "- Make income consistently numeric\n",
      "- Rename columns to something consistent (snake_case)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import json\n",
    "\n",
    "print(\"=== CSV export (note blanks, NA, weird spacing) ===\")\n",
    "csv_text = \"\"\"\n",
    "user_id,age,income,city\n",
    "101,34,72000,Denver\n",
    "102,,NA, Boulder\n",
    "103,29,65000,Denver\n",
    "104, -1,5400, \"Colorado Springs\"\n",
    "\"\"\"\n",
    "df_csv = pd.read_csv(StringIO(csv_text),\n",
    "                     skipinitialspace=True)\n",
    "print(df_csv)\n",
    "print(\"\\nDtypes:\", df_csv.dtypes.to_dict())\n",
    "print(\"Missing by column:\", df_csv.isna().sum().to_dict())\n",
    "\n",
    "print(\"\\n=== JSON export (nested fields) ===\")\n",
    "json_text = json.dumps([\n",
    "      {\"user_id\": 201, \"profile\": {\"age\": 31, \"city\": \"Denver\"}, \"income\": 80000},\n",
    "      {\"user_id\": 202, \"profile\": {\"age\": None, \"city\": \"Boulder\"}, \"income\": \"79000\"},\n",
    "      {\"user_id\": 203, \"profile\": {\"city\": \"Denver\"}, \"income\": 61000}\n",
    "])\n",
    "\n",
    "records = json.loads(json_text)\n",
    "df_json = pd.json_normalize(records)\n",
    "print(df_json)\n",
    "print(\"\\nDtypes:\", df_json.dtypes.to_dict())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Replace -1 with a real age and re-check missingness\")\n",
    "print(\"- Make income consistently numeric\")\n",
    "print(\"- Rename columns to something consistent (snake_case)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd55c4-9b9d-4eca-ba27-f62302e68d2a",
   "metadata": {},
   "source": [
    "#### Example with cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be4648a-1491-40eb-9b19-5eebfce68ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CSV export (note blanks, NA, weird spacing) ===\n",
      "   user_id  age  income              city\n",
      "0      101   34   72000            Denver\n",
      "1      102   27   99000           Boulder\n",
      "2      103   29   65000            Denver\n",
      "3      104   44    5400  Colorado Springs\n",
      "\n",
      "Dtypes: {'user_id': dtype('int64'), 'age': dtype('int64'), 'income': dtype('int64'), 'city': dtype('O')}\n",
      "Missing by column: {'user_id': 0, 'age': 0, 'income': 0, 'city': 0}\n",
      "\n",
      "=== JSON export (nested fields) ===\n",
      "   user_id  income  profile.age profile.city\n",
      "0      201   80000           31       Denver\n",
      "1      202   79000           27      Boulder\n",
      "2      203   61000           44       Denver\n",
      "\n",
      "Dtypes: {'user_id': dtype('int64'), 'income': dtype('int64'), 'profile.age': dtype('int64'), 'profile.city': dtype('O')}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import json\n",
    "\n",
    "print(\"=== CSV export (note blanks, NA, weird spacing) ===\")\n",
    "csv_text = \"\"\"\n",
    "user_id,age,income,city\n",
    "101,34,72000,Denver\n",
    "102,27,99000,Boulder\n",
    "103,29,65000,Denver\n",
    "104,44,5400,Colorado Springs\n",
    "\"\"\"\n",
    "df_csv = pd.read_csv(StringIO(csv_text),\n",
    "                     skipinitialspace=True)\n",
    "print(df_csv)\n",
    "print(\"\\nDtypes:\", df_csv.dtypes.to_dict())\n",
    "print(\"Missing by column:\", df_csv.isna().sum().to_dict())\n",
    "\n",
    "print(\"\\n=== JSON export (nested fields) ===\")\n",
    "json_text = json.dumps([\n",
    "      {\"user_id\": 201, \"profile\": {\"age\": 31, \"city\": \"Denver\"}, \"income\": 80000},\n",
    "      {\"user_id\": 202, \"profile\": {\"age\": 27, \"city\": \"Boulder\"}, \"income\": 79000},\n",
    "      {\"user_id\": 203, \"profile\": {\"age\": 44, \"city\": \"Denver\"}, \"income\": 61000}\n",
    "])\n",
    "\n",
    "records = json.loads(json_text)\n",
    "df_json = pd.json_normalize(records)\n",
    "print(df_json)\n",
    "print(\"\\nDtypes:\", df_json.dtypes.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046c3c1-abaf-4668-a633-cbe5c7088016",
   "metadata": {},
   "source": [
    "## A.2 SQL databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb2683-69cb-47c6-94a1-741c5411b15a",
   "metadata": {},
   "source": [
    "Databases reflect how applications think about the world.  \n",
    "\n",
    "**Why databases exist**  \n",
    "\n",
    "Databases are optimized for transactions, consistency, and multi-user access, not analysis. Their schemas encode business logic: users, orders, events, states.  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <tr>\n",
    "        <th>Type</th>\n",
    "        <th>What a row represents</th>\n",
    "        <th>Typical pitfall</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Events</th>\n",
    "        <th>Something that happened at a time</th>\n",
    "        <th>Accidentally double counting or missing time windows</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>State</th>\n",
    "        <th>Current snapshot of something</th>\n",
    "        <th>Assuming it contains historical truth</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed7b1f-9d0a-4325-bfc9-890ce276f3aa",
   "metadata": {},
   "source": [
    "### Microlab: Join logic and granularity traps\n",
    "\n",
    "This microlab creates two tables (users and orders) and shows how a join can silently change your \"row meaning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6ac734-3d32-4568-b929-d3cede53f537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== users (state table) ===\n",
      "(1, 'free')\n",
      "(2, 'paid')\n",
      "(3, 'paid')\n",
      "\n",
      "=== orders (event table) ====\n",
      "(101, 2, 20.0)\n",
      "(102, 2, 35.0)\n",
      "(103, 3, 15.0)\n",
      "(104, 3, 60.0)\n",
      "\n",
      "=== joined view ===\n",
      "(1, 'free', None, None)\n",
      "(2, 'paid', 101, 20.0)\n",
      "(2, 'paid', 102, 35.0)\n",
      "(3, 'paid', 103, 15.0)\n",
      "(3, 'paid', 104, 60.0)\n",
      "\n",
      "Question: How many paid users are there?\n",
      "Correct (from users table): 2\n",
      "WRONG (after join, counting rows): 4\n",
      "\n",
      "Fix: define unite of analysis explicitly.\n",
      "Paid users (DISTINCT user_id): 2\n",
      "\n",
      "Try:\n",
      "- Insert another order for user 2\n",
      "- Compute total revenue by plan using GROUP BY\n",
      "- Ask: what does ONE ROW represent after each query?\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE users (\n",
    "    user_id INTEGER PRIMARY KEY,\n",
    "    plan TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE orders (\n",
    "    order_id INTEGER PRIMARY KEY,\n",
    "    user_id INTEGER,\n",
    "    amount REAL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert data\n",
    "cur.executemany(\n",
    "    \"INSERT INTO users (user_id, plan) VALUES (?, ?)\",\n",
    "    [(1, \"free\"), (2, \"paid\"), (3, \"paid\")]\n",
    ")\n",
    "\n",
    "cur.executemany(\n",
    "    \"INSERT INTO orders (order_id, user_id, amount) VALUES (?, ?, ?)\",\n",
    "    [(101, 2, 20.0), (102, 2, 35.0), (103, 3, 15.0), (104, 3, 60.0)]\n",
    ")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "print(\"=== users (state table) ===\")\n",
    "for row in cur.execute(\"SELECT * FROM users\"):\n",
    "    print(row)\n",
    "\n",
    "print(\"\\n=== orders (event table) ====\")\n",
    "for row in cur.execute(\"SELECT * FROM orders\"):\n",
    "    print(row)\n",
    "\n",
    "print(\"\\n=== joined view ===\")\n",
    "for row in cur.execute(\"\"\"\n",
    "SELECT u.user_id, u.plan, o.order_id, o.amount\n",
    "FROM users u\n",
    "LEFT JOIN orders o\n",
    "ON u.user_id = o.user_id\n",
    "\"\"\"):\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nQuestion: How many paid users are there?\")\n",
    "\n",
    "# Correct answer\n",
    "cur.execute(\"SELECT COUNT(*) FROM users WHERE plan = 'paid'\")\n",
    "print(\"Correct (from users table):\", cur.fetchone()[0])\n",
    "\n",
    "# WRONG answer: counting rows after join\n",
    "cur.execute(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM users u\n",
    "LEFT JOIN orders o\n",
    "ON u.user_id = o.user_id\n",
    "WHERE u.plan = 'paid'\n",
    "\"\"\")\n",
    "print(\"WRONG (after join, counting rows):\", cur.fetchone()[0])\n",
    "\n",
    "print(\"\\nFix: define unite of analysis explicitly.\")\n",
    "\n",
    "#Correct fix using DISTINCT\n",
    "cur.execute(\"\"\"\n",
    "SELECT COUNT(DISTINCT u.user_id)\n",
    "FROM users u\n",
    "LEFT JOIN orders o\n",
    "ON u.user_id = o.user_id\n",
    "WHERE u.plan = 'paid'\n",
    "\"\"\")\n",
    "print(\"Paid users (DISTINCT user_id):\", cur.fetchone()[0])\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Insert another order for user 2\")\n",
    "print(\"- Compute total revenue by plan using GROUP BY\")\n",
    "print(\"- Ask: what does ONE ROW represent after each query?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba7c4f-97c1-4444-97ec-a2bcaf7967ea",
   "metadata": {},
   "source": [
    "## A.3 APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3c8de-d278-4175-a078-1206fac37b32",
   "metadata": {},
   "source": [
    "APIs give you data through someone else's interface and rules  \n",
    "\n",
    "**What APIs provide**  \n",
    "\n",
    "* Programmatic access to live or regularly updated data.\n",
    "* Structured responses (often JSON).\n",
    "* Authentication, quotas, and versioning\n",
    "\n",
    "**Common constraints**\n",
    "* Rate limits: you cannot pull everything at once\n",
    "* Partial views: pagination, filters, or redacted fields\n",
    "* Instability: fields can change or disappear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb0da0-39fb-4436-9dd5-0e65873bf876",
   "metadata": {},
   "source": [
    "### Microlab: Pagination + schema drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfa7669-1f42-47c8-8df1-8eb87279d378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id name score country\n",
      "0   1  Ava  0.91     NaN\n",
      "1   2  Ben  0.74     NaN\n",
      "2   3  Cam  0.88      US\n",
      "3   4  Dee  0.67    None\n",
      "\n",
      "Dtypes: {'id': dtype('int64'), 'name': dtype('O'), 'score': dtype('O'), 'country': dtype('O')}\n",
      "\n",
      "Common API tasks:\n",
      "1) Combine pages (done)\n",
      "2) Normalize fields (types + names)\n",
      "3) Decide how to handle missing optional fields\n",
      "\n",
      "After forcing score numeric:\n",
      "   id name  score country\n",
      "0   1  Ava   0.91     NaN\n",
      "1   2  Ben   0.74     NaN\n",
      "2   3  Cam   0.88      US\n",
      "3   4  Dee   0.67    None\n",
      "\n",
      "Dtypes: {'id': dtype('int64'), 'name': dtype('O'), 'score': dtype('float64'), 'country': dtype('O')}\n",
      "\n",
      "Try:\n",
      "- Add a page3 missing 'name'\n",
      "- Rename 'id' to 'user_id'\n",
      "- Drop rows with missing critical fields vs keep and flag\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simulated API responses (page 1 vs page 2)\n",
    "page1 = [\n",
    "    {\"id\": 1, \"name\": \"Ava\", \"score\": 0.91},\n",
    "    {\"id\": 2, \"name\": \"Ben\", \"score\": 0.74},\n",
    "]\n",
    "\n",
    "page2 = [\n",
    "    {\"id\": 3, \"name\": \"Cam\", \"score\": \"0.88\", \"country\": \"US\"},\n",
    "    {\"id\": 4, \"name\": \"Dee\", \"score\": 0.67, \"country\": None},\n",
    "]\n",
    "\n",
    "df = pd.json_normalize(page1 + page2)\n",
    "\n",
    "print(df)\n",
    "print(\"\\nDtypes:\", df.dtypes.to_dict())\n",
    "\n",
    "print(\"\\nCommon API tasks:\")\n",
    "print(\"1) Combine pages (done)\")\n",
    "print(\"2) Normalize fields (types + names)\")\n",
    "print(\"3) Decide how to handle missing optional fields\")\n",
    "\n",
    "# Example: force score numeric\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "print(\"\\nAfter forcing score numeric:\")\n",
    "print(df)\n",
    "print(\"\\nDtypes:\", df.dtypes.to_dict())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Add a page3 missing 'name'\")\n",
    "print(\"- Rename 'id' to 'user_id'\")\n",
    "print(\"- Drop rows with missing critical fields vs keep and flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993b141-710c-4e61-8fed-9c6d193465a7",
   "metadata": {},
   "source": [
    "## A.4 Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47289689-47cb-4111-8f15-0333c652963f",
   "metadata": {},
   "source": [
    "Scraping is extracting structure from pages built for humans.  \n",
    "\n",
    "**Why scraping exists**  \n",
    "\n",
    "Sometimes the data is visible but not downloadable. Scraping turns HTML pages into rows and columns.  \n",
    "\n",
    "**Why scraping is fragile**  \n",
    "* HTML structure changes without notice\n",
    "* Content may be dynamically loaded\n",
    "* Legal and ethical constraints apply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616baae1-8439-47d4-a8ef-ae56dc89b309",
   "metadata": {},
   "source": [
    "### Microlab: parse a tiny HTML snippet into a table\n",
    "\n",
    "This is a minimal example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff32694-ad0c-429c-8a23-1f9520506aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name price\n",
      "0  Widget A   $10\n",
      "1  Widget B   $12\n",
      "\n",
      "Try:\n",
      "- Change <td>$10</td> to <td>10 usd</td> and clean price\n",
      "- Add a new column in the HTML and update your parser\n",
      "- Remove the header row and see how the assumptions break\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<table>\n",
    "  <tr><th>Name</th><th>Price</th></tr>\n",
    "  <tr><td>Widget A</td><td>$10</td></tr>\n",
    "  <tr><td>Widget B</td><td>$12</td></tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "rows = []\n",
    "for tr in soup.find_all(\"tr\")[1:]:\n",
    "    tds = tr.find_all(\"td\")\n",
    "    rows.append({\n",
    "        \"name\": tds[0].get_text(strip=True),\n",
    "        \"price\": tds[1].get_text(strip=True),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Change <td>$10</td> to <td>10 usd</td> and clean price\")\n",
    "print(\"- Add a new column in the HTML and update your parser\")\n",
    "print(\"- Remove the header row and see how the assumptions break\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0bd2c1-dfa3-47df-88a1-f5ac59734af1",
   "metadata": {},
   "source": [
    "## A.5 Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412e436-1d44-4c16-9520-e4f7cccd4962",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "* **Files:** clean a messy CSV, normalize a nested JSON, handle Excel-style human edits\n",
    "* **SQL:** use SQLite to see how joins change row meaning and how to aggregate safely\n",
    "* **APIs:** combine paginated responses and handle schema drift (simulated)\n",
    "* **Scraping:** parse a tiny HTML table and clean extracted fields.\n",
    "* **MiniProject:** merge sources into one analysis-ready table + write a data quality report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e996c7-c46c-4264-ac5d-81b3f5e552fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# B -  Data Quality and Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed0d92-bae9-4d1b-8396-fdc3af9d6287",
   "metadata": {},
   "source": [
    "\"Messy data\" is not random. It has structure and causes. Quality problems usually come from how systems record events, how humans edit files, and how pipelines evolve over time.  \n",
    "\n",
    "This section focuses on auditing what the data means before it is trusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650e628-f15d-4e57-ae90-0c77bd1327db",
   "metadata": {},
   "source": [
    "## B.1 Missing Data: types and causes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ff162-651c-4073-a598-2cb7e8300f5e",
   "metadata": {},
   "source": [
    "**Three patterns of missingness**\n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <tr>\n",
    "        <th>Pattern</th>\n",
    "        <th>What is means (plain language)</th>\n",
    "        <th>Example</th>\n",
    "        <th>Why it matters</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MCAR (Missing Completely at Random)</th>\n",
    "        <th>Values are missing for reasons unrelated to the data itself (pure noise or chance)</th>\n",
    "        <th>A sensor randomly drops readings due to network glitches</th>\n",
    "        <th>Dropping or simple imputation usually does not bias results (but reduces sample size)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MAR (Missing At Random, conditional on observed data)</th>\n",
    "        <th>Missingness depends on other columns you can see, but not on the missing value itself once you account for them</th>\n",
    "        <th>Income is missing more often for younger users, but within each age group it is random.</th>\n",
    "        <th>Imputation using observed features can be reasonable, but assumptions must be documented.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MNAR (Missing Not At Random)</th>\n",
    "        <th>Missingness depends on the value that is missing, even after accounting for obserced data</th>\n",
    "        <th>People with very low income skip the income question because it is low</th>\n",
    "        <th>Naive dropping or imputation can bias conclusions; this is the hardest case to fix.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Missing is often encoded**\n",
    "* **Sentinels:** -1, 999, \"unknown\", \"N/A\"\n",
    "* **Empty strings:** \"\"\n",
    "* **Whitespace:** \" \"\n",
    "* **Type coercion:** numbers stored as strings, turning failures into nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebba490-c865-499a-8b11-cdea9ad8c58d",
   "metadata": {},
   "source": [
    "### Microlab: detect missingness + missingness as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7928dfd2-8745-41f7-9f01-eeb452397aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data:\n",
      "   user_id age income              city  plan\n",
      "0      101  34  72000            Denver  paid\n",
      "1      102         NA           Boulder  free\n",
      "2      103  29  65000                    paid\n",
      "3      104  -1  54000  Colorado Springs  free\n",
      "4      105  41   None            Denver  paid\n",
      "5      106        999           unknown  free\n",
      "\n",
      "After normalizing missing encodings:\n",
      "   user_id   age   income              city  plan\n",
      "0      101  34.0  72000.0            Denver  paid\n",
      "1      102   NaN      NaN           Boulder  free\n",
      "2      103  29.0  65000.0               NaN  paid\n",
      "3      104   NaN  54000.0  Colorado Springs  free\n",
      "4      105  41.0      NaN            Denver  paid\n",
      "5      106   NaN      NaN               NaN  free\n",
      "\n",
      "Missing counts: {'user_id': 0, 'age': 3, 'income': 3, 'city': 2, 'plan': 0}\n",
      "\n",
      "Income missing rate by plan:\n",
      "plan\n",
      "free    0.666667\n",
      "paid    0.333333\n",
      "Name: income_missing, dtype: float64\n",
      "\n",
      "Try:\n",
      "- Add more rows where only 'free' users hide income\n",
      "- Ask: does dropping missing-income rows change the plan mix?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23972/4204433707.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_clean = df.replace(missing_tokens, np.nan)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"user_id\": [101, 102, 103, 104, 105, 106],\n",
    "  \"age\": [34, \"\", 29, -1, 41, \"  \"],\n",
    "  \"income\": [\"72000\", \"NA\", \"65000\", \"54000\", None, \"999\"],\n",
    "  \"city\": [\"Denver\", \"Boulder\", \"\", \"Colorado Springs\", \"Denver\", \"unknown\"],\n",
    "  \"plan\": [\"paid\", \"free\", \"paid\", \"free\", \"paid\", \"free\"]\n",
    "})\n",
    "\n",
    "print(\"Raw data:\")\n",
    "print(df)\n",
    "\n",
    "# Step 1: standardize common missing encodings\n",
    "missing_tokens = [\"\", \"  \", \"NA\", \"N/A\", \"unknown\", \"999\"]\n",
    "df_clean = df.replace(missing_tokens, np.nan)\n",
    "\n",
    "# Step 2: coerce numeric columns\n",
    "df_clean[\"age\"] = pd.to_numeric(df_clean[\"age\"], errors=\"coerce\")\n",
    "df_clean.loc[df_clean[\"age\"] < 0, \"age\"] = np.nan\n",
    "df_clean[\"income\"] = pd.to_numeric(df_clean[\"income\"], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nAfter normalizing missing encodings:\")\n",
    "print(df_clean)\n",
    "print(\"\\nMissing counts:\", df_clean.isna().sum().to_dict())\n",
    "\n",
    "# Missingness can be informative:\n",
    "df_clean[\"income_missing\"] = df_clean[\"income\"].isna().astype(int)\n",
    "\n",
    "print(\"\\nIncome missing rate by plan:\")\n",
    "print(df_clean.groupby(\"plan\")[\"income_missing\"].mean())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Add more rows where only 'free' users hide income\")\n",
    "print(\"- Ask: does dropping missing-income rows change the plan mix?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47763dc2-4f18-487f-b9fd-ae1cc787fd3e",
   "metadata": {},
   "source": [
    "## B2. Duplicates and inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cde25c-5846-49a6-b619-dc4bf2f7fdaa",
   "metadata": {},
   "source": [
    "Duplication is usually a modeling bug waiting to happen: double-counting, label leakage, or identity confusion\n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <tr>\n",
    "        <th>Type</th>\n",
    "        <th>What it looks like</th>\n",
    "        <th>Common cause</th>\n",
    "        <th>What to do</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Exact duplicates</th>\n",
    "        <th>Same value across all columns</th>\n",
    "        <th>Export glitch, retry logic, copy/paste</th>\n",
    "        <th>Usually safe to drop</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Entity duplicates</th>\n",
    "        <th>Same real-world entity appears multiple times</th>\n",
    "        <th>Multiple IDs, casing\\typos, merges</th>\n",
    "        <th>Requires a dedupe rule</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Inconsistency is often \"almost the same\"**  \n",
    "\n",
    "* **Strings:** casing, whitespace, punctuation (\"NY\" vs \"New York\")\n",
    "* **Categories:** synonyms (\"M\", \"male\", \"man\")\n",
    "* **Units:** kg vs lb, dollars vs cents\n",
    "* **Timezones:** mixing UTC and local time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60eb38-e889-4a85-ab64-885fba54a9b5",
   "metadata": {},
   "source": [
    "### Microlab: Exact duplicates vs entity duplicates \n",
    "\n",
    "Remove exact duplicates and then face the harder case: multiple records that probably refer to the same customer. Practice dedupe rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b6eaad-1b19-4a49-b667-86909359b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:\n",
      "  customer_id            email      name updated_at       city\n",
      "0        C001  ava@example.com    Ava Li 2025-01-01     Denver\n",
      "1        C001  ava@example.com    Ava Li 2025-01-01     Denver\n",
      "2        C009  ava@example.com    Ava L. 2025-02-10     Denver\n",
      "3        C010  ben@example.com       Ben 2025-02-09   Boulder \n",
      "4        C011  ben@example.com  Benjamin 2025-02-12    Boulder\n",
      "\n",
      "1) Drop exact duplicates:\n",
      "  customer_id            email      name updated_at       city\n",
      "0        C001  ava@example.com    Ava Li 2025-01-01     Denver\n",
      "2        C009  ava@example.com    Ava L. 2025-02-10     Denver\n",
      "3        C010  ben@example.com       Ben 2025-02-09   Boulder \n",
      "4        C011  ben@example.com  Benjamin 2025-02-12    Boulder\n",
      "\n",
      "2) Normalize strings (city):\n",
      "  customer_id            email     city\n",
      "0        C001  ava@example.com   Denver\n",
      "2        C009  ava@example.com   Denver\n",
      "3        C010  ben@example.com  Boulder\n",
      "4        C011  ben@example.com  Boulder\n",
      "\n",
      "3) Entity dedupe rule: keep the most recent row per email\n",
      "  customer_id            email      name updated_at     city\n",
      "2        C009  ava@example.com    Ava L. 2025-02-10   Denver\n",
      "4        C011  ben@example.com  Benjamin 2025-02-12  Boulder\n",
      "\n",
      "Try:\n",
      "- Change the rule: keep row with longest name, or fewest missing fields\n",
      "- Ask: what is your unit of analysis — customer, email, or account?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23972/2530805920.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[\"city\"] = df1[\"city\"].str.strip()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "  {\"customer_id\": \"C001\", \"email\": \"ava@example.com\", \"name\": \"Ava Li\", \"updated_at\": \"2025-01-01\", \"city\": \"Denver\"},\n",
    "  {\"customer_id\": \"C001\", \"email\": \"ava@example.com\", \"name\": \"Ava Li\", \"updated_at\": \"2025-01-01\", \"city\": \"Denver\"},  # exact dup\n",
    "  {\"customer_id\": \"C009\", \"email\": \"ava@example.com\", \"name\": \"Ava L.\", \"updated_at\": \"2025-02-10\", \"city\": \"Denver\"},   # entity dup (same email)\n",
    "  {\"customer_id\": \"C010\", \"email\": \"ben@example.com\", \"name\": \"Ben\", \"updated_at\": \"2025-02-09\", \"city\": \" Boulder \"},    # whitespace\n",
    "  {\"customer_id\": \"C011\", \"email\": \"ben@example.com\", \"name\": \"Benjamin\", \"updated_at\": \"2025-02-12\", \"city\": \"Boulder\"}, # entity dup\n",
    "])\n",
    "\n",
    "df[\"updated_at\"] = pd.to_datetime(df[\"updated_at\"])\n",
    "print(\"Raw:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\n1) Drop exact duplicates:\")\n",
    "df1 = df.drop_duplicates()\n",
    "print(df1)\n",
    "\n",
    "print(\"\\n2) Normalize strings (city):\")\n",
    "df1[\"city\"] = df1[\"city\"].str.strip()\n",
    "print(df1[[\"customer_id\",\"email\",\"city\"]])\n",
    "\n",
    "print(\"\\n3) Entity dedupe rule: keep the most recent row per email\")\n",
    "df2 = df1.sort_values(\"updated_at\").drop_duplicates(subset=[\"email\"], keep=\"last\")\n",
    "print(df2)\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Change the rule: keep row with longest name, or fewest missing fields\")\n",
    "print(\"- Ask: what is your unit of analysis — customer, email, or account?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d5397-a05c-4119-90aa-782e33c19ae5",
   "metadata": {},
   "source": [
    "## B.3 Schema drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a13ae-bfdf-4a2c-888c-f949ca21be56",
   "metadata": {},
   "source": [
    "Over time, the meaning of columns change, sometimes quietly.  \n",
    "\n",
    "**What \"drift\" looks like in raw data**.\n",
    "* New columns appear (a new feature launches)\n",
    "* Columns disappear (a field is deprecated)\n",
    "* Type changes (integer becomes string; cents becomes dollars)\n",
    "* Semantic changes (same name, different meaning)\n",
    "\n",
    "**Why this matters**\n",
    "Drift can break pipelines, but worse: It can create silent correctness failures. Your model may keep training, on a different problem than intended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864010ff-c962-4fd5-9f53-114a07143d73",
   "metadata": {},
   "source": [
    "### Microlab: Compare \"yesterday vs today\" schemas\n",
    "\n",
    "Simulate two daily exports, detect differences, and choose a conservative strategy:\n",
    "* Align columns\n",
    "* Coerce types\n",
    "* Log drift for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15e1d15-af87-48ca-bdcd-8df3416c3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Day 1 ===\n",
      "   id  score country\n",
      "0   1   0.91      US\n",
      "1   2   0.74      US\n",
      "Dtypes: {'id': dtype('int64'), 'score': dtype('float64'), 'country': dtype('O')}\n",
      "\n",
      "=== Day 2 ===\n",
      "   id score country  tier\n",
      "0   3  0.88      CA   pro\n",
      "1   4  0.67    None  free\n",
      "Dtypes: {'id': dtype('int64'), 'score': dtype('O'), 'country': dtype('O'), 'tier': dtype('O')}\n",
      "\n",
      "Schema drift:\n",
      "Added columns: ['tier']\n",
      "Removed columns: []\n",
      "\n",
      "=== Combined (aligned + normalized) ===\n",
      "  country  id  score  tier\n",
      "0      US   1   0.91   NaN\n",
      "1      US   2   0.74   NaN\n",
      "2      CA   3   0.88   pro\n",
      "3    None   4   0.67  free\n",
      "Dtypes: {'country': dtype('O'), 'id': dtype('int64'), 'score': dtype('float64'), 'tier': dtype('O')}\n",
      "\n",
      "Try:\n",
      "- Rename 'score' to 'risk_score' on day2 and detect semantic drift\n",
      "- Decide: should missing 'tier' on day1 be 'unknown' or null?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "day1 = pd.DataFrame([\n",
    "  {\"id\": 1, \"score\": 0.91, \"country\": \"US\"},\n",
    "  {\"id\": 2, \"score\": 0.74, \"country\": \"US\"},\n",
    "])\n",
    "\n",
    "day2 = pd.DataFrame([\n",
    "  {\"id\": 3, \"score\": \"0.88\", \"country\": \"CA\", \"tier\": \"pro\"},  # score becomes string; new column tier\n",
    "  {\"id\": 4, \"score\": \"0.67\", \"country\": None, \"tier\": \"free\"},\n",
    "])\n",
    "\n",
    "print(\"=== Day 1 ===\")\n",
    "print(day1)\n",
    "print(\"Dtypes:\", day1.dtypes.to_dict())\n",
    "\n",
    "print(\"\\n=== Day 2 ===\")\n",
    "print(day2)\n",
    "print(\"Dtypes:\", day2.dtypes.to_dict())\n",
    "\n",
    "# Detect drift\n",
    "cols1, cols2 = set(day1.columns), set(day2.columns)\n",
    "print(\"\\nSchema drift:\")\n",
    "print(\"Added columns:\", sorted(list(cols2 - cols1)))\n",
    "print(\"Removed columns:\", sorted(list(cols1 - cols2)))\n",
    "\n",
    "# Align columns conservatively\n",
    "all_cols = sorted(list(cols1 | cols2))\n",
    "aligned = pd.concat([day1.reindex(columns=all_cols), day2.reindex(columns=all_cols)], ignore_index=True)\n",
    "\n",
    "# Coerce score numeric as a normalization step\n",
    "aligned[\"score\"] = pd.to_numeric(aligned[\"score\"], errors=\"coerce\")\n",
    "\n",
    "print(\"\\n=== Combined (aligned + normalized) ===\")\n",
    "print(aligned)\n",
    "print(\"Dtypes:\", aligned.dtypes.to_dict())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Rename 'score' to 'risk_score' on day2 and detect semantic drift\")\n",
    "print(\"- Decide: should missing 'tier' on day1 be 'unknown' or null?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce56c60-b47d-425f-937d-e970f5966cc7",
   "metadata": {},
   "source": [
    "## B.4 A practical quality checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca61fa-b4bc-414f-81ae-79ad45121960",
   "metadata": {},
   "source": [
    "A small set of checks catches most downstream bugs.  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Check</th>\n",
    "            <th>Question</th>\n",
    "            <th>Quick Method</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Row meaning</td>\n",
    "            <td>What does one row represent?</td>\n",
    "            <td>Write a one-line row definition</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Missingness</td>\n",
    "            <td>What is missing, and for whom?</td>\n",
    "            <td>Missing rate by column and by group</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Uniqueness</td>\n",
    "            <td>What should be unique?</td>\n",
    "            <td>Check duplicates on key fields</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Ranges</td>\n",
    "            <td>Are numeric values plausible?</td>\n",
    "            <td>Min/max, quantiles, unit sanity check</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Categories</td>\n",
    "            <td>Do labels look consistent?</td>\n",
    "            <td>Top values + \"other\" bucket</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>Are timestamps consistent?</td>\n",
    "            <td>Timezone check; gaps; ordering</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Schema stability</td>\n",
    "            <td>Did fields/types change over time?</td>\n",
    "            <td>Compare scchema snapshots (diff)</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f59e5-834a-4782-9949-8e818d839e06",
   "metadata": {},
   "source": [
    "## B.5 - Lab in other notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca99e2-d5af-4811-947c-511bb941414f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# C - Data Wrangling and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9b71b-abdf-4578-91f2-c48dbcf3103d",
   "metadata": {},
   "source": [
    "Cleaning is only step one. To build models, dashboards, or decisions, you typically need to **reshape**, **combine**, and **encode** raw data into analysis-ready form.  \n",
    "\n",
    "This section will cover the pandas patterns that show up everywhere:  \n",
    "* selecting\n",
    "* filtering\n",
    "* creating columns\n",
    "* grouping / aggregating\n",
    "* joining tables\n",
    "* extracting structure\n",
    "\n",
    "The goal is learning how to preserve meaning while tranforming data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077d316-65fb-430c-8b81-4fe1325ef78b",
   "metadata": {},
   "source": [
    "## C.1 - Pandas Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433b549-f141-4856-8266-fe69d7fd213b",
   "metadata": {},
   "source": [
    "**Three fundamental questions**\n",
    "* What does one row represent right now?\n",
    "* What columns are inputs vs derived outputs?\n",
    "* What assumptions am I encoding when I transform?\n",
    "\n",
    "Rule of Thumb:  \n",
    "Transformations should be reversible in your head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0adf7c5-71de-4070-9946-d911bae1e24a",
   "metadata": {},
   "source": [
    "### Microlab - Pandas Basics (columns, filters, summaries)  \n",
    "\n",
    "Create a few clean derived columns without changing row meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8accd263-f0ec-48fd-aac1-fc4e5ca73323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:\n",
      "   unique_key        created_date         closed_date    borough  status\n",
      "0           1 2025-02-01 10:00:00 2025-02-01 14:00:00  MANHATTAN  Closed\n",
      "1           2 2025-02-01 11:30:00                 NaT   BROOKLYN    Open\n",
      "2           3 2025-02-02 09:15:00 2025-02-02 10:00:00  MANHATTAN  Closed\n",
      "3           4 2025-02-02 12:00:00 2025-02-03 12:00:00     QUEENS  Closed\n",
      "\n",
      "With derived column:\n",
      "   unique_key        created_date         closed_date    borough  status  \\\n",
      "0           1 2025-02-01 10:00:00 2025-02-01 14:00:00  MANHATTAN  Closed   \n",
      "1           2 2025-02-01 11:30:00                 NaT   BROOKLYN    Open   \n",
      "2           3 2025-02-02 09:15:00 2025-02-02 10:00:00  MANHATTAN  Closed   \n",
      "3           4 2025-02-02 12:00:00 2025-02-03 12:00:00     QUEENS  Closed   \n",
      "\n",
      "   is_closed  resolution_hours  \n",
      "0          1              4.00  \n",
      "1          0               NaN  \n",
      "2          1              0.75  \n",
      "3          1             24.00  \n",
      "\n",
      "Summary:\n",
      "Closed rate: 0.75\n",
      "Median resolution (hrs): 4.0\n",
      "\n",
      "Try:\n",
      "- Filter to MANHATTEN and compute the same stats\n",
      "- Decide: should negative or huge durations be flagged?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"unique_key\": [1, 2, 3, 4],\n",
    "  \"created_date\": pd.to_datetime([\"2025-02-01 10:00\", \"2025-02-01 11:30\", \"2025-02-02 09:15\", \"2025-02-02 12:00\"]),\n",
    "  \"closed_date\":  pd.to_datetime([\"2025-02-01 14:00\", None, \"2025-02-02 10:00\", \"2025-02-03 12:00\"]),\n",
    "  \"borough\": [\"MANHATTAN\", \"BROOKLYN\", \"MANHATTAN\", \"QUEENS\"],\n",
    "  \"status\": [\"Closed\", \"Open\", \"Closed\", \"Closed\"],\n",
    "})\n",
    "\n",
    "print(\"Raw:\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Derived columns\n",
    "df[\"is_closed\"] = (df[\"status\"] == \"Closed\").astype(int)\n",
    "\n",
    "# Duration in hours (only when closed)\n",
    "df[\"resolution_hours\"] = (df[\"closed_date\"] - df[\"created_date\"]).dt.total_seconds() / 3600\n",
    "df.loc[df[\"closed_date\"].isna(), \"resolution_hours\"] = np.nan\n",
    "\n",
    "print(\"\\nWith derived column:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Closed rate:\", df[\"is_closed\"].mean())\n",
    "print(\"Median resolution (hrs):\", df[\"resolution_hours\"].median())\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Filter to MANHATTEN and compute the same stats\")\n",
    "print(\"- Decide: should negative or huge durations be flagged?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21c7dd-5883-4db4-9d5a-27649253841e",
   "metadata": {},
   "source": [
    "## C.2 - groupby, joins/merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95a1e8-d7ad-4aab-9754-18c7bb967a5a",
   "metadata": {},
   "source": [
    "Aggregation changes what a row means. Joins can multiply rows.\n",
    "\n",
    "**groupby: choosing the unit of analysis**  \n",
    "\n",
    "When you group and aggregate, you are redefining the dataset. A dataset of requests can become a dataset of boroughs, agencies, or days.  \n",
    "\n",
    "**Joins: the most ocmmon silent bug**  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Operation</th>\n",
    "            <th>Risk</th>\n",
    "            <th>Common Symptom</th>\n",
    "            <th>Guardrail</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>groupby + agg</td>\n",
    "            <td>Unit-of-analysis shift</td>\n",
    "            <td>Metrics no longer comparable</td>\n",
    "            <td>Write the new row definition</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>merge/join</td>\n",
    "            <td>Row multiplication</td>\n",
    "            <td>Counts and totals inflate</td>\n",
    "            <td>Check keys + row counts before/after</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6592eb7-9fbd-4594-a6f4-2512bf479304",
   "metadata": {},
   "source": [
    "### Microlab - aggregate, then join back safely  \n",
    "\n",
    "Complete an agency-level metric (like average resolution time) and then merge it back into the request-level table. This is a standard pattern for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9010f1e7-997b-4a63-9e4b-6b40364da79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request-level rows: 5\n",
      "   unique_key agency  resolution_hours  status\n",
      "0           1   NYPD               4.0  Closed\n",
      "1           2   NYPD               NaN    Open\n",
      "2           3    DOT               1.0  Closed\n",
      "3           4    DOT              10.0  Closed\n",
      "4           5    DOT               2.0  Closed\n",
      "\n",
      "Agency-level table:\n",
      "  agency  n_requests  closed_rate  median_resolution\n",
      "0    DOT           3          1.0                2.0\n",
      "1   NYPD           2          0.5                4.0\n",
      "\n",
      "After merge back (rows should match original): 5\n",
      "   unique_key agency  resolution_hours  status  n_requests  closed_rate  \\\n",
      "0           1   NYPD               4.0  Closed           2          0.5   \n",
      "1           2   NYPD               NaN    Open           2          0.5   \n",
      "2           3    DOT               1.0  Closed           3          1.0   \n",
      "3           4    DOT              10.0  Closed           3          1.0   \n",
      "4           5    DOT               2.0  Closed           3          1.0   \n",
      "\n",
      "   median_resolution  \n",
      "0                4.0  \n",
      "1                4.0  \n",
      "2                2.0  \n",
      "3                2.0  \n",
      "4                2.0  \n",
      "\n",
      "Try:\n",
      "- Change validate to see what happens if the keys are not unique\n",
      "- Add another table that has multiple rows per agency and observe row multiplication\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"unique_key\": [1, 2, 3, 4, 5],\n",
    "  \"agency\": [\"NYPD\", \"NYPD\", \"DOT\", \"DOT\", \"DOT\"],\n",
    "  \"resolution_hours\": [4.0, np.nan, 1.0, 10.0, 2.0],\n",
    "  \"status\": [\"Closed\", \"Open\", \"Closed\", \"Closed\", \"Closed\"],\n",
    "})\n",
    "\n",
    "print(\"Request-level rows:\", len(df))\n",
    "print(df)\n",
    "\n",
    "# Aggregate to agency-legel (new unit of analysis)\n",
    "agency_stats = (\n",
    "    df.groupby(\"agency\", dropna=False)\n",
    "      .agg(\n",
    "          n_requests=(\"unique_key\", \"count\"),\n",
    "          closed_rate=(\"status\", lambda s: (s == \"Closed\").mean()),\n",
    "          median_resolution=(\"resolution_hours\", \"median\"),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nAgency-level table:\")\n",
    "print(agency_stats)\n",
    "\n",
    "# Merge back: add columns, should not change row count\n",
    "df2 = df.merge(agency_stats, on=\"agency\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "print(\"\\nAfter merge back (rows should match original):\", len(df2))\n",
    "print(df2)\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Change validate to see what happens if the keys are not unique\")\n",
    "print(\"- Add another table that has multiple rows per agency and observe row multiplication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3edfa1-e7bf-421f-a328-6cec4090d880",
   "metadata": {},
   "source": [
    "## C.3 - String handling and regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3a471-e7fa-4bec-a04c-e4bcbf6673df",
   "metadata": {},
   "source": [
    "Real datasets hide structure inside messy text. Extract it carefully.  \n",
    "\n",
    "**What text cleaning is (and is not)**  \n",
    "* **Cleaning:** removing noise (whitespace, casing), normalizing formats.\n",
    "* **Extraction:** pulling a structured field out of text (zip code, street number, category).\n",
    "* **Not magic:** extraction always has errors, you must measure and handle them  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917af54-1a72-4037-9460-fc3ad1685cff",
   "metadata": {},
   "source": [
    "### Microlab - extract a number and normalize a label  \n",
    "\n",
    "Pull a street number from an address-like string, and normalize a messy category label. Then measure how many rows failed exctraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a51301-0552-4fb1-a57f-15269b9ed63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    incident_address           complaint_type           complaint_norm  \\\n",
      "0        123 Main St  Noise - Street/Sidewalk  noise - street/sidewalk   \n",
      "1    55-01 31st Ave   noise - street/sidewalk  noise - street/sidewalk   \n",
      "2               None  NOISE - Street/Sidewalk  noise - street/sidewalk   \n",
      "3  Broadway & W 50th                   Rodent                   rodent   \n",
      "4     12B Elm Street                  Rodents                  rodents   \n",
      "\n",
      "   street_number  \n",
      "0            123  \n",
      "1             55  \n",
      "2           <NA>  \n",
      "3           <NA>  \n",
      "4             12  \n",
      "\n",
      "Street number extraction fail rate: 0.4\n",
      "\n",
      "Try:\n",
      "- Improve the regex to capture patterns like '55-01' or '12B'\n",
      "- Decide whether you want a strict or lenient extractor\n",
      "- Measure: how many rows become null after extraction?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"incident_address\": [\"123 Main St\", \" 55-01 31st Ave \", None, \"Broadway & W 50th\", \"12B Elm Street\"],\n",
    "  \"complaint_type\": [\"Noise - Street/Sidewalk\", \"noise - street/sidewalk\", \"NOISE - Street/Sidewalk\", \"Rodent\", \"Rodents\"],\n",
    "})\n",
    "\n",
    "# Normalize labels (a light touch)\n",
    "df[\"complaint_norm\"] = (\n",
    "  df[\"complaint_type\"]\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# Regex: try to extract leading street number (very imperfect)\n",
    "df[\"street_number\"] = (\n",
    "  df[\"incident_address\"]\n",
    "    .astype(\"string\")\n",
    "    .str.extract(r\"^\\s*(\\d+)\", expand=False)\n",
    ")\n",
    "\n",
    "df[\"street_number\"] = pd.to_numeric(df[\"street_number\"], errors=\"coerce\")\n",
    "\n",
    "print(df)\n",
    "\n",
    "fail_rate = df[\"street_number\"].isna().mean()\n",
    "print(\"\\nStreet number extraction fail rate:\", round(fail_rate, 3))\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Improve the regex to capture patterns like '55-01' or '12B'\")\n",
    "print(\"- Decide whether you want a strict or lenient extractor\")\n",
    "print(\"- Measure: how many rows become null after extraction?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad38a84-12aa-45c0-9405-b0a03f8d8c5d",
   "metadata": {},
   "source": [
    "## C.4 - Feature construction from raw fields  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416f58f-7b21-415a-a0b1-7c8f7f7d671c",
   "metadata": {},
   "source": [
    "Features are not \"more columns\". They are structured signals that match your decision.  \n",
    "\n",
    "**Good features are predictable and auditable**  \n",
    "* **Stable:** defined the same way over time.\n",
    "* **Non-leaky:** available at prediction time\n",
    "* **Interpretable:** you can explain what it means and why it helps\n",
    "* **Measurable quality:** you can quantify missingness and noise\n",
    "\n",
    "**Common \"first-pass\" features for event-like data**  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Raw field</th>\n",
    "            <th>Feature idea</th>\n",
    "            <th>Why it helps</th>\n",
    "            <th>Risk</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td><code>created_date</code></td>\n",
    "            <td>hour/day-of=week, <br>is_weekend</td>\n",
    "            <td>captures seasonality and staffing effects</td>\n",
    "            <td>timezones issues</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>complaint_type</code></td>\n",
    "            <td>top-k categories + \"Other\"</td>\n",
    "            <td>reduces high-cardinality noise</td>\n",
    "            <td>rare categories get hidden</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>borough</code></td>\n",
    "            <td>one-hot or grouping</td>\n",
    "            <td>location differences</td>\n",
    "            <td>encodes demographic proxies</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>text fields</code></td>\n",
    "            <td>keyword flags / extracted tokens</td>\n",
    "            <td>adds signal from descriptions</td>\n",
    "            <td>high error rate; drift</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715c887-383a-4d3e-9372-0cf4955cc232",
   "metadata": {},
   "source": [
    "### Microlab - Create a few \"safe\" features  \n",
    "\n",
    "Create time-based features and simple top-k category encoding, the kind of features that are often good enough for a baseline model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67299c1-de82-46ef-b743-74fb68d7b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unique_key        created_date     complaint_type  hour  dayofweek  \\\n",
      "0           1 2025-02-01 10:00:00              Noise    10          5   \n",
      "1           2 2025-02-01 23:30:00              Noise    23          5   \n",
      "2           3 2025-02-02 09:15:00             Rodent     9          6   \n",
      "3           4 2025-02-02 12:00:00         Water Leak    12          6   \n",
      "4           5 2025-02-03 08:05:00              Noise     8          0   \n",
      "5           6 2025-02-03 17:45:00  Other Weird Thing    17          0   \n",
      "\n",
      "   is_weekend complaint_topk  \n",
      "0           1          Noise  \n",
      "1           1          Noise  \n",
      "2           1         Rodent  \n",
      "3           1          Other  \n",
      "4           0          Noise  \n",
      "5           0          Other  \n",
      "\n",
      "Try:\n",
      "- Change k to 3 or 4\n",
      "- Add a rare category and see how it collapses into 'Other'\n",
      "- Decide: is 'Other' acceptable for your decision?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"unique_key\": [1, 2, 3, 4, 5, 6],\n",
    "  \"created_date\": pd.to_datetime([\n",
    "    \"2025-02-01 10:00\", \"2025-02-01 23:30\", \"2025-02-02 09:15\",\n",
    "    \"2025-02-02 12:00\", \"2025-02-03 08:05\", \"2025-02-03 17:45\"\n",
    "  ]),\n",
    "  \"complaint_type\": [\"Noise\", \"Noise\", \"Rodent\", \"Water Leak\", \"Noise\", \"Other Weird Thing\"],\n",
    "})\n",
    "\n",
    "df[\"hour\"] = df[\"created_date\"].dt.hour\n",
    "df[\"dayofweek\"] = df[\"created_date\"].dt.dayofweek  # 0=Mon\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5,6]).astype(int)\n",
    "\n",
    "# Top-k category encoding\n",
    "k = 2\n",
    "topk = df[\"complaint_type\"].value_counts().head(k).index\n",
    "df[\"complaint_topk\"] = df[\"complaint_type\"].where(df[\"complaint_type\"].isin(topk), other=\"Other\")\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Change k to 3 or 4\")\n",
    "print(\"- Add a rare category and see how it collapses into 'Other'\")\n",
    "print(\"- Decide: is 'Other' acceptable for your decision?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50df236-3017-4e10-a05b-dc03c6e20a54",
   "metadata": {},
   "source": [
    "## C.5 - Lab in other notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb971c1-c8d9-4b49-82f3-a2248f1b2274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# D - Scaling Data Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73336b-48c2-4b13-8255-015ef1004ade",
   "metadata": {},
   "source": [
    "We will now scale the workflows giving:\n",
    "* repeatable runs\n",
    "* reliable storage\n",
    "* clear contracts\n",
    "* and performance that does not collapse at 10x data\n",
    "\n",
    "This section gives the conceptual map: ETL vs ELT, lakes vs warehouses, and when tools like Polars become worth learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff88d62-9fdb-4b25-b7e1-9ebea67037c7",
   "metadata": {},
   "source": [
    "## D.1 - ETL vs ELT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e264a8-30c1-44ea-b6e3-b8bb97ca5dc8",
   "metadata": {},
   "source": [
    "The order matters: Where do you transform, before storage or after?  \n",
    "\n",
    "<table style=\"text-index:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Pattern</th>\n",
    "            <th>Flow</th>\n",
    "            <th>Best When</th>\n",
    "            <th>Main Risk</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>ETL</td>\n",
    "            <td>Extract &rarr; <br>Transform &rarr; <br>Load</td>\n",
    "            <td>You need strict schemas, smaller curated datasets, or tight control before data lands in the system of record</td>\n",
    "            <td>Transform logic becomes a bottleneck; hard to reprocess when assumptions change.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>ELT</td>\n",
    "            <td>Extract &rarr; <br>Load &rarr; <br>Transform</td>\n",
    "            <td>You can store raw-ish data cheaply and transform in a scalable comput layer (SQL warehouse, Spark, etc.)</td>\n",
    "            <td>Raw becomes a swamp if you do not enforce contracts, lineage, and governance</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "**How this maps to what was built in 2.A - 2.c**  \n",
    "* **2.A:** Extraction into raw folder (raw/)\n",
    "* **2.B:** Quality Contracts (reference/quality_report.json)\n",
    "* **2.C:** Transformation into a warehouse-ready artifact (warehouse/*.parquet)\n",
    "\n",
    "**Tip:**  \n",
    "When you are unsure, favor **ELT with strong contracts:** store raw data + logs, then build curated tables from it. You can always tighten later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d72fd-6afb-49ee-8a2c-f89127b1a0f6",
   "metadata": {},
   "source": [
    "## D.2 - Data Lakes and Warehouses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed8eff-0c4d-42b2-ab89-573f46cb7b0b",
   "metadata": {},
   "source": [
    "Storage is not just where files go, it is a set of promises about structure and access  \n",
    "\n",
    "**Lake vs warehouse (the useful mental model)**  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Concept</th>\n",
    "            <th>What it optimizes for</th>\n",
    "            <th>Typical Storage</th>\n",
    "            <th>What can go wrong</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Data lake</td>\n",
    "            <td>Cheap storage of raw-ish data, many formats</td>\n",
    "            <td>Object storage + files <br>(CSV/JSON/Parquet)</td>\n",
    "            <td>Becomes a data swamp without naming, schemas, and ownership</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Data warehouse</td>\n",
    "            <td>Fast analyitics with enforced structure</td> \n",
    "            <td>Columnar tables <br>(often SQL-based)</td>\n",
    "            <td>Slow ingestion or brittle pipelines if over-modeled too early</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "**Parquet**  \n",
    "Parquet is a columnar format that tends to be dramatically faster than CSV for analytics and ML workflows, especially as datasets grow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ba510-be07-409f-b799-9b9fd54c8b48",
   "metadata": {},
   "source": [
    "## D.3 - When Polars Makes Sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35879fd9-8849-4831-8f0a-effda2b1c437",
   "metadata": {},
   "source": [
    "Pandas is great, until the dataset size or performance needs change.\n",
    "\n",
    "**What Polars Changes**  \n",
    "* **Speed:** often faster for large data and heavy groupby operations.\n",
    "* **Memory:** more efficient execution, especially with columnar data.\n",
    "* **Lazy execution:** build a query plan, then execute efficiently\n",
    "\n",
    "**When should it be used**  \n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Signal</th>\n",
    "            <th>What you notice</th>\n",
    "            <th>Likely fix</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Performance pain</td>\n",
    "            <td>Pandas groupby/joins are slow or crash on memory</td>\n",
    "            <td>Try Parquet + Polars or push work to SQL</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Pipeline-style transforms</td>\n",
    "            <td>Want clear, composable, transformation graph</td>\n",
    "            <td>Use Polars lazy queries or SQL transformations</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Repeatability</td>\n",
    "            <td>Rerun the same steps daily/weekly</td>\n",
    "            <td>Build a small ELT pipeline with checkpoints + logs</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1aa3f3-1655-44f7-93f7-3080313c924b",
   "metadata": {},
   "source": [
    "### Microlab - Why Parquet and columnar matters  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17577691-e339-4922-8278-126f777a94c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: wide.csv MB: 31.8\n",
      "Saved: wide.parquet MB: 16.34\n",
      "\n",
      "Read CSV (selected cols): (50000, 3)\n",
      "Read Parquet (selected cols): (50000, 3)\n",
      "\n",
      "Try:\n",
      "- Increase n to 200_000 and observe file sizes\n",
      "- Read ALL columns vs a few\n",
      "- This is why Parquet/columnar formats matter at scale\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a toy wide dataset\n",
    "n = 50_000\n",
    "df = pd.DataFrame({\n",
    "  \"id\": np.arange(n),\n",
    "  \"x\": np.random.randn(n),\n",
    "  \"y\": np.random.randn(n),\n",
    "  \"category\": np.random.choice([\"A\",\"B\",\"C\",\"D\"], size=n),\n",
    "})\n",
    "\n",
    "# Add many extra columns (wide table)\n",
    "for i in range(30):\n",
    "  df[f\"extra_{i}\"] = np.random.randn(n)\n",
    "\n",
    "p = Path.home() / \"work\" / \"m2\" / \"tmp\"\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = p / \"wide.csv\"\n",
    "parquet_path = p / \"wide.parquet\"\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(\"Saved:\", csv_path.name, \"MB:\", round(csv_path.stat().st_size/1e6, 2))\n",
    "print(\"Saved:\", parquet_path.name, \"MB:\", round(parquet_path.stat().st_size/1e6, 2))\n",
    "\n",
    "# Read only a few columns\n",
    "use_cols = [\"id\", \"category\", \"x\"]\n",
    "\n",
    "df_csv = pd.read_csv(csv_path, usecols=use_cols)\n",
    "df_pq = pd.read_parquet(parquet_path, columns=use_cols)\n",
    "\n",
    "print(\"\\nRead CSV (selected cols):\", df_csv.shape)\n",
    "print(\"Read Parquet (selected cols):\", df_pq.shape)\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Increase n to 200_000 and observe file sizes\")\n",
    "print(\"- Read ALL columns vs a few\")\n",
    "print(\"- This is why Parquet/columnar formats matter at scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f960413-b7ff-4415-aa33-23d010c710b2",
   "metadata": {},
   "source": [
    "## D.5 - Lab Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c2054-90c9-4936-8c81-1311db610ff3",
   "metadata": {},
   "source": [
    "# E - Outliers and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6083a1a-776c-421c-be50-90a19b4b20c4",
   "metadata": {},
   "source": [
    "Outliers are not just \"weird values\". They are often the first signal that something upstream is broken:  \n",
    "* schema\n",
    "* drift\n",
    "* unit changes\n",
    "* duplicated records\n",
    "* logging bugs\n",
    "* mismatched joins\n",
    "\n",
    "The goal is to remove outliers by investigating and then decide what to do.  \n",
    "\n",
    "This section covers two core habits:  \n",
    "1. Detect anomalies with simple and reliable checks\n",
    "2. Validate the cleaning pipeline so that mistakes *fail fast*\n",
    "\n",
    "Will also look at how \"helpful cleaning\" can accidentally introduce **data leakage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232fb75-b4c8-4131-a51c-1ff3e372998e",
   "metadata": {},
   "source": [
    "## E.1 - Detecting Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90338b1f-3fe0-4872-b1f1-18b36aad099b",
   "metadata": {},
   "source": [
    "Anomaly detection is often \"find rows that violate your expectations\".  \n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Type</th>\n",
    "            <th>What it means</th>\n",
    "            <th>Example</th>\n",
    "            <th>What to do first</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Rare but real</td>\n",
    "            <td>Uncommon, but valid</td>\n",
    "            <td>A truly long resolution time for a complex issue</td>\n",
    "            <td>Check domain plausibility; <br>keep, but consider robust models</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Data error</td>\n",
    "            <td>Incorrect value</td>\n",
    "            <td>Negative durations, impossible dates</td>\n",
    "            <td>Trace upstream; decide correction vs removal vs flag</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Definition shift</td>\n",
    "            <td>Meaning changed</td>\n",
    "            <td>Units changed (seconds - minutes), category renames</td>\n",
    "            <td>Detect drift, update contract, reprocess historical data</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "**Practical anomaly checks**\n",
    "* **Range checks:** negatives, future timestamps, physically impossible values\n",
    "* **Distribution checks:** sudden spikes, heavy tails, new modes\n",
    "* **Group sanity checks:** rates by category/agency that suddenly jump\n",
    "* **Cross-field consistency:** `closed_date` should not be before `created_date`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad5030-43ac-4f45-87f2-20b7af98f30c",
   "metadata": {},
   "source": [
    "### Microlab - Anomaly flags for durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4061eb-d168-409e-8ea7-86ce1d0e9969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unique_key        created_date         closed_date  status  is_closed  \\\n",
      "0           1 2025-02-01 10:00:00 2025-02-01 14:00:00  Closed          1   \n",
      "1           2 2025-02-01 11:30:00                 NaT  Closed          1   \n",
      "2           3 2025-02-02 09:15:00 2025-02-02 08:00:00  Closed          1   \n",
      "3           4 2025-02-02 12:00:00 2025-03-15 12:00:00  Closed          1   \n",
      "4           5 2025-02-03 08:00:00 2025-02-03 08:10:00    Open          0   \n",
      "\n",
      "   resolution_hours  anom_negative_duration  anom_over_30d  \\\n",
      "0          4.000000                       0              0   \n",
      "1               NaN                       0              0   \n",
      "2         -1.250000                       1              0   \n",
      "3        984.000000                       0              1   \n",
      "4          0.166667                       0              0   \n",
      "\n",
      "   anom_closed_missing_closed_date  anom_open_has_closed_date  anom_any  \n",
      "0                                0                          0         0  \n",
      "1                                1                          0         1  \n",
      "2                                0                          0         1  \n",
      "3                                0                          0         1  \n",
      "4                                0                          1         1  \n",
      "\n",
      "Suspicious rows:\n",
      "   unique_key  status        created_date         closed_date  \\\n",
      "1           2  Closed 2025-02-01 11:30:00                 NaT   \n",
      "2           3  Closed 2025-02-02 09:15:00 2025-02-02 08:00:00   \n",
      "3           4  Closed 2025-02-02 12:00:00 2025-03-15 12:00:00   \n",
      "4           5    Open 2025-02-03 08:00:00 2025-02-03 08:10:00   \n",
      "\n",
      "   resolution_hours  anom_negative_duration  anom_over_30d  \\\n",
      "1               NaN                       0              0   \n",
      "2         -1.250000                       1              0   \n",
      "3        984.000000                       0              1   \n",
      "4          0.166667                       0              0   \n",
      "\n",
      "   anom_closed_missing_closed_date  anom_open_has_closed_date  \n",
      "1                                1                          0  \n",
      "2                                0                          0  \n",
      "3                                0                          0  \n",
      "4                                0                          1  \n",
      "\n",
      "Try:\n",
      "- Change thresholds (7 days vs 30 days)\n",
      "- Decide: should you fix, drop, or flag these rows?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"unique_key\": [1, 2, 3, 4, 5],\n",
    "  \"created_date\": pd.to_datetime([\"2025-02-01 10:00\", \"2025-02-01 11:30\", \"2025-02-02 09:15\", \"2025-02-02 12:00\", \"2025-02-03 08:00\"]),\n",
    "  \"closed_date\":  pd.to_datetime([\"2025-02-01 14:00\", None, \"2025-02-02 08:00\", \"2025-03-15 12:00\", \"2025-02-03 08:10\"]),\n",
    "  \"status\": [\"Closed\", \"Closed\", \"Closed\", \"Closed\", \"Open\"],\n",
    "})\n",
    "\n",
    "df[\"is_closed\"] = (df[\"status\"] == \"Closed\").astype(int)\n",
    "\n",
    "df[\"resolution_hours\"] = (df[\"closed_date\"] - df[\"created_date\"]).dt.total_seconds() / 3600\n",
    "df.loc[df[\"closed_date\"].isna(), \"resolution_hours\"] = np.nan\n",
    "\n",
    "# Anomaly flags\n",
    "df[\"anom_negative_duration\"] = (df[\"resolution_hours\"] < 0).astype(int)\n",
    "df[\"anom_over_30d\"] = (df[\"resolution_hours\"] > 30*24).astype(int)\n",
    "df[\"anom_closed_missing_closed_date\"] = ((df[\"is_closed\"] == 1) & (df[\"closed_date\"].isna())).astype(int)\n",
    "df[\"anom_open_has_closed_date\"] = ((df[\"is_closed\"] == 0) & (df[\"closed_date\"].notna())).astype(int)\n",
    "\n",
    "# Investigation view\n",
    "anom_cols = [c for c in df.columns if c.startswith(\"anom_\")]\n",
    "df[\"anom_any\"] = (df[anom_cols].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(\"\\nSuspicious rows:\")\n",
    "print(df.loc[df[\"anom_any\"] == 1, [\"unique_key\",\"status\",\"created_date\",\"closed_date\",\"resolution_hours\"] + anom_cols])\n",
    "\n",
    "print(\"\\nTry:\")\n",
    "print(\"- Change thresholds (7 days vs 30 days)\")\n",
    "print(\"- Decide: should you fix, drop, or flag these rows?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e33479c-302d-4be1-bffd-dd5936188417",
   "metadata": {},
   "source": [
    "## E.2 - Validation as a Habit (Tests + Contracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc302d-480d-4cfa-8650-8a37f6b4b129",
   "metadata": {},
   "source": [
    "Validation is how you make data cleaning safe to scale.  \n",
    "\n",
    "**Validation is just \"makes assumptions explicit\"**  \n",
    "\n",
    "Every cleaning pipeline has assumptions. Validation is writing those assumptions down as checks:  \n",
    "* required columns\n",
    "* allowed values\n",
    "* ranges\n",
    "* uniqueness\n",
    "* invariants\n",
    "\n",
    "**High-leverage checks**\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Check</th>\n",
    "            <th>What it catches</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Schema checks</td>\n",
    "            <td>Missing/renamed columns</td>\n",
    "            <td>agency disappears &rarr; pipeline fails</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Uniqueness checks</td>\n",
    "            <td>Duplicates, join multiplication</td>\n",
    "            <td>unique_key repeats</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Range checks</td>\n",
    "            <td>Unit changes, sensor bugs</td>\n",
    "            <td>Duration goes from hours to seconds</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Distribution checks</td>\n",
    "            <td>Drift and logging changes</td>\n",
    "            <td>Complaint categories shift overnight</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229ca6a-4009-44b8-805f-a7039f5c0e4f",
   "metadata": {},
   "source": [
    "### Microlab - Write two pipeline tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4beb67c6-6ccd-40a3-80ce-350ed0ca3e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unique_key agency  status\n",
      "0           1   NYPD  Closed\n",
      "1           2    DOT    Open\n",
      "2           2    DOT  Closed\n",
      "\n",
      "Running checks...\n"
     ]
    },
    {
     "ename": "DataContractError",
     "evalue": "Key 'unique_key' has 1 duplicate values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataContractError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning checks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m require_columns(df, [\u001b[33m\"\u001b[39m\u001b[33munique_key\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33magency\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mrequire_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munique_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# should raise\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIf you see this, checks ran correctly!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mrequire_unique\u001b[39m\u001b[34m(df, key)\u001b[39m\n\u001b[32m     12\u001b[39m dupes = df[key].duplicated().sum()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dupes:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m DataContractError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(dupes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m duplicate values\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mDataContractError\u001b[39m: Key 'unique_key' has 1 duplicate values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataContractError(RuntimeError):\n",
    "  pass\n",
    "\n",
    "def require_columns(df, cols):\n",
    "  missing = [c for c in cols if c not in df.columns]\n",
    "  if missing:\n",
    "    raise DataContractError(f\"Missing columns: {missing}\")\n",
    "\n",
    "def require_unique(df, key):\n",
    "  dupes = df[key].duplicated().sum()\n",
    "  if dupes:\n",
    "    raise DataContractError(f\"Key '{key}' has {int(dupes)} duplicate values\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"unique_key\": [1, 2, 2],  # duplicate on purpose\n",
    "  \"agency\": [\"NYPD\", \"DOT\", \"DOT\"],\n",
    "  \"status\": [\"Closed\", \"Open\", \"Closed\"],\n",
    "})\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(\"\\nRunning checks...\")\n",
    "require_columns(df, [\"unique_key\", \"agency\", \"status\"])\n",
    "require_unique(df, \"unique_key\")  # should raise\n",
    "\n",
    "print(\"If you see this, checks ran correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903014b0-3513-47e4-9fe3-f1d1f2e731c0",
   "metadata": {},
   "source": [
    "**Reality Check**  \n",
    "\n",
    "If there is an error this code is supposed to fail, like above.  \n",
    "\n",
    "In production, run these checks on every pipeline run and stop the job if the fail. Quitely continuing is how broken dashboards and models are created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4cd44-da99-49b2-b2f1-94e20e96a9a9",
   "metadata": {},
   "source": [
    "## E.3 - Leakage Risks Introduced During Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600f976-0180-4eec-b183-8752832f96b6",
   "metadata": {},
   "source": [
    "**Leakage often looks like \"healpful cleaning\"**\n",
    "\n",
    "Leakage happens when a feature contains information that would not exist at prediction time. Cleaning and feature engineering are common places leakage sneaks in because you are combining tables, imputing missing values, and creating summary statistics.  \n",
    "\n",
    "**Common leakage patterns**\n",
    "\n",
    "<table style=\"text-align:left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Pattern</th>\n",
    "            <th>What it looks like</th>\n",
    "            <th>Why it's leakage</th>\n",
    "            <th>Fix</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Using outcome fields</td>\n",
    "            <td>Include closed_date to predict closure speed</td>\n",
    "            <td>Outcome is known only after the event</td>\n",
    "            <td>Drop outcome fields from features; keep only as labels</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Global aggregation</td>\n",
    "            <td>Compute overall mean using full dataset (incl. future)</td>\n",
    "            <td>Test set \"influences\" training features</td>\n",
    "            <td>Fit transformers on train only; apply to test</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Post-event joins</td>\n",
    "            <td>Join with tables updated after the decision</td>\n",
    "            <td>Brings in future info</td>\n",
    "            <td>Join only on data available at event time; time-aware joins</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Imputation with target leakage</td>\n",
    "            <td>Fill missing using group stats that depend on outcome</td>\n",
    "            <td>Encodes label information</td>\n",
    "            <td>Impute without using target; use pipelines</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ea6da-7f81-477c-be04-7581464e0aab",
   "metadata": {},
   "source": [
    "### Microlab - A Subtle Leakage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0585a236-81ef-475b-8ed0-5a39a9886618",
   "metadata": {},
   "source": [
    "Compute group means the wrong way vs the right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be84912-8b02-427d-a21f-cd7933d50533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "  agency     x  y\n",
      "1      A   2.0  0\n",
      "3      B  10.0  1\n",
      "0      A   1.0  0\n",
      "4      B   NaN  1\n",
      "\n",
      "Test:\n",
      "  agency     x  y\n",
      "5      B  12.0  1\n",
      "2      A   NaN  1\n",
      "\n",
      "Global means (WRONG):\n",
      "agency\n",
      "A     1.5\n",
      "B    11.0\n",
      "Name: x, dtype: float64\n",
      "\n",
      "Train means (RIGHT):\n",
      "agency\n",
      "A     1.5\n",
      "B    10.0\n",
      "Name: x, dtype: float64\n",
      "\n",
      "Test with WRONG imputation:\n",
      "  agency     x  y  x_imputed\n",
      "5      B  12.0  1       12.0\n",
      "2      A   NaN  1        1.5\n",
      "\n",
      "Test with RIGHT imputation:\n",
      "  agency     x  y  x_imputed\n",
      "5      B  12.0  1       12.0\n",
      "2      A   NaN  1        1.5\n",
      "\n",
      "Key idea:\n",
      "- If test rows influence preprocessing statistics, you are leaking information.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"agency\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"],\n",
    "  \"x\": [1, 2, None, 10, None, 12],\n",
    "  \"y\": [0, 0, 1, 1, 1, 1],  # pretend this is the label\n",
    "})\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.33, random_state=0)\n",
    "\n",
    "print(\"Train:\")\n",
    "print(train)\n",
    "print(\"\\nTest:\")\n",
    "print(test)\n",
    "\n",
    "# WRONG: compute imputation stats using ALL data (train + test)\n",
    "global_means = df.groupby(\"agency\")[\"x\"].mean()\n",
    "test_wrong = test.copy()\n",
    "test_wrong[\"x_imputed\"] = test_wrong[\"x\"].fillna(test_wrong[\"agency\"].map(global_means))\n",
    "\n",
    "# RIGHT: compute stats on TRAIN only, then apply to test\n",
    "train_means = train.groupby(\"agency\")[\"x\"].mean()\n",
    "test_right = test.copy()\n",
    "test_right[\"x_imputed\"] = test_right[\"x\"].fillna(test_right[\"agency\"].map(train_means))\n",
    "\n",
    "print(\"\\nGlobal means (WRONG):\")\n",
    "print(global_means)\n",
    "\n",
    "print(\"\\nTrain means (RIGHT):\")\n",
    "print(train_means)\n",
    "\n",
    "print(\"\\nTest with WRONG imputation:\")\n",
    "print(test_wrong)\n",
    "\n",
    "print(\"\\nTest with RIGHT imputation:\")\n",
    "print(test_right)\n",
    "\n",
    "print(\"\\nKey idea:\")\n",
    "print(\"- If test rows influence preprocessing statistics, you are leaking information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff623f7-8095-4276-9fbf-6f3e1d5303c1",
   "metadata": {},
   "source": [
    "## E.4 Lab Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105550a-4d91-4dfe-8143-7d38959375dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
