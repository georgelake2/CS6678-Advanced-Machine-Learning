{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee1a4aa-b5a4-4c88-a391-0e9ab228fbfd",
   "metadata": {},
   "source": [
    "# Practical Project: USGS Earthquakes Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d98ea1-5b2a-40d9-938b-0300ad7f3d75",
   "metadata": {},
   "source": [
    "In this project we will build a small but real **scientific data pipeline** using the **USGS Earthquake Catalog API**.  \n",
    "\n",
    "### What will be produced:\n",
    "Artifacts (all under a project folder):\n",
    "* `data/raw/`: raw API snapshots + metadata (query params, timestamps)\n",
    "* `data/staged/`: parsed/normalized table (deduped, typed)\n",
    "* `data/warehouse/`: curated table (Parquet; optionally partitioned by day)\n",
    "* `data/reference/validation_report.json`: contracts + anomaly rates + canaries\n",
    "* `data/reference/pipeline_runs/`: run logs for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15945e4-1a09-4a7e-80fd-81aec9ab67ca",
   "metadata": {},
   "source": [
    "## 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3edf6d5-3857-4ac4-8e98-cbc5c0528268",
   "metadata": {},
   "source": [
    "Project will be created in the path relative to this notebook  \n",
    "\n",
    "`/work/m2_project/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d35c31-caf8-46f1-b974-273f664e5355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: work/m2_project\n",
      "Raw: work/m2_project/data/raw\n",
      "Staged: work/m2_project/data/staged\n",
      "Warehouse: work/m2_project/data/warehouse\n",
      "Reference: work/m2_project/data/reference\n",
      "Runs: work/m2_project/data/reference/pipeline_runs\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import json\n",
    "import hashlib\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "WORK_DIR = Path(\"work\")\n",
    "PROJECT_DIR = WORK_DIR / \"m2_project\"\n",
    "\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGED_DIR = DATA_DIR / \"staged\"\n",
    "WH_DIR = DATA_DIR / \"warehouse\"\n",
    "REF_DIR = DATA_DIR / \"reference\"\n",
    "RUN_DIR = REF_DIR / \"pipeline_runs\"\n",
    "\n",
    "for p in [RAW_DIR, STAGED_DIR, WH_DIR, REF_DIR, RUN_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project:\", PROJECT_DIR)\n",
    "print(\"Raw:\", RAW_DIR)\n",
    "print(\"Staged:\", STAGED_DIR)\n",
    "print(\"Warehouse:\", WH_DIR)\n",
    "print(\"Reference:\", REF_DIR)\n",
    "print(\"Runs:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c449d-907d-48e4-815d-781533958e67",
   "metadata": {},
   "source": [
    "### Helper Utilities  \n",
    "\n",
    "These helpers keep the notebook focused on pipeline thinking rather than boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51db39c8-79ac-4c91-ad1b-10b159f3f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready.\n"
     ]
    }
   ],
   "source": [
    "class PipelineError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def sha16(x: str) -> str:\n",
    "    return hashlib.sha256(x.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def write_json(path: Path, obj: dict) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, default=str))\n",
    "\n",
    "def read_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "def require_columns(df: pd.DataFrame, cols: list[str], context: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise PipelineError(f\"[{context}] Missing required columns: {missing}\")\n",
    "\n",
    "def require_unique(df: pd.DataFrame, key: str, context: str) -> None:\n",
    "    if key not in df.columns:\n",
    "        raise PipelineError(f\"[{context}] Missing key column '{key}'\")\n",
    "    dupes = int(df[key].duplicated().sum())\n",
    "    if dupes:\n",
    "        raise PipelineError(f\"[{context}] Key '{key}' has {dupes} duplicates\")\n",
    "\n",
    "print(\"Helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea692be-f057-4a04-9150-1b68fd6a667a",
   "metadata": {},
   "source": [
    "## 1 - Ingest: Pull Earthquakes From USGS API (Paginated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e290b-9a5d-4a1e-b231-4358db460749",
   "metadata": {},
   "source": [
    "We will use the USGS **event** endpoint (GeoJSON):\n",
    "* base `https://earthquak.usgs.gov/fdsnws/event/1/query`\n",
    "* params `format=geojson`, `starttime`, `endtime`, `minmagnitude`, plus pagination (`limit, offset`)\n",
    "\n",
    "### Choose a query window\n",
    "Set:\n",
    "* `DAYS_BACK` = 30\n",
    "* `MIN_MAG` = 2.5\n",
    "\n",
    "Then build `starttime` and `endtime` in **UTC** as ISO dates\n",
    "\n",
    "**Note:** The API likes `YYYY-MM-DD` strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0f5527-4166-4f92-9712-01a09fd9d2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starttime: 2026-01-04\n",
      "endtime: 2026-02-03\n",
      "minmagnitude: 2.5\n"
     ]
    }
   ],
   "source": [
    "# Set query window\n",
    "DAYS_BACK = 30\n",
    "MIN_MAG = 2.5\n",
    "\n",
    "# compute starttime/endtime as YYYY-MM-DD (UTC-based)\n",
    "endtime_dt = datetime.now(timezone.utc)\n",
    "starttime_dt = endtime_dt - timedelta(days=DAYS_BACK)\n",
    "\n",
    "starttime = starttime_dt.strftime('%Y-%m-%d')\n",
    "endtime = endtime_dt.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"starttime: {starttime}\")\n",
    "print(f\"endtime: {endtime}\")\n",
    "print(f\"minmagnitude: {MIN_MAG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92ee37-5b88-46d4-b568-ed2f2a3224d1",
   "metadata": {},
   "source": [
    "### Implement pagination\n",
    "\n",
    "Write `fetch_usgs_pages(...)` that:\n",
    "* requests pages using `limit` and `offset`\n",
    "* stops when a page returns fewer than `limit` features\n",
    "* returns a list of page dictionaries\n",
    "\n",
    "**Constraints:**  \n",
    "* Use a small `limit` while testing (e.g., 200) to see pagination work.\n",
    "* Add a polite `sleep` if wanted, but keep it simple\n",
    "\n",
    "**Note:**\n",
    "* `obj[\"features\"]` = list of events\n",
    "* `obj[\"metadata\"][\"count\"]` = count for the query (not always equal to returned features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b58514-7abe-46b3-9719-48ee1e1231f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetcher ready.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "USGS_URL = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "\n",
    "def fetch_usgs_pages(starttime: str, endtime: str, minmag: float, limit: int=200, max_pages: int=50) -> list[dict]:\n",
    "    pages = []\n",
    "    offset = 1  # USGS uses 1-based offsets\n",
    "    for page_i in range(max_pages):\n",
    "        params = {\n",
    "            \"format\": \"geojson\",\n",
    "            \"starttime\": starttime,\n",
    "            \"endtime\": endtime,\n",
    "            \"minmagnitude\": minmag,\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "            \"orderby\": \"time\",\n",
    "        }\n",
    "\n",
    "        # Request the page, check the status, parse JSON\n",
    "        print(f\"Fetching page {page_i + 1} (offset={offset})...\", end=\" \")\n",
    "        response = requests.get(USGS_URL, params=params)\n",
    "\n",
    "        # Check for HTTP erros\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        features = data.get(\"features\", [])\n",
    "        total_count = data.get(\"metadata\", {}).get(\"count\", 0)\n",
    "        print(f\"got {len(features)} events (total in query: {total_count})\")\n",
    "\n",
    "        # Quick stop if there are no results\n",
    "        if not features:\n",
    "            print(\"No more results, stopping\")\n",
    "            break\n",
    "\n",
    "        pages.append(data)\n",
    "\n",
    "        # Stop if we get fewer results than the limit (last page)\n",
    "        if len(features) < limit:\n",
    "            print(\"Partial page received, stopping\")\n",
    "            break\n",
    "\n",
    "        # Increment offset for next page\n",
    "        offset += limit\n",
    "\n",
    "        # Short polite break for API\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return pages\n",
    "\n",
    "print(\"Fetcher ready.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547327c-691b-488e-b829-e97cbecee144",
   "metadata": {},
   "source": [
    "### Fetch data and write a raw snapshot\n",
    "\n",
    "Run the fetch and write:  \n",
    "* raw pages: `data/raw/usgs_pages_<runid>.jsonl`\n",
    "* raw_metadata: `data/raw/usgs_meta_<runid>.json`\n",
    "\n",
    "**Note:**  \n",
    "`.jsonl` means \"JSON lines\": one JSON object per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aff9a96-4330-4acc-ae14-c7604ad2571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 (offset=1)... got 200 events (total in query: 200)\n",
      "Fetching page 2 (offset=201)... got 200 events (total in query: 200)\n",
      "Fetching page 3 (offset=401)... got 200 events (total in query: 200)\n",
      "Fetching page 4 (offset=601)... got 200 events (total in query: 200)\n",
      "Fetching page 5 (offset=801)... got 200 events (total in query: 200)\n",
      "Fetching page 6 (offset=1001)... got 200 events (total in query: 200)\n",
      "Fetching page 7 (offset=1201)... got 200 events (total in query: 200)\n",
      "Fetching page 8 (offset=1401)... got 190 events (total in query: 190)\n",
      "Partial page received, stopping\n",
      "\n",
      "Fetched 8 pages\n",
      "Total events: 1590\n",
      "Wrote pages: work/m2_project/data/raw/usgs_pages_20260203_171853_utc.jsonl\n",
      "Wrote metadata: work/m2_project/data/raw/usgs_meta_20260203_171853_utc.json\n",
      "\n",
      "Metadata:\n",
      "{'run_id': '20260203_171853_utc',\n",
      " 'generated_at_utc': '2026-02-03T17:18:58.340696+00:00',\n",
      " 'query': {'starttime': '2026-01-04',\n",
      "           'endtime': '2026-02-03',\n",
      "           'minmagnitude': 2.5},\n",
      " 'n_pages': 8,\n",
      " 'n_features_total': 1590,\n",
      " 'source': 'USGS Earthquake Catalog (GeoJSON)',\n",
      " 'endpoint': 'https://earthquake.usgs.gov/fdsnws/event/1/query'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Fetch data and write raw snapshot\n",
    "\n",
    "# Compute run_id\n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S_utc\")\n",
    "\n",
    "# Fetch pages\n",
    "pages = fetch_usgs_pages(starttime, endtime, MIN_MAG, limit=200)\n",
    "print(f\"\\nFetched {len(pages)} pages\")\n",
    "\n",
    "# Count total events\n",
    "total_events = sum(len(p['features']) for p in pages)\n",
    "print(f\"Total events: {total_events}\")\n",
    "\n",
    "# Write raw pages as JSONL (one JSON object per line)\n",
    "pages_path = RAW_DIR / f\"usgs_pages_{RUN_ID}.jsonl\"\n",
    "pages_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with pages_path.open('w') as f:\n",
    "    for page in pages:\n",
    "        f.write(json.dumps(page) + '\\n')\n",
    "\n",
    "print(f\"Wrote pages: {pages_path}\")\n",
    "\n",
    "# Write metadata\n",
    "metadata = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"generated_at_utc\": utc_now_iso(),\n",
    "    \"query\": {\n",
    "        \"starttime\": starttime,\n",
    "        \"endtime\": endtime,\n",
    "        \"minmagnitude\": MIN_MAG,\n",
    "    },\n",
    "    \"n_pages\": len(pages),\n",
    "    \"n_features_total\": total_events,\n",
    "    \"source\": \"USGS Earthquake Catalog (GeoJSON)\",\n",
    "    \"endpoint\": USGS_URL,\n",
    "}\n",
    "\n",
    "meta_path = RAW_DIR / f\"usgs_meta_{RUN_ID}.json\"\n",
    "write_json(meta_path, metadata)\n",
    "print(f\"Wrote metadata: {meta_path}\")\n",
    "\n",
    "\n",
    "# Pretty print the metadata\n",
    "print(\"\\nMetadata:\")\n",
    "pprint(metadata, sort_dicts=False)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a69ce-d272-45fd-a041-043d9539d056",
   "metadata": {},
   "source": [
    "## 2 - Stage: Normalize GeoJSON &rarr; Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e96f32-fad2-4a60-bb50-63c4d0715164",
   "metadata": {},
   "source": [
    "USGS GeoJSON structure:\n",
    "* `feature[\"id\"]` is a stable event id\n",
    "* `feature[\"properties\"]` contains magnitude, place, time, etc.\n",
    "* `feature[\"geometry\"][\"coordinates\"]`is`[longitude, latitude, depth_km]`\n",
    "\n",
    "### Flatten features into a DataFrame  \n",
    "\n",
    "Implement `features_to_df(pages)` that returns one DataFrame with one row per event.\n",
    "\n",
    "**Note:**  \n",
    "`pd.json_normalize(features)` will help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409a579e-ca3a-40f8-9117-e4ba2c6814ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening function ready\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with one row per event\n",
    "def features_to_df(pages: list[dict]) -> pd.DataFrame:\n",
    "    # 1. Extract all features from all pages\n",
    "    all_features = []\n",
    "    for page in pages:\n",
    "        all_features.extend(page.get(\"features\", []))\n",
    "\n",
    "    # 2. Use pd.json_normalize to flatten nested structures, automatically\n",
    "    df = pd.json_normalize(all_features)\n",
    "\n",
    "    # 3. Extract coordinates into separate columns\n",
    "    # geometry.coordintates is [longitude, latitude, depth km]\n",
    "    df['longitude'] = df['geometry.coordinates'].apply(lambda x: x[0] if x else None)\n",
    "    df['latitude'] = df['geometry.coordinates'].apply(lambda x: x[1] if x else None)\n",
    "    df['depth_km'] = df['geometry.coordinates'].apply(lambda x: x[2] if x else None)\n",
    "\n",
    "    # 4. Clean up column names (remove properties prefix)\n",
    "    df.columns = df.columns.str.replace('properties.', '', regex=False)\n",
    "\n",
    "    # 5. Drop the original nested column (geometry.coordinates)\n",
    "    df = df.drop(columns=['geometry.coordinates'], errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"flattening function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baea2c24-4a45-484a-8dc2-8c0e1686a5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw flattened shape: (1590, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>time</th>\n",
       "      <th>updated</th>\n",
       "      <th>tz</th>\n",
       "      <th>url</th>\n",
       "      <th>detail</th>\n",
       "      <th>felt</th>\n",
       "      <th>cdi</th>\n",
       "      <th>mmi</th>\n",
       "      <th>alert</th>\n",
       "      <th>status</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>sig</th>\n",
       "      <th>net</th>\n",
       "      <th>code</th>\n",
       "      <th>ids</th>\n",
       "      <th>sources</th>\n",
       "      <th>types</th>\n",
       "      <th>nst</th>\n",
       "      <th>dmin</th>\n",
       "      <th>rms</th>\n",
       "      <th>gap</th>\n",
       "      <th>magType</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>geometry.type</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>depth_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature</td>\n",
       "      <td>us6000s620</td>\n",
       "      <td>4.10</td>\n",
       "      <td>16 km E of Calingasta, Argentina</td>\n",
       "      <td>1770075292434</td>\n",
       "      <td>1770080742040</td>\n",
       "      <td>None</td>\n",
       "      <td>https://earthquake.usgs.gov/earthquakes/eventp...</td>\n",
       "      <td>https://earthquake.usgs.gov/fdsnws/event/1/que...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>us</td>\n",
       "      <td>6000s620</td>\n",
       "      <td>,us6000s620,</td>\n",
       "      <td>,us,</td>\n",
       "      <td>,origin,phase-data,</td>\n",
       "      <td>27</td>\n",
       "      <td>1.3220</td>\n",
       "      <td>0.79</td>\n",
       "      <td>89.0</td>\n",
       "      <td>mb</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>M 4.1 - 16 km E of Calingasta, Argentina</td>\n",
       "      <td>Point</td>\n",
       "      <td>-69.251700</td>\n",
       "      <td>-31.317600</td>\n",
       "      <td>162.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature</td>\n",
       "      <td>uw62216847</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5 km ESE of Benton City, Washington</td>\n",
       "      <td>1770075173110</td>\n",
       "      <td>1770096410010</td>\n",
       "      <td>None</td>\n",
       "      <td>https://earthquake.usgs.gov/earthquakes/eventp...</td>\n",
       "      <td>https://earthquake.usgs.gov/fdsnws/event/1/que...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>uw</td>\n",
       "      <td>62216847</td>\n",
       "      <td>,uw62216847,</td>\n",
       "      <td>,uw,</td>\n",
       "      <td>,origin,phase-data,</td>\n",
       "      <td>14</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.28</td>\n",
       "      <td>202.0</td>\n",
       "      <td>ml</td>\n",
       "      <td>explosion</td>\n",
       "      <td>M 2.8 Explosion - 5 km ESE of Benton City, Was...</td>\n",
       "      <td>Point</td>\n",
       "      <td>-119.422000</td>\n",
       "      <td>46.234500</td>\n",
       "      <td>-0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature</td>\n",
       "      <td>nc75306276</td>\n",
       "      <td>2.94</td>\n",
       "      <td>5 km SE of San Ramon, CA</td>\n",
       "      <td>1770073598570</td>\n",
       "      <td>1770101775717</td>\n",
       "      <td>None</td>\n",
       "      <td>https://earthquake.usgs.gov/earthquakes/eventp...</td>\n",
       "      <td>https://earthquake.usgs.gov/fdsnws/event/1/que...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>automatic</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>nc</td>\n",
       "      <td>75306276</td>\n",
       "      <td>,nc75306276,us6000s61u,</td>\n",
       "      <td>,nc,us,</td>\n",
       "      <td>,dyfi,focal-mechanism,nearby-cities,origin,pha...</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.10</td>\n",
       "      <td>37.0</td>\n",
       "      <td>ml</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>M 2.9 - 5 km SE of San Ramon, CA</td>\n",
       "      <td>Point</td>\n",
       "      <td>-121.935837</td>\n",
       "      <td>37.754501</td>\n",
       "      <td>7.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature</td>\n",
       "      <td>nc75306271</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4 km ESE of San Ramon, CA</td>\n",
       "      <td>1770073458430</td>\n",
       "      <td>1770134850183</td>\n",
       "      <td>None</td>\n",
       "      <td>https://earthquake.usgs.gov/earthquakes/eventp...</td>\n",
       "      <td>https://earthquake.usgs.gov/fdsnws/event/1/que...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>nc</td>\n",
       "      <td>75306271</td>\n",
       "      <td>,nc75306271,us6000s61t,</td>\n",
       "      <td>,nc,us,</td>\n",
       "      <td>,dyfi,focal-mechanism,nearby-cities,origin,pha...</td>\n",
       "      <td>74</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.10</td>\n",
       "      <td>39.0</td>\n",
       "      <td>ml</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>M 3.0 - 4 km ESE of San Ramon, CA</td>\n",
       "      <td>Point</td>\n",
       "      <td>-121.935333</td>\n",
       "      <td>37.763332</td>\n",
       "      <td>8.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature</td>\n",
       "      <td>nc75306256</td>\n",
       "      <td>3.14</td>\n",
       "      <td>4 km ESE of San Ramon, CA</td>\n",
       "      <td>1770072973340</td>\n",
       "      <td>1770098210597</td>\n",
       "      <td>None</td>\n",
       "      <td>https://earthquake.usgs.gov/earthquakes/eventp...</td>\n",
       "      <td>https://earthquake.usgs.gov/fdsnws/event/1/que...</td>\n",
       "      <td>97.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>nc</td>\n",
       "      <td>75306256</td>\n",
       "      <td>,nc75306256,us6000s61s,</td>\n",
       "      <td>,nc,us,</td>\n",
       "      <td>,dyfi,focal-mechanism,nearby-cities,origin,pha...</td>\n",
       "      <td>80</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.12</td>\n",
       "      <td>26.0</td>\n",
       "      <td>ml</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>M 3.1 - 4 km ESE of San Ramon, CA</td>\n",
       "      <td>Point</td>\n",
       "      <td>-121.937164</td>\n",
       "      <td>37.765167</td>\n",
       "      <td>7.820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type          id   mag                                place           time        updated    tz                                                url  \\\n",
       "0  Feature  us6000s620  4.10     16 km E of Calingasta, Argentina  1770075292434  1770080742040  None  https://earthquake.usgs.gov/earthquakes/eventp...   \n",
       "1  Feature  uw62216847  2.80  5 km ESE of Benton City, Washington  1770075173110  1770096410010  None  https://earthquake.usgs.gov/earthquakes/eventp...   \n",
       "2  Feature  nc75306276  2.94             5 km SE of San Ramon, CA  1770073598570  1770101775717  None  https://earthquake.usgs.gov/earthquakes/eventp...   \n",
       "3  Feature  nc75306271  3.00            4 km ESE of San Ramon, CA  1770073458430  1770134850183  None  https://earthquake.usgs.gov/earthquakes/eventp...   \n",
       "4  Feature  nc75306256  3.14            4 km ESE of San Ramon, CA  1770072973340  1770098210597  None  https://earthquake.usgs.gov/earthquakes/eventp...   \n",
       "\n",
       "                                              detail  felt  cdi  mmi alert     status  tsunami  sig net      code                      ids  sources  \\\n",
       "0  https://earthquake.usgs.gov/fdsnws/event/1/que...   NaN  NaN  NaN  None   reviewed        0  259  us  6000s620             ,us6000s620,     ,us,   \n",
       "1  https://earthquake.usgs.gov/fdsnws/event/1/que...   NaN  NaN  NaN  None   reviewed        0  121  uw  62216847             ,uw62216847,     ,uw,   \n",
       "2  https://earthquake.usgs.gov/fdsnws/event/1/que...  10.0  3.3  NaN  None  automatic        0  136  nc  75306276  ,nc75306276,us6000s61u,  ,nc,us,   \n",
       "3  https://earthquake.usgs.gov/fdsnws/event/1/que...  25.0  4.1  NaN  None   reviewed        0  149  nc  75306271  ,nc75306271,us6000s61t,  ,nc,us,   \n",
       "4  https://earthquake.usgs.gov/fdsnws/event/1/que...  97.0  3.4  NaN  None   reviewed        0  185  nc  75306256  ,nc75306256,us6000s61s,  ,nc,us,   \n",
       "\n",
       "                                               types  nst    dmin   rms    gap magType        type                                              title geometry.type   longitude  \\\n",
       "0                                ,origin,phase-data,   27  1.3220  0.79   89.0      mb  earthquake           M 4.1 - 16 km E of Calingasta, Argentina         Point  -69.251700   \n",
       "1                                ,origin,phase-data,   14  0.1610  0.28  202.0      ml   explosion  M 2.8 Explosion - 5 km ESE of Benton City, Was...         Point -119.422000   \n",
       "2  ,dyfi,focal-mechanism,nearby-cities,origin,pha...   64  0.1036  0.10   37.0      ml  earthquake                   M 2.9 - 5 km SE of San Ramon, CA         Point -121.935837   \n",
       "3  ,dyfi,focal-mechanism,nearby-cities,origin,pha...   74  0.1056  0.10   39.0      ml  earthquake                  M 3.0 - 4 km ESE of San Ramon, CA         Point -121.935333   \n",
       "4  ,dyfi,focal-mechanism,nearby-cities,origin,pha...   80  0.1033  0.12   26.0      ml  earthquake                  M 3.1 - 4 km ESE of San Ramon, CA         Point -121.937164   \n",
       "\n",
       "    latitude  depth_km  \n",
       "0 -31.317600   162.179  \n",
       "1  46.234500    -0.240  \n",
       "2  37.754501     7.370  \n",
       "3  37.763332     8.380  \n",
       "4  37.765167     7.820  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw = features_to_df(pages)\n",
    "print(f\"Raw flattened shape: {df_raw.shape}\")\n",
    "display(df_raw.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdab803-d015-4009-8abb-f8fb3fc9d902",
   "metadata": {},
   "source": [
    "### Parse types and extract coordinates  \n",
    "\n",
    "Create a staged table with columns:  \n",
    "* `event_id`: string\n",
    "* `time_utc`: datetime\n",
    "* `updated_utc`: datetime\n",
    "* `mag`: float\n",
    "* `place`: string\n",
    "* `longitude`: float\n",
    "* `latitude`: float\n",
    "* `depth_km`: float\n",
    "* `tsunami`: int\n",
    "* `status`: string\n",
    "\n",
    "USGS times are often **milliseconds since epoch**  \n",
    "\n",
    "**Note:** `pd.to_datetime(ms, unit=\"ms\", utc=True)  \n",
    "\n",
    "Also:\n",
    "* Deduplicate by `event_id` (keep first)\n",
    "* Normalize missing-like strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c94593-d99a-4ec4-877d-5d2d0b86f0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function to create staged DataFrame ready\n"
     ]
    }
   ],
   "source": [
    "# Build staged dataframe\n",
    "def staged_dataframe(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    MISSING_TOKENS = {\"\", \"nan\", \"None\", \"UNKNOWN\", \"Unknown\", \"N/A\", \"NA\", \"NULL\", \"null\"}\n",
    "    \n",
    "    staged = pd.DataFrame()\n",
    "\n",
    "    # 1. event_id: basic identification - rename and ensure string type\n",
    "    staged['event_id'] = data['id'].astype(str)\n",
    "\n",
    "    # 2. time_utc: parse timestamps from unix milliseconds to datetime\n",
    "    staged['time_utc'] = pd.to_datetime(data['time'], unit='ms', utc=True)\n",
    "\n",
    "    # 3. updated_utc: parse timestamps from unix milliseconds to datetime\n",
    "    staged['updated_utc'] = pd.to_datetime(data['updated'], unit='ms', utc=True)\n",
    "\n",
    "    # 4. mag: handle missing values and convert to float (coerce = NaN)\n",
    "    staged['mag'] = pd.to_numeric(data['mag'], errors='coerce')\n",
    "\n",
    "    # 5. place: location description - ensure string type\n",
    "    staged['place'] = data['place'].astype(str)\n",
    "\n",
    "    # 6. coordinates (longitude, latitude, depth_km): already extracted, just add and handle missing values\n",
    "    staged['longitude'] = pd.to_numeric(data['longitude'], errors='coerce')\n",
    "    staged['latitude'] = pd.to_numeric(data['latitude'], errors='coerce')\n",
    "    staged['depth_km'] = pd.to_numeric(data['depth_km'], errors='coerce')\n",
    "\n",
    "    # 7. tsunami: flag to indicate a tsunami - convert to int, fill missing with 0\n",
    "    staged['tsunami'] = pd.to_numeric(data['tsunami'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # 8. status: review status - convert to string\n",
    "    staged['status'] = data['status'].astype(str)\n",
    "\n",
    "    # 9. Normalize missing-like strings to NaN for all string columns\n",
    "    for col in staged.columns:\n",
    "        if staged[col].dtype == 'object':  # string columns have dtype 'object'\n",
    "            staged[col] = staged[col].replace(MISSING_TOKENS, np.nan)\n",
    "\n",
    "    # 10. Delete duplicate entries by event_id. Keep the first entry\n",
    "    entries_prior = len(staged)\n",
    "    staged = staged.sort_values('time_utc', ascending=False)\n",
    "    staged = staged.drop_duplicates(subset=['event_id'], keep='first')\n",
    "    entries_after = len(staged)\n",
    "    print(f\"Deduped: {entries_prior} -> {entries_after}\")\n",
    "\n",
    "    return staged\n",
    "\n",
    "print(\"function to create staged DataFrame ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88fe5d00-fc56-4a90-a8b6-4eef6ca68f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduped: 1590 -> 1590\n",
      "Staged shape: (1590, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>time_utc</th>\n",
       "      <th>updated_utc</th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>depth_km</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us6000s620</td>\n",
       "      <td>2026-02-02 23:34:52.434000+00:00</td>\n",
       "      <td>2026-02-03 01:05:42.040000+00:00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>16 km E of Calingasta, Argentina</td>\n",
       "      <td>-69.251700</td>\n",
       "      <td>-31.317600</td>\n",
       "      <td>162.179</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uw62216847</td>\n",
       "      <td>2026-02-02 23:32:53.110000+00:00</td>\n",
       "      <td>2026-02-03 05:26:50.010000+00:00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5 km ESE of Benton City, Washington</td>\n",
       "      <td>-119.422000</td>\n",
       "      <td>46.234500</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nc75306276</td>\n",
       "      <td>2026-02-02 23:06:38.570000+00:00</td>\n",
       "      <td>2026-02-03 06:56:15.717000+00:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>5 km SE of San Ramon, CA</td>\n",
       "      <td>-121.935837</td>\n",
       "      <td>37.754501</td>\n",
       "      <td>7.370</td>\n",
       "      <td>0</td>\n",
       "      <td>automatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nc75306271</td>\n",
       "      <td>2026-02-02 23:04:18.430000+00:00</td>\n",
       "      <td>2026-02-03 16:07:30.183000+00:00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4 km ESE of San Ramon, CA</td>\n",
       "      <td>-121.935333</td>\n",
       "      <td>37.763332</td>\n",
       "      <td>8.380</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nc75306256</td>\n",
       "      <td>2026-02-02 22:56:13.340000+00:00</td>\n",
       "      <td>2026-02-03 05:56:50.597000+00:00</td>\n",
       "      <td>3.14</td>\n",
       "      <td>4 km ESE of San Ramon, CA</td>\n",
       "      <td>-121.937164</td>\n",
       "      <td>37.765167</td>\n",
       "      <td>7.820</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     event_id                         time_utc                      updated_utc   mag                                place   longitude   latitude  depth_km  tsunami     status\n",
       "0  us6000s620 2026-02-02 23:34:52.434000+00:00 2026-02-03 01:05:42.040000+00:00  4.10     16 km E of Calingasta, Argentina  -69.251700 -31.317600   162.179        0   reviewed\n",
       "1  uw62216847 2026-02-02 23:32:53.110000+00:00 2026-02-03 05:26:50.010000+00:00  2.80  5 km ESE of Benton City, Washington -119.422000  46.234500    -0.240        0   reviewed\n",
       "2  nc75306276 2026-02-02 23:06:38.570000+00:00 2026-02-03 06:56:15.717000+00:00  2.94             5 km SE of San Ramon, CA -121.935837  37.754501     7.370        0  automatic\n",
       "3  nc75306271 2026-02-02 23:04:18.430000+00:00 2026-02-03 16:07:30.183000+00:00  3.00            4 km ESE of San Ramon, CA -121.935333  37.763332     8.380        0   reviewed\n",
       "4  nc75306256 2026-02-02 22:56:13.340000+00:00 2026-02-03 05:56:50.597000+00:00  3.14            4 km ESE of San Ramon, CA -121.937164  37.765167     7.820        0   reviewed"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_staged = staged_dataframe(df_raw)\n",
    "print(f\"Staged shape: {df_staged.shape}\")\n",
    "display(df_staged.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0885431-f6aa-41c8-bfe8-6d3354401095",
   "metadata": {},
   "source": [
    "### Write staged output  \n",
    "\n",
    "Write:  \n",
    "* `data/staged/usgs_earthquakes_staged_<runid>.parquet`\n",
    "* `data/staged/usgs_staged_meta_<runid>.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "511189e6-aeee-424c-92df-a3333080c11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: work/m2_project/data/staged/usgs_earthquakes_staged_20260203_171853_utc.parquet\n",
      "Wrote: work/m2_project/data/staged/usgs_staged_meta_20260203_171853_utc.json\n"
     ]
    }
   ],
   "source": [
    "# Determine file paths\n",
    "staged_parquet_path = STAGED_DIR / f\"usgs_earthquakes_staged_{RUN_ID}.parquet\"\n",
    "staged_metadata_path = STAGED_DIR / f\"usgs_staged_meta_{RUN_ID}.json\"\n",
    "\n",
    "# Create staged metadata\n",
    "staged_metadata = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"pipeline_stage\": \"staged\",\n",
    "\n",
    "    \"input\": {\n",
    "        \"source\": \"USGS Earthquake API\",\n",
    "        \"api_endpoint\": USGS_URL,\n",
    "        \"raw_records_fetched\": len(df_raw),\n",
    "    },\n",
    "\n",
    "    \"output\": {\n",
    "        \"parquet_file\": staged_parquet_path.name,\n",
    "        \"parquet_path\": str(staged_parquet_path),\n",
    "        \"rows\": len(df_staged),\n",
    "        \"columns\": len(df_staged.columns),\n",
    "        \"column_list\": df_staged.columns.tolist(),\n",
    "    },\n",
    "\n",
    "    \"schema\": {\n",
    "        col: str(dtype) for col, dtype in df_staged.dtypes.items()\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "# Write outputs\n",
    "df_staged.to_parquet(staged_parquet_path, index=False)\n",
    "print(f\"Wrote: {staged_parquet_path}\")\n",
    "with open(staged_metadata_path, 'w') as f:\n",
    "    json.dump(staged_metadata, f, indent=2)\n",
    "print(f\"Wrote: {staged_metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f941168-836b-41f3-80cf-fe40a32ff95e",
   "metadata": {},
   "source": [
    "## 3 - Curate: Build Analysis-Ready Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03487f1a-4985-40cc-ac4a-004e21b3f21a",
   "metadata": {},
   "source": [
    "Create a curated table with:  \n",
    "* time features: `event_day`, `event_hour`, `dayofweek`, `is_weekend`\n",
    "* magnitude bins and flags\n",
    "* a lightweight region extraction from place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8e65b-5c36-4f91-b74a-b5b46cec9205",
   "metadata": {},
   "source": [
    "### Feature Construction  \n",
    "\n",
    "Implement:\n",
    "* `event_day`: date(UTC)\n",
    "* `event_hour`: hour(UTC)\n",
    "* `mag_bin`: (example bins): <2.5, 2.5-4.5, 4.5-6.0, 6.0+\n",
    "* `is_major`: mag >= 6.0\n",
    "* `region`: parsed from `place`:\n",
    "   * if `place` contains `\" of \"`, take the substring after the last `\" of \"`\n",
    "   * otherwise use the full `place`\n",
    "\n",
    "**Note:**\n",
    "`str.rsplit(\" of\", 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd095311-e875-4628-ae2b-51b31aecdd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region parser function ready\n"
     ]
    }
   ],
   "source": [
    "def parse_region(place: str) -> str:\n",
    "    if pd.isna(place):\n",
    "        return None\n",
    "\n",
    "    if \" of \" in place:\n",
    "        return place.rsplit(\" of \", 1)[1]\n",
    "    else:\n",
    "        return place\n",
    "\n",
    "print(\"Region parser function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68ab848a-57d8-448c-a3d7-ac5a58c21ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curated dataframe function ready\n"
     ]
    }
   ],
   "source": [
    "def curated_dataframe(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Build a curated DataFrame\n",
    "    df_curated = data.copy()\n",
    "    \n",
    "    # 1. Extract date (removes time component)\n",
    "    df_curated['event_day'] = df_curated['time_utc'].dt.date\n",
    "    \n",
    "    # 2. Extract hour\n",
    "    df_curated['event_hour'] = df_curated['time_utc'].dt.hour\n",
    "    \n",
    "    # 3. Extract day of week\n",
    "    df_curated['dayofweek'] = df_curated['time_utc'].dt.dayofweek\n",
    "    \n",
    "    # 4. Weekend flag\n",
    "    df_curated['is_weekend'] = df_curated['dayofweek'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # 5. Magnitude bins\n",
    "    # define\n",
    "    bins = [-float('inf'), 2.5, 4.5, 6.0, float('inf')]\n",
    "    labels = ['<2.5', '2.5-4.5', '4.5-6.0', '6.0+']\n",
    "    # add\n",
    "    df_curated['mag_bin'] = pd.cut(df_curated['mag'], bins=bins, labels=labels)\n",
    "    \n",
    "    # 6. Flag for major earthquakes (>6.0)\n",
    "    df_curated['is_major'] = (df_curated['mag'] >- 6.0).astype(int)\n",
    "    \n",
    "    # 7. Parse region\n",
    "    df_curated['region'] = df_curated['place'].apply(parse_region)\n",
    "\n",
    "    return df_curated\n",
    "\n",
    "print(\"curated dataframe function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fbfef7f-fe0a-468c-aeff-f30d1e1edaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated shape: (1590, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>time_utc</th>\n",
       "      <th>updated_utc</th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>depth_km</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>status</th>\n",
       "      <th>event_day</th>\n",
       "      <th>event_hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>mag_bin</th>\n",
       "      <th>is_major</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us6000s620</td>\n",
       "      <td>2026-02-02 23:34:52.434000+00:00</td>\n",
       "      <td>2026-02-03 01:05:42.040000+00:00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>16 km E of Calingasta, Argentina</td>\n",
       "      <td>-69.251700</td>\n",
       "      <td>-31.317600</td>\n",
       "      <td>162.179</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>Calingasta, Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uw62216847</td>\n",
       "      <td>2026-02-02 23:32:53.110000+00:00</td>\n",
       "      <td>2026-02-03 05:26:50.010000+00:00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5 km ESE of Benton City, Washington</td>\n",
       "      <td>-119.422000</td>\n",
       "      <td>46.234500</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>Benton City, Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nc75306276</td>\n",
       "      <td>2026-02-02 23:06:38.570000+00:00</td>\n",
       "      <td>2026-02-03 06:56:15.717000+00:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>5 km SE of San Ramon, CA</td>\n",
       "      <td>-121.935837</td>\n",
       "      <td>37.754501</td>\n",
       "      <td>7.370</td>\n",
       "      <td>0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>San Ramon, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nc75306271</td>\n",
       "      <td>2026-02-02 23:04:18.430000+00:00</td>\n",
       "      <td>2026-02-03 16:07:30.183000+00:00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4 km ESE of San Ramon, CA</td>\n",
       "      <td>-121.935333</td>\n",
       "      <td>37.763332</td>\n",
       "      <td>8.380</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>San Ramon, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nc75306256</td>\n",
       "      <td>2026-02-02 22:56:13.340000+00:00</td>\n",
       "      <td>2026-02-03 05:56:50.597000+00:00</td>\n",
       "      <td>3.14</td>\n",
       "      <td>4 km ESE of San Ramon, CA</td>\n",
       "      <td>-121.937164</td>\n",
       "      <td>37.765167</td>\n",
       "      <td>7.820</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>San Ramon, CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     event_id                         time_utc                      updated_utc   mag                                place   longitude   latitude  depth_km  tsunami     status  \\\n",
       "0  us6000s620 2026-02-02 23:34:52.434000+00:00 2026-02-03 01:05:42.040000+00:00  4.10     16 km E of Calingasta, Argentina  -69.251700 -31.317600   162.179        0   reviewed   \n",
       "1  uw62216847 2026-02-02 23:32:53.110000+00:00 2026-02-03 05:26:50.010000+00:00  2.80  5 km ESE of Benton City, Washington -119.422000  46.234500    -0.240        0   reviewed   \n",
       "2  nc75306276 2026-02-02 23:06:38.570000+00:00 2026-02-03 06:56:15.717000+00:00  2.94             5 km SE of San Ramon, CA -121.935837  37.754501     7.370        0  automatic   \n",
       "3  nc75306271 2026-02-02 23:04:18.430000+00:00 2026-02-03 16:07:30.183000+00:00  3.00            4 km ESE of San Ramon, CA -121.935333  37.763332     8.380        0   reviewed   \n",
       "4  nc75306256 2026-02-02 22:56:13.340000+00:00 2026-02-03 05:56:50.597000+00:00  3.14            4 km ESE of San Ramon, CA -121.937164  37.765167     7.820        0   reviewed   \n",
       "\n",
       "    event_day  event_hour  dayofweek  is_weekend  mag_bin  is_major                   region  \n",
       "0  2026-02-02          23          0           0  2.5-4.5         1    Calingasta, Argentina  \n",
       "1  2026-02-02          23          0           0  2.5-4.5         1  Benton City, Washington  \n",
       "2  2026-02-02          23          0           0  2.5-4.5         1            San Ramon, CA  \n",
       "3  2026-02-02          23          0           0  2.5-4.5         1            San Ramon, CA  \n",
       "4  2026-02-02          22          0           0  2.5-4.5         1            San Ramon, CA  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_curated = curated_dataframe(df_staged)\n",
    "print(f\"Curated shape: {df_curated.shape}\")\n",
    "display(df_curated.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb3f7-0a6b-4565-96f5-6340606ef982",
   "metadata": {},
   "source": [
    "### Choose curated columns and write Parquet  \n",
    "\n",
    "Write:  \n",
    "* `data/warehouse/usgs_earthquakes_curated_<runid>.parquet`\n",
    "* partition by `event_day` under `data/warehouse/partitions/event_day=YYYY-MM-DD/earthquakes.parquet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f37ef4a-c992-48cc-8acf-0f4e9ec529b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote curated parquet: usgs_earthquakes_curated_20260203_171853_utc.parquet\n",
      "Wrote 30 partitions to m2_project/data/warehouse/partitions\n"
     ]
    }
   ],
   "source": [
    "# Select curated columns\n",
    "curated_columns = [\n",
    "    # Identification\n",
    "    \"event_id\",\n",
    "\n",
    "    # Time features\n",
    "    \"time_utc\",\n",
    "    \"event_day\",\n",
    "    \"event_hour\",\n",
    "    \"dayofweek\",\n",
    "    \"is_weekend\",\n",
    "\n",
    "    # Location\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"depth_km\",\n",
    "    \"place\",\n",
    "    \"region\",\n",
    "\n",
    "    # Magnitude\n",
    "    \"mag\",\n",
    "    \"mag_bin\",\n",
    "    \"is_major\",\n",
    "\n",
    "    # Impact/flags\n",
    "    \"tsunami\",\n",
    "    \"status\",\n",
    "]\n",
    "\n",
    "# Create final curated datafram with selected columns and write\n",
    "df_curated_final = df_curated[curated_columns].copy()\n",
    "curated_parquet_path = WH_DIR / f\"usgs_earthquakes_curated_{RUN_ID}.parquet\"\n",
    "df_curated_final.to_parquet(curated_parquet_path, index=False)\n",
    "print(f\"Wrote curated parquet: {curated_parquet_path.name}\")\n",
    "\n",
    "# Write partitioned parquet\n",
    "PART_DIR = WH_DIR / \"partitions\"\n",
    "PART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "partition_count = 0\n",
    "for day, day_data in df_curated_final.groupby('event_day'):\n",
    "    day_str = str(day)\n",
    "    partition_dir = PART_DIR / f\"event_day={day_str}\"\n",
    "    partition_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    partition_path = partition_dir / \"earthquakes.parquet\"\n",
    "    day_data.to_parquet(partition_path, index=False)\n",
    "    partition_count += 1\n",
    "\n",
    "print(f\"Wrote {partition_count} partitions to {PART_DIR.relative_to(WORK_DIR)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9128558-d55b-414c-84e4-c5cb429d87ef",
   "metadata": {},
   "source": [
    "## 4 - Validate: Contracts + Anomalies + Canaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba4250-0f19-4001-99b5-2d49529ba568",
   "metadata": {},
   "source": [
    "### Define Contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb76356b-81e9-42f3-9884-65f906c71a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contracts defined:\n",
      "  Required cols: {'longitude', 'time_utc', 'depth_km', 'mag', 'event_id', 'latitude'}\n",
      "  Magnitude range: (-1.0, 10.0)\n",
      "  Depth range (km): (-10.0, 700.0)\n"
     ]
    }
   ],
   "source": [
    "# Determine required columns\n",
    "REQUIRED_COLS = {\n",
    "    \"event_id\",\n",
    "    \"time_utc\",\n",
    "    \"mag\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"depth_km\",\n",
    "}\n",
    "\n",
    "# Determine valid ranges\n",
    "MAG_MIN = -1.0\n",
    "MAG_MAX = 10.0\n",
    "DEPTH_MIN = -10.0\n",
    "DEPTH_MAX = 700.0\n",
    "\n",
    "print(\"Contracts defined:\")\n",
    "print(f\"  Required cols: {REQUIRED_COLS}\")\n",
    "print(f\"  Magnitude range: ({MAG_MIN}, {MAG_MAX})\")\n",
    "print(f\"  Depth range (km): ({DEPTH_MIN}, {DEPTH_MAX})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d7fd8-649c-48e1-9cef-33b5a424d0b1",
   "metadata": {},
   "source": [
    "### Implement validation checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d76e476-8836-4352-9943-dc5f73fde436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation function ready\n"
     ]
    }
   ],
   "source": [
    "def validate_contracts(data: pd.DataFrame) -> dict:\n",
    "    results = {\"passed\": True, \"checks\": []}\n",
    "\n",
    "    def record_check(name: str, passed: bool, details: str):\n",
    "        results[\"checks\"].append({\n",
    "            \"name\": name,\n",
    "            \"passed\": passed,\n",
    "            \"details\": details\n",
    "        })\n",
    "        if not passed:\n",
    "            results[\"passed\"] = False\n",
    "\n",
    "    # 1. Required columns exist\n",
    "    missing_cols = set(REQUIRED_COLS) - set(data.columns)\n",
    "    record_check(\n",
    "        \"required_columns\",\n",
    "        len(missing_cols) == 0,\n",
    "        f\"Missing columns: {list(missing_cols)}\" if missing_cols else \"All required columns present\"\n",
    "    )\n",
    "\n",
    "    # 2. Non-empty dataset\n",
    "    record_check(\n",
    "        \"non_empty\",\n",
    "        len(data) > 0,\n",
    "        f\"Dataset has {len(data)} rows\"\n",
    "    )\n",
    "\n",
    "    # 3. event_id unique\n",
    "    duplicate_count = data['event_id'].duplicated().sum()\n",
    "    record_check(\n",
    "        \"event_id_unique\",\n",
    "        duplicate_count == 0,\n",
    "        f\"Found {duplicate_count} duplicate event_ids\" if duplicate_count > 0 else \"All event_ids unique\"\n",
    "    )\n",
    "\n",
    "    # 4. Latitude range\n",
    "    invalid_lat = (~data['latitude'].between(-90, 90)).sum()\n",
    "    record_check(\n",
    "        \"latitude_range\",\n",
    "        invalid_lat == 0,\n",
    "        f\"Found {invalid_lat} invalid latitudes\" if invalid_lat > 0 else \"All latitudes in valid range [-90, 90]\"\n",
    "    )\n",
    "\n",
    "    # 5. Longitude range\n",
    "    invalid_lon = (~data['longitude'].between(-180, 180)).sum()\n",
    "    record_check(\n",
    "        \"longitude_range\",\n",
    "        invalid_lon == 0,\n",
    "        f\"Found {invalid_lon} invalid longitudes\" if invalid_lon > 0 else \"All longitudes in valid range [-180, 180]\"\n",
    "    )\n",
    "\n",
    "    # 6. Magnitude range\n",
    "    invalid_mag = (~data['mag'].between(MAG_MIN, MAG_MAX)).sum()\n",
    "    record_check(\n",
    "        \"magnitude_range\",\n",
    "        invalid_mag == 0,\n",
    "        f\"Found {invalid_mag} magnitudes outside [{MAG_MIN}, {MAG_MAX}]\" if invalid_mag > 0 else f\"All magnitudes in valid range [{MAG_MIN}, {MAG_MAX}]\"\n",
    "    )\n",
    "\n",
    "    # 7. Depth Range\n",
    "    invalid_depth = (~data['depth_km'].between(DEPTH_MIN, DEPTH_MAX)).sum()\n",
    "    record_check(\n",
    "        \"depth_range\",\n",
    "        invalid_depth == 0,\n",
    "        f\"Found {invalid_depth} depths outside [{DEPTH_MIN}, {DEPTH_MAX}] km\" if invalid_depth > 0 else f\"All depths in valid range [{DEPTH_MIN}, {DEPTH_MAX}] km\"\n",
    "    )\n",
    "\n",
    "    # 8. No future events\n",
    "    now = pd.Timestamp.now(tz='UTC')\n",
    "    future_events = (data['time_utc'] > now).sum()\n",
    "    record_check(\n",
    "        \"no_future_events\",\n",
    "        future_events == 0,\n",
    "        f\"Found {future_events} events in the future\" if future_events > 0 else \"No future events detected\"\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Validation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58dcf5f9-bc3a-4761-b2b5-12ff24182a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation passed? True\n",
      "Failures: 0\n"
     ]
    }
   ],
   "source": [
    "validation_results = validate_contracts(df_curated_final)\n",
    "\n",
    "failures = sum(1 for check in validation_results[\"checks\"] if not check[\"passed\"])\n",
    "\n",
    "print(f\"\\nValidation passed? {validation_results['passed']}\")\n",
    "print(f\"Failures: {failures}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b05ab-38a9-4228-89be-5f760bc03215",
   "metadata": {},
   "source": [
    "### Anomaly flags and investigation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88c994f7-86be-41ff-8f56-cca3f454b648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suspicious rows: 38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>time_utc</th>\n",
       "      <th>mag</th>\n",
       "      <th>depth_km</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region</th>\n",
       "      <th>place</th>\n",
       "      <th>anom_mag_missing</th>\n",
       "      <th>anom_mag_extreme</th>\n",
       "      <th>anom_depth_negative</th>\n",
       "      <th>anom_depth_extreme</th>\n",
       "      <th>anom_coords_missing</th>\n",
       "      <th>anom_any</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uw62216847</td>\n",
       "      <td>2026-02-02 23:32:53.110000+00:00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>46.234500</td>\n",
       "      <td>-119.422000</td>\n",
       "      <td>Benton City, Washington</td>\n",
       "      <td>5 km ESE of Benton City, Washington</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>us6000s5vj</td>\n",
       "      <td>2026-02-01 18:09:34.279000+00:00</td>\n",
       "      <td>4.70</td>\n",
       "      <td>527.156</td>\n",
       "      <td>-23.597700</td>\n",
       "      <td>179.933700</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>us6000s5s2</td>\n",
       "      <td>2026-01-31 20:08:33.970000+00:00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>507.818</td>\n",
       "      <td>-24.978100</td>\n",
       "      <td>179.786200</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>us6000s5rp</td>\n",
       "      <td>2026-01-31 18:34:43.926000+00:00</td>\n",
       "      <td>4.80</td>\n",
       "      <td>584.727</td>\n",
       "      <td>6.738000</td>\n",
       "      <td>123.626900</td>\n",
       "      <td>Gadung, Philippines</td>\n",
       "      <td>44 km W of Gadung, Philippines</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>us6000s5rc</td>\n",
       "      <td>2026-01-31 15:59:11.987000+00:00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>530.235</td>\n",
       "      <td>-20.003200</td>\n",
       "      <td>-177.552700</td>\n",
       "      <td>Houma, Tonga</td>\n",
       "      <td>267 km WNW of Houma, Tonga</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>us6000s5bj</td>\n",
       "      <td>2026-01-29 20:13:42.056000+00:00</td>\n",
       "      <td>4.70</td>\n",
       "      <td>544.114</td>\n",
       "      <td>-20.525300</td>\n",
       "      <td>-178.214000</td>\n",
       "      <td>Fiji region</td>\n",
       "      <td>Fiji region</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>uw62216317</td>\n",
       "      <td>2026-01-27 22:26:04.650000+00:00</td>\n",
       "      <td>2.61</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>42.653833</td>\n",
       "      <td>-124.439167</td>\n",
       "      <td>Port Orford, Oregon</td>\n",
       "      <td>11 km SSE of Port Orford, Oregon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>uw62216302</td>\n",
       "      <td>2026-01-27 21:25:04.700000+00:00</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>42.644500</td>\n",
       "      <td>-124.445667</td>\n",
       "      <td>Port Orford, Oregon</td>\n",
       "      <td>12 km SSE of Port Orford, Oregon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>us6000s4p9</td>\n",
       "      <td>2026-01-27 08:31:31.478000+00:00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>573.799</td>\n",
       "      <td>-4.367600</td>\n",
       "      <td>149.686000</td>\n",
       "      <td>Kimbe, Papua New Guinea</td>\n",
       "      <td>140 km NNW of Kimbe, Papua New Guinea</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>us6000s4p3</td>\n",
       "      <td>2026-01-27 07:54:37.140000+00:00</td>\n",
       "      <td>4.20</td>\n",
       "      <td>572.586</td>\n",
       "      <td>-23.433300</td>\n",
       "      <td>178.866100</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>us7000rrww</td>\n",
       "      <td>2026-01-25 04:46:08.098000+00:00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>577.764</td>\n",
       "      <td>-19.891400</td>\n",
       "      <td>-178.277900</td>\n",
       "      <td>Fiji region</td>\n",
       "      <td>Fiji region</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>us7000rrwm</td>\n",
       "      <td>2026-01-25 04:13:02.738000+00:00</td>\n",
       "      <td>5.20</td>\n",
       "      <td>509.606</td>\n",
       "      <td>-25.230000</td>\n",
       "      <td>179.685700</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>us7000rrst</td>\n",
       "      <td>2026-01-24 12:12:41.564000+00:00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>590.366</td>\n",
       "      <td>-22.666500</td>\n",
       "      <td>-179.718600</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>us7000rrsg</td>\n",
       "      <td>2026-01-24 10:05:53.405000+00:00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>587.326</td>\n",
       "      <td>-21.072200</td>\n",
       "      <td>-178.758300</td>\n",
       "      <td>Fiji region</td>\n",
       "      <td>Fiji region</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>us7000rqsi</td>\n",
       "      <td>2026-01-20 08:45:24.268000+00:00</td>\n",
       "      <td>4.70</td>\n",
       "      <td>527.005</td>\n",
       "      <td>-23.697000</td>\n",
       "      <td>-179.959800</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>us7000rql6</td>\n",
       "      <td>2026-01-19 17:30:06.258000+00:00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>526.354</td>\n",
       "      <td>-17.968500</td>\n",
       "      <td>-178.358900</td>\n",
       "      <td>Levuka, Fiji</td>\n",
       "      <td>246 km E of Levuka, Fiji</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>us7000rpyy</td>\n",
       "      <td>2026-01-15 22:23:40.622000+00:00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>530.955</td>\n",
       "      <td>-24.060700</td>\n",
       "      <td>179.768500</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>us6000s5kj</td>\n",
       "      <td>2026-01-14 17:14:31.874000+00:00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>525.656</td>\n",
       "      <td>-20.350100</td>\n",
       "      <td>-177.793100</td>\n",
       "      <td>Houma, Tonga</td>\n",
       "      <td>274 km WNW of Houma, Tonga</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>us7000rpn6</td>\n",
       "      <td>2026-01-14 16:21:08.725000+00:00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>529.997</td>\n",
       "      <td>-23.595800</td>\n",
       "      <td>179.531300</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>us6000s5j2</td>\n",
       "      <td>2026-01-13 23:42:25.546000+00:00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>591.174</td>\n",
       "      <td>-17.821600</td>\n",
       "      <td>-178.703100</td>\n",
       "      <td>Levuka, Fiji</td>\n",
       "      <td>211 km E of Levuka, Fiji</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>us7000rpci</td>\n",
       "      <td>2026-01-13 07:38:07.943000+00:00</td>\n",
       "      <td>4.60</td>\n",
       "      <td>566.522</td>\n",
       "      <td>-18.056500</td>\n",
       "      <td>-178.248500</td>\n",
       "      <td>Levuka, Fiji</td>\n",
       "      <td>257 km E of Levuka, Fiji</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>us6000s5gy</td>\n",
       "      <td>2026-01-12 21:49:42.695000+00:00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>550.704</td>\n",
       "      <td>-4.296900</td>\n",
       "      <td>128.556800</td>\n",
       "      <td>Ambon, Indonesia</td>\n",
       "      <td>78 km SSE of Ambon, Indonesia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>ci41155815</td>\n",
       "      <td>2026-01-11 23:54:22.580000+00:00</td>\n",
       "      <td>4.11</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>31.537333</td>\n",
       "      <td>-115.697500</td>\n",
       "      <td>Maneadero, B.C., MX</td>\n",
       "      <td>85 km ESE of Maneadero, B.C., MX</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>us6000s527</td>\n",
       "      <td>2026-01-10 20:26:52.781000+00:00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>548.652</td>\n",
       "      <td>-23.470800</td>\n",
       "      <td>-179.776600</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>us7000rny2</td>\n",
       "      <td>2026-01-10 16:11:38.021000+00:00</td>\n",
       "      <td>4.70</td>\n",
       "      <td>531.072</td>\n",
       "      <td>-25.948800</td>\n",
       "      <td>179.012600</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>us7000rntz</td>\n",
       "      <td>2026-01-10 03:39:59.320000+00:00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>572.110</td>\n",
       "      <td>-7.116200</td>\n",
       "      <td>120.471700</td>\n",
       "      <td>Ruteng, Indonesia</td>\n",
       "      <td>165 km N of Ruteng, Indonesia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>us6000s50q</td>\n",
       "      <td>2026-01-09 20:11:50.392000+00:00</td>\n",
       "      <td>4.60</td>\n",
       "      <td>541.207</td>\n",
       "      <td>-24.535600</td>\n",
       "      <td>179.692100</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>us6000s50z</td>\n",
       "      <td>2026-01-09 17:44:23.832000+00:00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>589.158</td>\n",
       "      <td>-17.480300</td>\n",
       "      <td>-178.615200</td>\n",
       "      <td>Levuka, Fiji</td>\n",
       "      <td>228 km ENE of Levuka, Fiji</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>us7000rnea</td>\n",
       "      <td>2026-01-08 06:41:45.111000+00:00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>619.726</td>\n",
       "      <td>-14.668700</td>\n",
       "      <td>170.101100</td>\n",
       "      <td>Sola, Vanuatu</td>\n",
       "      <td>288 km ESE of Sola, Vanuatu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>us7000rnbi</td>\n",
       "      <td>2026-01-07 22:00:12.599000+00:00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>538.866</td>\n",
       "      <td>-23.421500</td>\n",
       "      <td>-179.921000</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>ci41153359</td>\n",
       "      <td>2026-01-07 05:22:22.810000+00:00</td>\n",
       "      <td>3.37</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>31.515833</td>\n",
       "      <td>-115.665500</td>\n",
       "      <td>Maneadero, B.C., MX</td>\n",
       "      <td>88 km ESE of Maneadero, B.C., MX</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>us6000s4rw</td>\n",
       "      <td>2026-01-07 02:55:51.295000+00:00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>583.721</td>\n",
       "      <td>-17.644800</td>\n",
       "      <td>-178.917600</td>\n",
       "      <td>Levuka, Fiji</td>\n",
       "      <td>192 km ENE of Levuka, Fiji</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>us7000rmug</td>\n",
       "      <td>2026-01-06 09:53:23.183000+00:00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>554.172</td>\n",
       "      <td>-27.164700</td>\n",
       "      <td>-63.420200</td>\n",
       "      <td>El Hoyo, Argentina</td>\n",
       "      <td>23 km SW of El Hoyo, Argentina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>us7000rmuf</td>\n",
       "      <td>2026-01-06 09:47:16.539000+00:00</td>\n",
       "      <td>4.20</td>\n",
       "      <td>516.261</td>\n",
       "      <td>-24.588900</td>\n",
       "      <td>179.849000</td>\n",
       "      <td>the Fiji Islands</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>us7000rmrb</td>\n",
       "      <td>2026-01-06 02:58:00.827000+00:00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>571.187</td>\n",
       "      <td>-17.725100</td>\n",
       "      <td>-178.709700</td>\n",
       "      <td>Levuka, Fiji</td>\n",
       "      <td>212 km E of Levuka, Fiji</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>us7000rmir</td>\n",
       "      <td>2026-01-05 09:07:30.530000+00:00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>534.883</td>\n",
       "      <td>-5.705600</td>\n",
       "      <td>110.496500</td>\n",
       "      <td>Batang, Indonesia</td>\n",
       "      <td>89 km NNW of Batang, Indonesia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>us7000rrmy</td>\n",
       "      <td>2026-01-05 01:59:29.315000+00:00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>576.959</td>\n",
       "      <td>-17.964100</td>\n",
       "      <td>-178.490700</td>\n",
       "      <td>Levuka, Fiji</td>\n",
       "      <td>232 km E of Levuka, Fiji</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>us7000rrm2</td>\n",
       "      <td>2026-01-04 05:43:17.486000+00:00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>558.708</td>\n",
       "      <td>-19.394800</td>\n",
       "      <td>-177.554500</td>\n",
       "      <td>Fiji region</td>\n",
       "      <td>Fiji region</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        event_id                         time_utc   mag  depth_km   latitude   longitude                   region                                  place  anom_mag_missing  \\\n",
       "1     uw62216847 2026-02-02 23:32:53.110000+00:00  2.80    -0.240  46.234500 -119.422000  Benton City, Washington    5 km ESE of Benton City, Washington                 0   \n",
       "56    us6000s5vj 2026-02-01 18:09:34.279000+00:00  4.70   527.156 -23.597700  179.933700         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "83    us6000s5s2 2026-01-31 20:08:33.970000+00:00  4.40   507.818 -24.978100  179.786200         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "87    us6000s5rp 2026-01-31 18:34:43.926000+00:00  4.80   584.727   6.738000  123.626900      Gadung, Philippines         44 km W of Gadung, Philippines                 0   \n",
       "91    us6000s5rc 2026-01-31 15:59:11.987000+00:00  4.40   530.235 -20.003200 -177.552700             Houma, Tonga             267 km WNW of Houma, Tonga                 0   \n",
       "175   us6000s5bj 2026-01-29 20:13:42.056000+00:00  4.70   544.114 -20.525300 -178.214000              Fiji region                            Fiji region                 0   \n",
       "282   uw62216317 2026-01-27 22:26:04.650000+00:00  2.61    -0.430  42.653833 -124.439167      Port Orford, Oregon       11 km SSE of Port Orford, Oregon                 0   \n",
       "283   uw62216302 2026-01-27 21:25:04.700000+00:00  3.05    -0.640  42.644500 -124.445667      Port Orford, Oregon       12 km SSE of Port Orford, Oregon                 0   \n",
       "306   us6000s4p9 2026-01-27 08:31:31.478000+00:00  4.50   573.799  -4.367600  149.686000  Kimbe, Papua New Guinea  140 km NNW of Kimbe, Papua New Guinea                 0   \n",
       "309   us6000s4p3 2026-01-27 07:54:37.140000+00:00  4.20   572.586 -23.433300  178.866100         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "430   us7000rrww 2026-01-25 04:46:08.098000+00:00  4.90   577.764 -19.891400 -178.277900              Fiji region                            Fiji region                 0   \n",
       "431   us7000rrwm 2026-01-25 04:13:02.738000+00:00  5.20   509.606 -25.230000  179.685700         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "465   us7000rrst 2026-01-24 12:12:41.564000+00:00  4.90   590.366 -22.666500 -179.718600         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "468   us7000rrsg 2026-01-24 10:05:53.405000+00:00  5.00   587.326 -21.072200 -178.758300              Fiji region                            Fiji region                 0   \n",
       "661   us7000rqsi 2026-01-20 08:45:24.268000+00:00  4.70   527.005 -23.697000 -179.959800         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "715   us7000rql6 2026-01-19 17:30:06.258000+00:00  4.40   526.354 -17.968500 -178.358900             Levuka, Fiji               246 km E of Levuka, Fiji                 0   \n",
       "890   us7000rpyy 2026-01-15 22:23:40.622000+00:00  4.40   530.955 -24.060700  179.768500         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "950   us6000s5kj 2026-01-14 17:14:31.874000+00:00  4.30   525.656 -20.350100 -177.793100             Houma, Tonga             274 km WNW of Houma, Tonga                 0   \n",
       "951   us7000rpn6 2026-01-14 16:21:08.725000+00:00  4.10   529.997 -23.595800  179.531300         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "979   us6000s5j2 2026-01-13 23:42:25.546000+00:00  4.40   591.174 -17.821600 -178.703100             Levuka, Fiji               211 km E of Levuka, Fiji                 0   \n",
       "1024  us7000rpci 2026-01-13 07:38:07.943000+00:00  4.60   566.522 -18.056500 -178.248500             Levuka, Fiji               257 km E of Levuka, Fiji                 0   \n",
       "1054  us6000s5gy 2026-01-12 21:49:42.695000+00:00  4.30   550.704  -4.296900  128.556800         Ambon, Indonesia          78 km SSE of Ambon, Indonesia                 0   \n",
       "1112  ci41155815 2026-01-11 23:54:22.580000+00:00  4.11    -0.280  31.537333 -115.697500      Maneadero, B.C., MX       85 km ESE of Maneadero, B.C., MX                 0   \n",
       "1173  us6000s527 2026-01-10 20:26:52.781000+00:00  4.30   548.652 -23.470800 -179.776600         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "1181  us7000rny2 2026-01-10 16:11:38.021000+00:00  4.70   531.072 -25.948800  179.012600         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "1211  us7000rntz 2026-01-10 03:39:59.320000+00:00  4.30   572.110  -7.116200  120.471700        Ruteng, Indonesia          165 km N of Ruteng, Indonesia                 0   \n",
       "1230  us6000s50q 2026-01-09 20:11:50.392000+00:00  4.60   541.207 -24.535600  179.692100         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "1235  us6000s50z 2026-01-09 17:44:23.832000+00:00  4.50   589.158 -17.480300 -178.615200             Levuka, Fiji             228 km ENE of Levuka, Fiji                 0   \n",
       "1307  us7000rnea 2026-01-08 06:41:45.111000+00:00  4.50   619.726 -14.668700  170.101100            Sola, Vanuatu            288 km ESE of Sola, Vanuatu                 0   \n",
       "1326  us7000rnbi 2026-01-07 22:00:12.599000+00:00  5.50   538.866 -23.421500 -179.921000         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "1370  ci41153359 2026-01-07 05:22:22.810000+00:00  3.37    -0.250  31.515833 -115.665500      Maneadero, B.C., MX       88 km ESE of Maneadero, B.C., MX                 0   \n",
       "1382  us6000s4rw 2026-01-07 02:55:51.295000+00:00  4.30   583.721 -17.644800 -178.917600             Levuka, Fiji             192 km ENE of Levuka, Fiji                 0   \n",
       "1439  us7000rmug 2026-01-06 09:53:23.183000+00:00  4.30   554.172 -27.164700  -63.420200       El Hoyo, Argentina         23 km SW of El Hoyo, Argentina                 0   \n",
       "1440  us7000rmuf 2026-01-06 09:47:16.539000+00:00  4.20   516.261 -24.588900  179.849000         the Fiji Islands              south of the Fiji Islands                 0   \n",
       "1470  us7000rmrb 2026-01-06 02:58:00.827000+00:00  4.90   571.187 -17.725100 -178.709700             Levuka, Fiji               212 km E of Levuka, Fiji                 0   \n",
       "1511  us7000rmir 2026-01-05 09:07:30.530000+00:00  4.40   534.883  -5.705600  110.496500        Batang, Indonesia         89 km NNW of Batang, Indonesia                 0   \n",
       "1524  us7000rrmy 2026-01-05 01:59:29.315000+00:00  4.40   576.959 -17.964100 -178.490700             Levuka, Fiji               232 km E of Levuka, Fiji                 0   \n",
       "1577  us7000rrm2 2026-01-04 05:43:17.486000+00:00  4.40   558.708 -19.394800 -177.554500              Fiji region                            Fiji region                 0   \n",
       "\n",
       "      anom_mag_extreme  anom_depth_negative  anom_depth_extreme  anom_coords_missing  anom_any  \n",
       "1                    0                    1                   0                    0         1  \n",
       "56                   0                    0                   1                    0         1  \n",
       "83                   0                    0                   1                    0         1  \n",
       "87                   0                    0                   1                    0         1  \n",
       "91                   0                    0                   1                    0         1  \n",
       "175                  0                    0                   1                    0         1  \n",
       "282                  0                    1                   0                    0         1  \n",
       "283                  0                    1                   0                    0         1  \n",
       "306                  0                    0                   1                    0         1  \n",
       "309                  0                    0                   1                    0         1  \n",
       "430                  0                    0                   1                    0         1  \n",
       "431                  0                    0                   1                    0         1  \n",
       "465                  0                    0                   1                    0         1  \n",
       "468                  0                    0                   1                    0         1  \n",
       "661                  0                    0                   1                    0         1  \n",
       "715                  0                    0                   1                    0         1  \n",
       "890                  0                    0                   1                    0         1  \n",
       "950                  0                    0                   1                    0         1  \n",
       "951                  0                    0                   1                    0         1  \n",
       "979                  0                    0                   1                    0         1  \n",
       "1024                 0                    0                   1                    0         1  \n",
       "1054                 0                    0                   1                    0         1  \n",
       "1112                 0                    1                   0                    0         1  \n",
       "1173                 0                    0                   1                    0         1  \n",
       "1181                 0                    0                   1                    0         1  \n",
       "1211                 0                    0                   1                    0         1  \n",
       "1230                 0                    0                   1                    0         1  \n",
       "1235                 0                    0                   1                    0         1  \n",
       "1307                 0                    0                   1                    0         1  \n",
       "1326                 0                    0                   1                    0         1  \n",
       "1370                 0                    1                   0                    0         1  \n",
       "1382                 0                    0                   1                    0         1  \n",
       "1439                 0                    0                   1                    0         1  \n",
       "1440                 0                    0                   1                    0         1  \n",
       "1470                 0                    0                   1                    0         1  \n",
       "1511                 0                    0                   1                    0         1  \n",
       "1524                 0                    0                   1                    0         1  \n",
       "1577                 0                    0                   1                    0         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create anomaly dataframe\n",
    "anomalies = pd.DataFrame(index=df_curated_final.index)\n",
    "\n",
    "# 1. Missing magnitude\n",
    "anomalies['anom_mag_missing'] = df_curated_final['mag'].isna().astype(int)\n",
    "\n",
    "# 2. Extreme magnitude\n",
    "# Flag magnitudes outside typical range (< 0 or > 8.0)\n",
    "anomalies['anom_mag_extreme'] = (\n",
    "    (df_curated_final['mag'] < 0) | \n",
    "    (df_curated_final['mag'] > 8.0)\n",
    ").astype(int)\n",
    "\n",
    "# 3. Negative depth\n",
    "# Events above sea level (unusual but possible)\n",
    "anomalies['anom_depth_negative'] = (df_curated_final['depth_km'] < 0).astype(int)\n",
    "\n",
    "# 4. Extreme depth\n",
    "# Very deep events (> 500 km)\n",
    "anomalies['anom_depth_extreme'] = (df_curated_final['depth_km'] > 500).astype(int)\n",
    "\n",
    "# 5. Missing coordinates\n",
    "anomalies['anom_coords_missing'] = (\n",
    "    df_curated_final['longitude'].isna() | \n",
    "    df_curated_final['latitude'].isna()\n",
    ").astype(int)\n",
    "\n",
    "# Summary - Any anomaly\n",
    "anomalies['anom_any'] = (anomalies.sum(axis=1) > 0).astype(int)\n",
    "\n",
    "# Count suspicious rows\n",
    "suspicious_count = anomalies['anom_any'].sum()\n",
    "print(f\"Suspicious rows: {suspicious_count}\")\n",
    "\n",
    "# Create investigation table\n",
    "if suspicious_count > 0:\n",
    "    # Select columns for investigation\n",
    "    investigation_cols = [\n",
    "        'event_id', 'time_utc', 'mag', 'depth_km', \n",
    "        'latitude', 'longitude', 'region', 'place'\n",
    "    ]\n",
    "    \n",
    "    # Combine curated data with anomaly flags\n",
    "    investigation = pd.concat([\n",
    "        df_curated_final[investigation_cols],\n",
    "        anomalies\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Filter to only suspicious rows\n",
    "    investigation = investigation[investigation['anom_any'] == 1]\n",
    "    \n",
    "    # Display investigation table\n",
    "    display(investigation)\n",
    "else:\n",
    "    print(\"No anomalies detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbda87-01e9-4efa-b448-43a43cfaf1db",
   "metadata": {},
   "source": [
    "### Canaries + spike days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12b00b83-37e2-4ec3-8494-7f4bc5ad66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'n_days': 30, 'min': 24, 'median': 50.0, 'max': 90}, [])\n"
     ]
    }
   ],
   "source": [
    "# Compute daily event counts\n",
    "daily_counts = df_curated_final.groupby('event_day').size().reset_index(name='n_events')\n",
    "\n",
    "# Canary metrics\n",
    "canaries = {\n",
    "    \"n_days\": len(daily_counts),\n",
    "    \"min\": int(daily_counts[\"n_events\"].min()),\n",
    "    \"median\": float(daily_counts[\"n_events\"].median()),\n",
    "    \"max\": int(daily_counts[\"n_events\"].max())\n",
    "}\n",
    "\n",
    "# Spike detection\n",
    "median_count = canaries['median']\n",
    "spike_threshold = 2.5 * median_count\n",
    "\n",
    "# find spike days\n",
    "spike_days = daily_counts[daily_counts[\"n_events\"] > spike_threshold][\"event_day\"].tolist()\n",
    "\n",
    "print((canaries, spike_days))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0999b0b-326c-4494-a34e-e5461ac753a9",
   "metadata": {},
   "source": [
    "## 5 - Leakage Audit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40091842-5fb8-43f0-991c-c24cf89795db",
   "metadata": {},
   "source": [
    "### Leakage Audit Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3aca5-5477-4f84-9733-47bd10077803",
   "metadata": {},
   "source": [
    "### Understanding Each Check:\n",
    "\n",
    "#### 1. \"Any preprocessing stats (bins, encoders) computed on TRAIN only?\"\n",
    "* **Bad**: Compute magnitude bins using min/max from entire dataset\n",
    "* **Good**: Compute bins only on training set, apply to test set\n",
    "* **Why**: Test set info shouldn't influence training preprocessing\n",
    "\n",
    "#### 2. \"Any joins pulling in data that arrives after event time?\"\n",
    "* **Bad**: Join earthquake with aftershock counts (aftershocks happen AFTER)\n",
    "* **Good**: Only use data available at time of earthquake\n",
    "* **Why**: At prediction time, you won't have future data\n",
    "\n",
    "#### 3. \"Are you using updated_utc as a feature when it may not exist at prediction time?\"\n",
    "* **Bad**: Use `updated_utc` as a feature (it gets revised later)\n",
    "* **Good**: Only use `time_utc` (the event occurrence time)\n",
    "* **Why**: `updated_utc` reflects when USGS last revised the data, not event properties\n",
    "\n",
    "#### 4. \"Are you computing group summaries using future rows (time leakage)?\"\n",
    "* **Bad**: \"Average magnitude in this region this month\" (includes future events in same month)\n",
    "* **Good**: \"Average magnitude in this region in past 30 days\" (only uses past)\n",
    "* **Why**: You can't know future events when making predictions\n",
    "\n",
    "#### 5. \"Is the prediction time clearly defined for the task?\"\n",
    "* **Important**: When exactly are you making the prediction?\n",
    "   * At earthquake detection time? (few minutes after)\n",
    "   * 1 hour after earthquake?\n",
    "   * Next day?\n",
    "* **Why**: Defines what data is \"available\" vs \"future\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d5a3cb8-3dc8-46ff-9263-216f7b87e31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Any preprocessing stats (bins, encoders) computed on TRAIN only?',\n",
       " 'Any joins pulling in data that arrives after event time?',\n",
       " 'Are you using updated_utc as a feature when it may not exist at prediction time?',\n",
       " 'Are you computing group summaries using future rows (time leakage)?',\n",
       " 'Is the prediction time clearly defined for the task?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leakage_checklist = [\n",
    "    \"Any preprocessing stats (bins, encoders) computed on TRAIN only?\",\n",
    "    \"Any joins pulling in data that arrives after event time?\",\n",
    "    \"Are you using updated_utc as a feature when it may not exist at prediction time?\",\n",
    "    \"Are you computing group summaries using future rows (time leakage)?\",\n",
    "    \"Is the prediction time clearly defined for the task?\",\n",
    "]\n",
    "\n",
    "display(leakage_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3318dd97-1351-4414-9d16-b10f6b147824",
   "metadata": {},
   "source": [
    "## 6 - Write a Validation Report + Run Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4bf2587-2fc3-4ba1-8f8b-29cbf1ba87e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote validation report: validation_report_20260203_171853_utc.json\n"
     ]
    }
   ],
   "source": [
    "# Build Validation Report\n",
    "validation_report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"dataset\": \"USGS Earthquake Data - Curated\",\n",
    "    \"n_rows\": int(len(df_curated_final)),\n",
    "    \"n_cols\": int(len(df_curated_final.columns)),\n",
    "    \n",
    "    # Contract validation results\n",
    "    \"contracts\": {\n",
    "        \"defined\": {\n",
    "            \"required_cols\": list(REQUIRED_COLS),\n",
    "            \"mag_range\": [float(MAG_MIN), float(MAG_MAX)],\n",
    "            \"depth_range\": [float(DEPTH_MIN), float(DEPTH_MAX)],\n",
    "        },\n",
    "        \"validation_passed\": bool(validation_results[\"passed\"]),\n",
    "        \"failures\": int(sum(1 for check in validation_results[\"checks\"] if not check[\"passed\"])),\n",
    "        \"checks\": [\n",
    "            {\n",
    "                \"name\": check[\"name\"],\n",
    "                \"passed\": bool(check[\"passed\"]),\n",
    "                \"details\": str(check[\"details\"])\n",
    "            }\n",
    "            for check in validation_results[\"checks\"]\n",
    "        ],\n",
    "    },\n",
    "    # Anomaly detection results\n",
    "    \"anomalies\": {\n",
    "        \"suspicious_rows\": int(anomalies['anom_any'].sum()),\n",
    "        \"suspicious_pct\": float(anomalies['anom_any'].mean() * 100),\n",
    "        \"by_type\": {\n",
    "            col: int(anomalies[col].sum()) \n",
    "            for col in anomalies.columns if col.startswith('anom_') and col != 'anom_any'\n",
    "        },\n",
    "    },\n",
    "    # Canary metrics\n",
    "    \"canaries\": {\n",
    "        \"daily_events\": canaries,\n",
    "        \"spike_days\": [str(d) for d in spike_days],\n",
    "        \"spike_threshold\": float(spike_threshold),\n",
    "    },\n",
    "    # Leakage audit checklist\n",
    "    \"leakage_checklist\": leakage_checklist,\n",
    "}\n",
    "\n",
    "# Write Validation Report\n",
    "validation_report_path = REF_DIR / f\"validation_report_{RUN_ID}.json\"\n",
    "with open(validation_report_path, 'w') as f:\n",
    "    json.dump(validation_report, f, indent=2)\n",
    "\n",
    "print(f\"Wrote validation report: {validation_report_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c1832-f121-4aee-93b6-f059ad36b0fb",
   "metadata": {},
   "source": [
    "### Write a run log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8186ec47-4f69-4ec8-891b-b849c3587a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: work/m2_project/data/reference/pipeline_runs/run_20260203_171853_utc.json\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'run_id': 'run_20260203_171853_utc',\n",
       " 'generated_at_utc': '2026-02-03T20:46:13.746210+00:00',\n",
       " 'inputs': {'query_fingerprint': 'ae99c964a6922b52',\n",
       "  'raw_pages_path': 'work/m2_project/data/raw/usgs_pages_run_20260203_171853_utc.jsonl',\n",
       "  'raw_pages_size_bytes': 0,\n",
       "  'raw_meta_path': 'work/m2_project/data/raw/usgs_meta_run_20260203_171853_utc.json'},\n",
       " 'outputs': {'staged_path': 'work/m2_project/data/staged/usgs_earthquakes_staged_20260203_171853_utc.parquet',\n",
       "  'curated_path': 'work/m2_project/data/warehouse/usgs_earthquakes_curated_20260203_171853_utc.parquet',\n",
       "  'partition_root': 'work/m2_project/data/warehouse/partitions',\n",
       "  'validation_report_path': 'work/m2_project/data/reference/validation_report_20260203_171853_utc.json'},\n",
       " 'row_definition': 'Each row is one earthquake event from USGS (event_id) at time_utc.',\n",
       " 'notes': ['If validations fail, do not silently continue. Investigate upstream changes or parsing issues.',\n",
       "  'Consider adding alerts for spike days or missingness threshold breaches.']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline Run Log\n",
    "run_log = {\n",
    "    \"run_id\": f\"run_{RUN_ID}\",\n",
    "    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \n",
    "    # Input tracking\n",
    "    \"inputs\": {\n",
    "        \"query_fingerprint\": hashlib.md5(\n",
    "            f\"{starttime}{endtime}{MIN_MAG}\".encode()\n",
    "        ).hexdigest()[:16],\n",
    "        \"raw_pages_path\": str((RAW_DIR / f\"usgs_pages_run_{RUN_ID}.jsonl\")),\n",
    "        \"raw_pages_size_bytes\": (RAW_DIR / f\"usgs_pages_run_{RUN_ID}.jsonl\").stat().st_size if (RAW_DIR / f\"usgs_pages_run_{RUN_ID}.jsonl\").exists() else 0,\n",
    "        \"raw_meta_path\": str(RAW_DIR / f\"usgs_meta_run_{RUN_ID}.json\"),\n",
    "    },\n",
    "    \n",
    "    # Output tracking\n",
    "    \"outputs\": {\n",
    "        \"staged_path\": str(staged_parquet_path),\n",
    "        \"curated_path\": str(curated_parquet_path),\n",
    "        \"partition_root\": str(PART_DIR),\n",
    "        \"validation_report_path\": str(validation_report_path),\n",
    "    },\n",
    "    \n",
    "    # Row definition\n",
    "    \"row_definition\": \"Each row is one earthquake event from USGS (event_id) at time_utc.\",\n",
    "    \n",
    "    # Pipeline notes\n",
    "    \"notes\": [\n",
    "        \"If validations fail, do not silently continue. Investigate upstream changes or parsing issues.\",\n",
    "        \"Consider adding alerts for spike days or missingness threshold breaches.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Write Run Log\n",
    "run_log_path = RUN_DIR / f\"run_{RUN_ID}.json\"\n",
    "with open(run_log_path, 'w') as f:\n",
    "    json.dump(run_log, f, indent=2)\n",
    "\n",
    "print(f\"Saved: {run_log_path}\")\n",
    "print()\n",
    "display(run_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb700707-20d6-409d-b77d-360c84c5f9d3",
   "metadata": {},
   "source": [
    "# USGS Earthquake Data Pipeline - Summary\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a complete, production-ready data pipeline for earthquake data from the USGS API. It follows best practices for data engineering and ML pipeline design, structured around the **Extract  Stage  Curate  Validate** pattern.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Stages\n",
    "\n",
    "### **Stage 0: Setup**\n",
    "- Establishes workspace structure (`data/raw/`, `data/staged/`, `data/warehouse/`, `data/reference/`)\n",
    "- Defines helper utilities for consistent file handling\n",
    "- Generates a **run ID** (`RUN_ID`) that links all artifacts from a single execution\n",
    "- Sets up pandas display options\n",
    "\n",
    "**Key Learning:** Workspace organization and run tracking are foundational to reproducible pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage 1: Ingest - Pull Data from USGS API**\n",
    "**Objective:** Fetch raw earthquake data with proper pagination and error handling\n",
    "\n",
    "**What it does:**\n",
    "- Queries USGS Earthquake API with configurable parameters (time window, minimum magnitude)\n",
    "- Implements pagination to handle large result sets (200 events per page)\n",
    "- Saves raw JSON responses to preserve original data structure\n",
    "- Creates metadata tracking query parameters and fetch statistics\n",
    "\n",
    "**Key Files Produced:**\n",
    "- `data/raw/usgs_pages_run_{RUN_ID}.jsonl` - Raw API responses\n",
    "- `data/raw/usgs_meta_run_{RUN_ID}.json` - Query metadata\n",
    "\n",
    "**Best Practices:**\n",
    "-  Always save raw data exactly as received (no transformations yet)\n",
    "-  Track provenance: what query produced this data?\n",
    "-  Handle pagination and rate limiting\n",
    "-  Use consistent run IDs across all artifacts\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage 2: Stage - Normalize GeoJSON  Table**\n",
    "**Objective:** Transform nested JSON into typed, cleaned tabular data\n",
    "\n",
    "**What it does:**\n",
    "1. **Flatten nested structures** - Converts GeoJSON features to flat columns using `pd.json_normalize()`\n",
    "2. **Extract coordinates** - Pulls longitude, latitude, depth from nested `geometry.coordinates` array\n",
    "3. **Parse timestamps** - Converts Unix milliseconds to proper datetime objects with UTC timezone\n",
    "4. **Type enforcement** - Converts strings to appropriate types (float, int, datetime)\n",
    "5. **Normalize missing values** - Replaces sentinel values (`\"None\"`, `\"NA\"`, `\"\"`) with proper `NaN`\n",
    "6. **Deduplication** - Keeps most recent version of each event by `event_id`\n",
    "\n",
    "**Key Files Produced:**\n",
    "- `data/staged/usgs_earthquakes_staged_{RUN_ID}.parquet` - Typed, deduplicated data\n",
    "- `data/staged/usgs_staged_meta_{RUN_ID}.json` - Schema and quality metadata\n",
    "\n",
    "**Best Practices:**\n",
    "-  Staging focuses on structure/types, NOT feature engineering\n",
    "-  Use `errors='coerce'` for safe type conversions\n",
    "-  Explicit deduplication policy (keep most recent update)\n",
    "-  Document schema in metadata\n",
    "\n",
    "**Key Function:** `staged_dataframe(df_raw)` - Encapsulates all staging logic\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage 3: Curate - Build Analysis-Ready Features**\n",
    "**Objective:** Create derived features for downstream analysis/modeling\n",
    "\n",
    "**What it does:**\n",
    "1. **Temporal features**\n",
    "   - `event_day` - Date portion (for partitioning)\n",
    "   - `event_hour` - Hour of day (0-23)\n",
    "   - `dayofweek` - Day of week (0=Monday, 6=Sunday)\n",
    "   - `is_weekend` - Binary flag for Saturday/Sunday\n",
    "\n",
    "2. **Magnitude features**\n",
    "   - `mag_bin` - Categorized magnitude (<2.5, 2.5-4.5, 4.5-6.0, 6.0+)\n",
    "   - `is_major` - Binary flag for magnitude  6.0\n",
    "\n",
    "3. **Location features**\n",
    "   - `region` - Parsed from `place` string (e.g., \"16 km E of San Ramon, CA\"  \"San Ramon, CA\")\n",
    "\n",
    "**Key Files Produced:**\n",
    "- `data/warehouse/usgs_earthquakes_curated_{RUN_ID}.parquet` - Single curated file\n",
    "- `data/warehouse/partitions/event_day={YYYY-MM-DD}/earthquakes.parquet` - Partitioned by day\n",
    "\n",
    "**Best Practices:**\n",
    "-  Curated layer adds value (derived features), staged layer ensures quality (types/dedup)\n",
    "-  Partition by frequently-filtered columns (date) for query efficiency\n",
    "-  Select only relevant columns for curated output (drop technical metadata)\n",
    "-  Hive-style partitioning (`event_day=YYYY-MM-DD/`) for compatibility with Spark/Dask\n",
    "\n",
    "**Why Partition?**\n",
    "- Faster queries: \"Show February 1st\" reads 1 file, not 1000\n",
    "- Incremental updates: Add new days without rewriting history\n",
    "- Standard practice: Works with modern data tools (DuckDB, Polars, Spark)\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage 4: Validate - Contracts + Anomalies + Canaries**\n",
    "**Objective:** Ensure data quality and detect drift\n",
    "\n",
    "#### **4.1 Contracts (Hard Requirements)**\n",
    "Define and enforce expectations:\n",
    "- **Required columns** - Must have `event_id`, `time_utc`, `mag`, `latitude`, `longitude`, `depth_km`\n",
    "- **Valid ranges** - Latitude [-90, 90], Longitude [-180, 180], Magnitude [-1, 10], Depth [-10, 700]\n",
    "- **Uniqueness** - `event_id` must be unique\n",
    "- **Temporal validity** - No future events\n",
    "\n",
    "**Output:** Pass/Fail + failure count\n",
    "\n",
    "**Best Practice:**\n",
    "-  Contracts are domain-specific (earthquake physics, not just \"data exists\")\n",
    "-  Fail fast: Pipeline should halt if critical contracts fail\n",
    "-  Contracts prevent silent data corruption\n",
    "\n",
    "#### **4.2 Anomaly Detection (Soft Flags)**\n",
    "Flag unusual patterns for investigation (not necessarily errors):\n",
    "- `anom_mag_missing` - Missing magnitude\n",
    "- `anom_mag_extreme` - Magnitude < 0 or > 8.0\n",
    "- `anom_depth_negative` - Depth < 0 (above sea level)\n",
    "- `anom_depth_extreme` - Depth > 500 km (very deep subduction)\n",
    "- `anom_coords_missing` - Missing location\n",
    "\n",
    "**Output:** Investigation table showing flagged events\n",
    "\n",
    "**Best Practice:**\n",
    "-  Anomalies  Errors (deep earthquakes are real, just unusual)\n",
    "-  Create `anom_any` summary flag for quick filtering\n",
    "-  Display sample anomalous records for manual review\n",
    "\n",
    "#### **4.3 Canaries (Distribution Checks)**\n",
    "Simple aggregate metrics to detect data drift:\n",
    "- Daily event counts: `min`, `median`, `max`, `n_days`\n",
    "- Spike detection: Days with > 2.5 median events\n",
    "\n",
    "**Output:** Dictionary of metrics + list of spike days\n",
    "\n",
    "**Best Practice:**\n",
    "-  Canaries are cheap, high-signal metrics\n",
    "-  Compare across runs to detect: source changes, API issues, seasonal patterns\n",
    "-  Example: If median magnitude suddenly shifts, investigate data source\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage 5: Leakage Audit**\n",
    "**Objective:** Prevent future data from contaminating training\n",
    "\n",
    "**Key Questions:**\n",
    "1. Are preprocessing stats computed on TRAIN only? (not entire dataset)\n",
    "2. Are joins pulling in data that arrives after event time?\n",
    "3. Is `updated_utc` used as a feature? (it gets revised later!)\n",
    "4. Are group summaries using future rows? (e.g., \"this month's average\" includes future)\n",
    "5. Is prediction time clearly defined?\n",
    "\n",
    "**Best Practice:**\n",
    "-  Leakage breaks time-series models silently\n",
    "-  Review checklist before feature engineering\n",
    "-  Example: Don't use aftershock counts to predict earthquake magnitude (aftershocks happen AFTER)\n",
    "\n",
    "**Common Leaky Features in Earthquake Data:**\n",
    "-  `updated_utc` - Gets revised after event\n",
    "-  `status` - Changes from \"automatic\"  \"reviewed\" later\n",
    "-  Aftershock counts\n",
    "-  Historical summaries (\"past 30 days in this region\") are safe\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage 6: Write Validation Report + Run Log**\n",
    "\n",
    "#### **Validation Report**\n",
    "Comprehensive JSON capturing:\n",
    "- Contract results (pass/fail, details)\n",
    "- Anomaly statistics (counts by type)\n",
    "- Canary metrics (distribution stats)\n",
    "- Leakage checklist\n",
    "\n",
    "**File:** `data/reference/validation_report_{RUN_ID}.json`\n",
    "\n",
    "**Use Case:** Compare reports across runs to detect drift\n",
    "\n",
    "#### **Pipeline Run Log**\n",
    "Links inputs  outputs for full traceability:\n",
    "- Input fingerprint (query parameters hash)\n",
    "- All input file paths and sizes\n",
    "- All output file paths\n",
    "- Row definition\n",
    "- Operational notes\n",
    "\n",
    "**File:** `data/reference/pipeline_runs/run_{RUN_ID}.json`\n",
    "\n",
    "**Use Case:** \"This curated file came from which raw data?\"\n",
    "\n",
    "**Best Practice:**\n",
    "-  Run logs enable reproducibility\n",
    "-  Fingerprints detect when same query produces different results (API changes)\n",
    "-  Each run is self-documenting\n",
    "\n",
    "---\n",
    "\n",
    "## Key Design Patterns\n",
    "\n",
    "### **1. ELT (Extract-Load-Transform) Architecture**\n",
    "```\n",
    "Raw (load as-is)  Staged (types/structure)  Curated (features)\n",
    "```\n",
    "- Preserves raw data for reprocessing\n",
    "- Staged layer is the \"clean checkpoint\"\n",
    "- Curated layer is analysis-specific\n",
    "\n",
    "### **2. Run ID Consistency**\n",
    "All artifacts from one execution share the same `RUN_ID`:\n",
    "```\n",
    "usgs_pages_run_20260203_152345_utc.jsonl\n",
    "usgs_earthquakes_staged_run_20260203_152345_utc.parquet\n",
    "usgs_earthquakes_curated_run_20260203_152345_utc.parquet\n",
    "validation_report_20260203_152345_utc.json\n",
    "run_20260203_152345_utc.json\n",
    "```\n",
    "- Enables tracing: \"Which raw data produced this curated file?\"\n",
    "- Enables comparison: \"How did validation change between runs?\"\n",
    "\n",
    "### **3. Separation of Concerns**\n",
    "- **Staging** = Data quality (types, dedup, normalization)\n",
    "- **Curating** = Feature engineering (derived columns, binning)\n",
    "- **Validation** = Quality assurance (contracts, anomalies, drift)\n",
    "\n",
    "Each stage has a single responsibility.\n",
    "\n",
    "### **4. Metadata First**\n",
    "Every file has accompanying metadata:\n",
    "- Raw  Query parameters, fetch stats\n",
    "- Staged  Schema, quality metrics, deduplication\n",
    "- Curated  Feature definitions, partitioning\n",
    "- Validation  Contract results, anomaly counts, canaries\n",
    "- Run Log  Input/output traceability\n",
    "\n",
    "---\n",
    "\n",
    "## What Makes This a Good ML Pipeline Example?\n",
    "\n",
    "###  **Production-Ready Practices**\n",
    "1. **Error handling** - Pagination handles API limits, type coercion handles bad data\n",
    "2. **Validation** - Contracts catch data quality issues early\n",
    "3. **Reproducibility** - Run IDs link artifacts, metadata documents decisions\n",
    "4. **Scalability** - Partitioning enables efficient querying on large datasets\n",
    "5. **Maintainability** - Clear stage separation makes debugging easier\n",
    "\n",
    "###  **Educational Value**\n",
    "1. **Real-world data** - USGS API has messy, nested JSON (not clean CSVs)\n",
    "2. **Complete workflow** - Ingestion  Staging  Curation  Validation\n",
    "3. **Best practices** - Demonstrates contracts, anomaly detection, leakage prevention\n",
    "4. **Documented decisions** - Comments explain \"why\", not just \"what\"\n",
    "\n",
    "###  **Extensibility**\n",
    "1. **New features?** Add to curated stage without re-fetching raw data\n",
    "2. **Different time window?** Change query parameters, run pipeline again\n",
    "3. **Additional validation?** Add contracts without changing staging logic\n",
    "4. **ML modeling?** Curated data is ready, validation report shows data quality\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls This Pipeline Avoids\n",
    "\n",
    " **Overwriting raw data**   Each run creates timestamped files\n",
    " **Mixing staging and features**   Separate stages with clear responsibilities\n",
    " **No validation**   Contracts + anomalies + canaries\n",
    " **Manual tracking**   Automated run logs and metadata\n",
    " **Data leakage**   Leakage audit checklist\n",
    " **Losing provenance**   Run ID links all artifacts\n",
    " **Silent failures**   Contracts fail loudly with details\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use This as a Template\n",
    "\n",
    "### For Similar API-Based Pipelines:\n",
    "1. **Stage 1 (Ingest)** - Replace USGS API with your source, keep pagination pattern\n",
    "2. **Stage 2 (Stage)** - Adjust `staged_dataframe()` function for your schema\n",
    "3. **Stage 3 (Curate)** - Define features relevant to your domain\n",
    "4. **Stage 4 (Validate)** - Define contracts based on your domain logic\n",
    "5. **Stages 5-6** - Leakage audit and logging patterns remain the same\n",
    "\n",
    "### For CSV/File-Based Pipelines:\n",
    "- Skip pagination logic in Stage 1\n",
    "- Keep validation, staging, and curation patterns identical\n",
    "\n",
    "### For Different Domains:\n",
    "- **E-commerce:** Events = purchases, features = customer segments, contracts = valid prices\n",
    "- **IoT sensors:** Events = readings, features = rolling windows, contracts = sensor ranges\n",
    "- **Finance:** Events = transactions, features = time-based aggregations, contracts = balance checks\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: From Pipeline to ML\n",
    "\n",
    "This pipeline produces analysis-ready data. To add ML modeling:\n",
    "\n",
    "1. **Train/Test Split**\n",
    "   - Use `event_day` for time-based splits (train on Jan 1-20, test on Jan 21-30)\n",
    "   - Never shuffle time-series data randomly\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Historical features: \"Earthquakes in past 7 days in 100km radius\"\n",
    "   - Geographic features: \"Distance to nearest fault line\"\n",
    "   - Remember: Compute stats on train only, apply to test\n",
    "\n",
    "3. **Model Training**\n",
    "   - Use curated parquet files (fast to load)\n",
    "   - Check validation report before training (data quality OK?)\n",
    "   - Example tasks: Predict aftershock likelihood, classify event type\n",
    "\n",
    "4. **Model Validation**\n",
    "   - Use same contract validation on predictions\n",
    "   - Check for distribution drift (canaries on prediction outputs)\n",
    "\n",
    "---\n",
    "\n",
    "## File Structure Summary\n",
    "\n",
    "```\n",
    "work/m2_project/\n",
    " data/\n",
    "    raw/\n",
    "       usgs_pages_run_{RUN_ID}.jsonl          # API responses\n",
    "       usgs_meta_run_{RUN_ID}.json            # Query metadata\n",
    "    staged/\n",
    "       usgs_earthquakes_staged_{RUN_ID}.parquet\n",
    "       usgs_staged_meta_{RUN_ID}.json\n",
    "    warehouse/\n",
    "       usgs_earthquakes_curated_{RUN_ID}.parquet\n",
    "       partitions/\n",
    "           event_day=2026-01-01/earthquakes.parquet\n",
    "           event_day=2026-01-02/earthquakes.parquet\n",
    "           ...\n",
    "    reference/\n",
    "        validation_report_{RUN_ID}.json\n",
    "        pipeline_runs/\n",
    "            run_{RUN_ID}.json\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a **mini ML pipeline** that:\n",
    "-  Handles real-world messy data (nested JSON, pagination, missing values)\n",
    "-  Follows data engineering best practices (ELT, validation, provenance)\n",
    "-  Prevents common pitfalls (leakage, silent failures, lost metadata)\n",
    "-  Produces analysis-ready, partitioned data with quality guarantees\n",
    "\n",
    "**Use this as a template** for building robust data pipelines in any domain. The patterns (staging, contracts, run logs, leakage audits) are universal, even when the data source and features change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c3511-56a8-4b50-b04a-2be60a034452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
