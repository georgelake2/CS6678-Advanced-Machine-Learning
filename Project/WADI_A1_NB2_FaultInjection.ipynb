{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9197d07-42dc-4a3a-902d-d97aac5bf804",
   "metadata": {},
   "source": [
    "# WaDi A1 - Pipeline Notebook 2: Fault Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca13038-7f39-440e-abd8-684bae33222c",
   "metadata": {},
   "source": [
    "## Prior to this Notebook, Notebook 1 needs to be complete\n",
    "\n",
    "* **Input:** Staged parquet from Notebook 1\n",
    "* **Scope:** Injects synthetic sensor faults into normal rows in the train split, producing a three-class training dataset:\n",
    "    * 0=normal\n",
    "    * 1=cyber attack\n",
    "    * 2=sensor fault\n",
    "* **Output:** Injected parquet ready for Notebook 3 (Curate/Validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4f52a-3d67-4d64-8ee7-de480a3f16dc",
   "metadata": {},
   "source": [
    "# Stage 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca529a-aaf5-4c85-90c9-0e207648c02b",
   "metadata": {},
   "source": [
    "## 0.1 - Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccfd2c0-9390-44d9-98cf-e5ece405b86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project:   work/wadi_A1\n",
      "Staged:    work/wadi_A1/data/staged\n",
      "Injected:  work/wadi_A1/data/injected\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# ── Paths ─────────────────────────────────────────────────────────────────────\n",
    "WORK_DIR     = Path(\"work\")\n",
    "PROJECT_DIR  = WORK_DIR / \"wadi_A1\"\n",
    "DATA_DIR     = PROJECT_DIR / \"data\"\n",
    "STAGED_DIR   = DATA_DIR / \"staged\"\n",
    "INJECTED_DIR = DATA_DIR / \"injected\"\n",
    "REF_DIR      = DATA_DIR / \"reference\"\n",
    "RUN_DIR      = REF_DIR / \"pipeline_runs\"\n",
    "\n",
    "for p in [INJECTED_DIR, RUN_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project:  \", PROJECT_DIR)\n",
    "print(\"Staged:   \", STAGED_DIR)\n",
    "print(\"Injected: \", INJECTED_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabbb8f-43e1-4bc2-8305-4df3375abf98",
   "metadata": {},
   "source": [
    "## 0.2 - Helper Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ba7fec-6fe8-4ec6-8aac-d579e779bcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready.\n"
     ]
    }
   ],
   "source": [
    "class PipelineError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def write_json(path: Path, obj: dict) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, default=str))\n",
    "\n",
    "def read_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "print(\"Helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd55ea9-7e18-4360-8281-bbfc2f98d7d8",
   "metadata": {},
   "source": [
    "## 0.3 - Configuration  \n",
    "Fault injection parameters.  \n",
    "All tunable constants live here.  \n",
    "Seed is fixed for reproducibility. Changing it produces a different, but equally valid injection. Can be used for sensitivity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065d1e70-14ca-49fc-8304-28f0cc9226e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:                      WaDi.A1_9 Oct 2017\n",
      "Random seed:                  42\n",
      "Faults per sensor per split:  8%\n",
      "Fault duration:               120–600 seconds\n",
      "Min coverage ratio:           0.1\n",
      "Run ID:                       20260223_141149_utc\n"
     ]
    }
   ],
   "source": [
    "# 1. Dataset identity\n",
    "DATASET_NAME = \"WaDi.A1_9 Oct 2017\"\n",
    "\n",
    "# 2. Reproducibility\n",
    "RANDOM_SEED  = 42\n",
    "\n",
    "# 3. Fault injection parameters \n",
    "TARGET_FAULT_PCT = 0.08        # target fraction of normal rows to receive injected faults (~8%)\n",
    "FAULT_DURATION_MIN  = 120      # minimum fault duration in seconds\n",
    "FAULT_DURATION_MAX  = 600      # maximum fault duration in seconds\n",
    "MIN_COVERAGE_RATIO  = 0.10     # sensor must have at least 10% non-null normal rows\n",
    "BINARY_MAX_UNIQUE   = 5        # sensors with <= this many unique values treated as binary\n",
    "\n",
    "# 4. Run ID \n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S_utc\")\n",
    "\n",
    "# 5. Fix random seeds \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Dataset:                      {DATASET_NAME}\")\n",
    "print(f\"Random seed:                  {RANDOM_SEED}\")\n",
    "print(f\"Faults per sensor per split:  {TARGET_FAULT_PCT:.0%}\")\n",
    "print(f\"Fault duration:               {FAULT_DURATION_MIN}–{FAULT_DURATION_MAX} seconds\")\n",
    "print(f\"Min coverage ratio:           {MIN_COVERAGE_RATIO}\")\n",
    "print(f\"Run ID:                       {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b647c-948f-4fa3-b124-b7a73fd60d00",
   "metadata": {},
   "source": [
    "# Stage 1 - Load Staged Data   \n",
    "Load the staged parquet from Notebook 1 and the canonical `SENSOR_COLS` reference.  \n",
    "All downstream stages operate on these inputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d82f4-6cba-482d-9a6a-1508a905c503",
   "metadata": {},
   "source": [
    "## 1.1 - Load Staged Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43915af6-1f32-461e-9b93-ae980be31ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: work/wadi_A1/data/staged/wadi_staged_20260223_140105_utc.parquet\n",
      "Shape:   (1382402, 103)\n",
      "Columns: ['timestamp', 'observation_day', 'seconds_since_start', 'split', 'label', '1_AIT_001_PV', '1_AIT_002_PV', '1_AIT_003_PV'] ... (103 total)\n",
      "\n",
      "Label counts:\n",
      "  normal (0): 1,209,601\n",
      "  attack (1):   172,801\n",
      "\n",
      "Split counts:\n",
      "  train : 1,209,601\n",
      "  test  :   172,801\n"
     ]
    }
   ],
   "source": [
    "# Load most recent staged parquet\n",
    "staged_files = sorted(STAGED_DIR.glob(\"wadi_staged_*.parquet\"))\n",
    "if not staged_files:\n",
    "    raise PipelineError(f\"No staged parquet found in {STAGED_DIR}\")\n",
    "\n",
    "staged_path = staged_files[-1]\n",
    "print(f\"Loading: {staged_path}\")\n",
    "\n",
    "df = pd.read_parquet(staged_path)\n",
    "print(f\"Shape:   {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()[:8]} ... ({len(df.columns)} total)\")\n",
    "print(f\"\\nLabel counts:\")\n",
    "for label_val, label_name in [(0, \"normal\"), (1, \"attack\")]:\n",
    "    n = (df[\"label\"] == label_val).sum()\n",
    "    print(f\"  {label_name} ({label_val}): {n:>9,}\")\n",
    "\n",
    "print(f\"\\nSplit counts:\")\n",
    "for split in [\"train\", \"test\"]:\n",
    "    n = (df[\"split\"] == split).sum()\n",
    "    print(f\"  {split:<6}: {n:>9,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58492099-80d1-4269-9a44-360b38f678eb",
   "metadata": {},
   "source": [
    "## 1.2 - Load SENSOR_COLS Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99421cd-b788-4587-8777-5135a06a76f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENSOR_COLS loaded: 98 columns\n",
      "Source run ID:      20260223_140105_utc\n",
      "All SENSOR_COLS present in staged data.\n"
     ]
    }
   ],
   "source": [
    "# Load canonical sensor column list from Notebook 1\n",
    "sensor_cols_path = REF_DIR / \"sensor_cols.json\"\n",
    "if not sensor_cols_path.exists():\n",
    "    raise PipelineError(f\"sensor_cols.json not found at {sensor_cols_path}\")\n",
    "\n",
    "sensor_ref  = read_json(sensor_cols_path)\n",
    "SENSOR_COLS = sensor_ref[\"sensor_cols\"]\n",
    "\n",
    "print(f\"SENSOR_COLS loaded: {len(SENSOR_COLS)} columns\")\n",
    "print(f\"Source run ID:      {sensor_ref['run_id']}\")\n",
    "\n",
    "# Verify all SENSOR_COLS are present in the staged data\n",
    "missing = [c for c in SENSOR_COLS if c not in df.columns]\n",
    "if missing:\n",
    "    raise PipelineError(f\"SENSOR_COLS missing from staged data: {missing}\")\n",
    "\n",
    "print(\"All SENSOR_COLS present in staged data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776258c5-0b5c-43d0-8ef1-5a75ab89ee46",
   "metadata": {},
   "source": [
    "# Stage 2 - Identify Eligible Sensors  \n",
    "Determine which sensors are candidates for fault injection.  \n",
    "Binary and status columns are excluded. \n",
    "Injecting drift or bias into a column that only takes values {0, 1, 2} produces physically implausible results.  \n",
    "Sensors with insufficient normal-operation data coverage are also excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b5bd2-ed5c-417c-9f7f-e0edda4b5e80",
   "metadata": {},
   "source": [
    "## 2.1 - Identify Eligible Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b069dff-1ab9-48c3-b13d-f8fdc663ee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal rows available for injection: 1,209,601\n",
      "\n",
      "Normal rows by split:\n",
      "  train : 1,209,601\n",
      "  test  :         0\n",
      "\n",
      "Eligible sensors:   67\n",
      "Ineligible sensors: 31\n",
      "\n",
      "Ineligible breakdown:\n",
      "  1_MV_001_STATUS                      binary/status (unique=3)\n",
      "  1_MV_002_STATUS                      binary/status (unique=1)\n",
      "  1_MV_003_STATUS                      binary/status (unique=1)\n",
      "  1_MV_004_STATUS                      binary/status (unique=3)\n",
      "  1_P_001_STATUS                       binary/status (unique=2)\n",
      "  1_P_003_STATUS                       binary/status (unique=2)\n",
      "  1_P_005_STATUS                       binary/status (unique=2)\n",
      "  1_P_006_STATUS                       binary/status (unique=1)\n",
      "  2_LS_101_AH                          binary/status (unique=2)\n",
      "  2_LS_101_AL                          binary/status (unique=2)\n",
      "  2_LS_201_AH                          binary/status (unique=2)\n",
      "  2_LS_201_AL                          binary/status (unique=2)\n",
      "  2_LS_301_AH                          binary/status (unique=2)\n",
      "  2_LS_301_AL                          binary/status (unique=2)\n",
      "  2_LS_401_AH                          binary/status (unique=2)\n",
      "  2_LS_401_AL                          binary/status (unique=2)\n",
      "  2_LS_501_AH                          binary/status (unique=2)\n",
      "  2_LS_501_AL                          binary/status (unique=2)\n",
      "  2_LS_601_AH                          binary/status (unique=2)\n",
      "  2_LS_601_AL                          binary/status (unique=2)\n",
      "  2_MCV_007_CO                         binary/status (unique=1)\n",
      "  2_MV_003_STATUS                      binary/status (unique=3)\n",
      "  2_MV_006_STATUS                      binary/status (unique=3)\n",
      "  2_MV_101_STATUS                      binary/status (unique=3)\n",
      "  2_MV_201_STATUS                      binary/status (unique=3)\n",
      "  2_MV_301_STATUS                      binary/status (unique=3)\n",
      "  2_MV_401_STATUS                      binary/status (unique=3)\n",
      "  2_MV_501_STATUS                      binary/status (unique=3)\n",
      "  2_MV_601_STATUS                      binary/status (unique=3)\n",
      "  2_P_003_STATUS                       binary/status (unique=2)\n",
      "  2_PIC_003_SP                         binary/status (unique=1)\n"
     ]
    }
   ],
   "source": [
    "# Work from normal rows only — faults are injected into normal operation\n",
    "df_normal = df[df[\"label\"] == 0].copy()\n",
    "\n",
    "print(f\"Normal rows available for injection: {len(df_normal):,}\")\n",
    "print(f\"\\nNormal rows by split:\")\n",
    "for split in [\"train\", \"test\"]:\n",
    "    n = (df_normal[\"split\"] == split).sum()\n",
    "    print(f\"  {split:<6}: {n:>9,}\")\n",
    "\n",
    "# Identify eligible sensors \n",
    "eligible   = []\n",
    "ineligible = []\n",
    "\n",
    "for col in SENSOR_COLS:\n",
    "    series = df_normal[col].dropna()\n",
    "\n",
    "    # Exclude binary/status columns\n",
    "    n_unique = series.nunique()\n",
    "    if n_unique <= BINARY_MAX_UNIQUE:\n",
    "        ineligible.append((col, f\"binary/status (unique={n_unique})\"))\n",
    "        continue\n",
    "\n",
    "    # Exclude sensors with insufficient coverage\n",
    "    coverage = len(series) / len(df_normal)\n",
    "    if coverage < MIN_COVERAGE_RATIO:\n",
    "        ineligible.append((col, f\"low coverage ({coverage:.1%})\"))\n",
    "        continue\n",
    "\n",
    "    eligible.append(col)\n",
    "\n",
    "print(f\"\\nEligible sensors:   {len(eligible)}\")\n",
    "print(f\"Ineligible sensors: {len(ineligible)}\")\n",
    "print(f\"\\nIneligible breakdown:\")\n",
    "for col, reason in ineligible:\n",
    "    print(f\"  {col:<35s}  {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66497fa5-4829-4399-ad85-9049ccae7894",
   "metadata": {},
   "source": [
    "# Stage 3 - Define Fault Types  \n",
    "Defines five fault types based on industrial sensor failure literature.  \n",
    "Each fault type has physically grounded parameter ranges derived from real-world sensor behavior in water distribution systems.  \n",
    "\n",
    "<table style=\"text-align:left\">\n",
    "    <tr>\n",
    "        <th>Fault Type</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>drift</td>\n",
    "        <td>Gradual linear deviation from true value</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>bias</td>\n",
    "        <td>Sudden fixed offset added to readings</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>precision_degradation</td>\n",
    "        <td>Increased noise/variance around true value</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>stuck_at</td>\n",
    "        <td>Sensor freezes at a fixed value</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>intermittent_dropout</td>\n",
    "        <td>Random NaN dropouts simulating signal loss</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2abb53-9f9f-4aed-be30-340513f07b2a",
   "metadata": {},
   "source": [
    "## 3.1 - Define Fault Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3d862e-590c-42f3-8442-0108ca4a6c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault types defined:\n",
      "  drift                      Gradual linear deviation from true value\n",
      "  bias                       Sudden fixed offset added to all readings\n",
      "  precision_degradation      Increased noise around true value\n",
      "  stuck_at                   Sensor freezes at a fixed value\n",
      "  intermittent_dropout       Random NaN dropouts simulating signal loss\n"
     ]
    }
   ],
   "source": [
    "# Fault type definitions with parameter ranges \n",
    "FAULT_TYPES = {\n",
    "    \"drift\": {\n",
    "        \"description\": \"Gradual linear deviation from true value\",\n",
    "        \"rate_pct_per_min\": (0.5, 3.0),   # drift rate as % of sensor std per minute\n",
    "    },\n",
    "    \"bias\": {\n",
    "        \"description\": \"Sudden fixed offset added to all readings\",\n",
    "        \"offset_pct\": (10.0, 40.0),        # offset as % of sensor std\n",
    "    },\n",
    "    \"precision_degradation\": {\n",
    "        \"description\": \"Increased noise around true value\",\n",
    "        \"noise_multiplier\": (3.0, 8.0),    # multiply normal noise by this factor\n",
    "    },\n",
    "    \"stuck_at\": {\n",
    "        \"description\": \"Sensor freezes at a fixed value\",\n",
    "        \"stuck_pct\": (0.05, 0.95),         # stuck value as percentile of normal range\n",
    "    },\n",
    "    \"intermittent_dropout\": {\n",
    "        \"description\": \"Random NaN dropouts simulating signal loss\",\n",
    "        \"dropout_rate\": (0.2, 0.6),        # fraction of rows set to NaN\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Fault types defined:\")\n",
    "for name, cfg in FAULT_TYPES.items():\n",
    "    print(f\"  {name:<25s}  {cfg['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b64a8-d101-4b3a-9e5a-18176ef21ac6",
   "metadata": {},
   "source": [
    "## 3.2 - Define Fault Application Functions  \n",
    "One function per fault type. Each takes a sensor series and fault parameters and returns the modified series with the fault signature applied.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c977ea-704b-45cf-9c36-60214817e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault applier functions defined:\n",
      "  drift\n",
      "  bias\n",
      "  precision_degradation\n",
      "  stuck_at\n",
      "  intermittent_dropout\n"
     ]
    }
   ],
   "source": [
    "def apply_drift(series: pd.Series, rate_pct_per_min: float) -> pd.Series:\n",
    "    s = series.copy()\n",
    "    std = float(s.std())\n",
    "    n = len(s)\n",
    "    rate_per_sec = (rate_pct_per_min / 100.0) * std / 60.0\n",
    "    drift = np.linspace(0, rate_per_sec * n, n)\n",
    "    return (s + drift).astype(\"float32\")\n",
    "\n",
    "def apply_bias(series: pd.Series, offset_pct: float) -> pd.Series:\n",
    "    s = series.copy()\n",
    "    std = float(s.std())\n",
    "    offset = (offset_pct / 100.0) * std\n",
    "    # Randomly positive or negative\n",
    "    offset *= np.random.choice([-1, 1])\n",
    "    return (s + offset).astype(\"float32\")\n",
    "\n",
    "def apply_precision_degradation(series: pd.Series, noise_multiplier: float) -> pd.Series:\n",
    "    s = series.copy()\n",
    "    std = float(s.std())\n",
    "    noise = np.random.normal(0, std * noise_multiplier, size=len(s))\n",
    "    return (s + noise).astype(\"float32\")\n",
    "\n",
    "def apply_stuck_at(series: pd.Series, stuck_pct: float) -> pd.Series:\n",
    "    s = series.copy()\n",
    "    stuck_val = float(np.nanpercentile(s, stuck_pct * 100))\n",
    "    return pd.Series(stuck_val, index=s.index, dtype=\"float32\")\n",
    "\n",
    "def apply_intermittent_dropout(series: pd.Series, dropout_rate: float) -> pd.Series:\n",
    "    s = series.copy().astype(\"float32\")\n",
    "    mask = np.random.rand(len(s)) < dropout_rate\n",
    "    s[mask] = np.nan\n",
    "    return s\n",
    "\n",
    "FAULT_APPLIERS = {\n",
    "    \"drift\":                apply_drift,\n",
    "    \"bias\":                 apply_bias,\n",
    "    \"precision_degradation\": apply_precision_degradation,\n",
    "    \"stuck_at\":             apply_stuck_at,\n",
    "    \"intermittent_dropout\": apply_intermittent_dropout,\n",
    "}\n",
    "\n",
    "print(\"Fault applier functions defined:\")\n",
    "for name in FAULT_APPLIERS:\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731fbd58-d6a6-4244-bbbc-ddeb6358f6b6",
   "metadata": {},
   "source": [
    "# Stage 4 - Inject Faults  \n",
    "Runs fault injection independently per split.  \n",
    "Each split's normal rows receive their own set of fault events with independently placed windows, ensuring no cross-split leakage.  \n",
    "Fault metadata columns are added to track injection details for validation and error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519487fc-e247-4390-aaea-66928bf40405",
   "metadata": {},
   "source": [
    "## 4.1 - Define Fault Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b1e6184-7c9e-425c-a7e5-a698220c8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault orchestrator defined.\n"
     ]
    }
   ],
   "source": [
    "def inject_faults_for_split(\n",
    "    df_split_normal: pd.DataFrame,\n",
    "    eligible_sensors: list[str],\n",
    "    fault_types: dict,\n",
    "    fault_appliers: dict,\n",
    "    target_fault_pct: float,\n",
    "    duration_min: int,\n",
    "    duration_max: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Injects faults into normal rows for a single split.\n",
    "    Computes faults_per_sensor dynamically to hit target_fault_pct of\n",
    "    normal rows. Windows are non-overlapping per sensor.\n",
    "    Returns a DataFrame of fault rows only (label=2) with metadata columns.\n",
    "    \"\"\"\n",
    "    df = df_split_normal.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # Compute faults per sensor to hit target percentage \n",
    "    target_fault_rows  = int(len(df) * target_fault_pct)\n",
    "    avg_duration       = (duration_min + duration_max) / 2\n",
    "    faults_per_sensor  = max(1, int(\n",
    "        target_fault_rows / (len(eligible_sensors) * avg_duration)\n",
    "    ))\n",
    "\n",
    "    fault_rows = []\n",
    "\n",
    "    for sensor in eligible_sensors:\n",
    "        sensor_data = df[df[sensor].notna()].copy()\n",
    "        if len(sensor_data) < duration_max * 2:\n",
    "            continue\n",
    "\n",
    "        timestamps = sensor_data[\"timestamp\"].values\n",
    "        n          = len(timestamps)\n",
    "        occupied   = np.zeros(n, dtype=bool)\n",
    "\n",
    "        for _ in range(faults_per_sensor):\n",
    "            # Sample fault type \n",
    "            fault_type = rng.choice(list(fault_types.keys()))\n",
    "            duration   = int(rng.integers(duration_min, duration_max + 1))\n",
    "\n",
    "            # Find a valid non-overlapping window \n",
    "            placed = False\n",
    "            for _ in range(50):\n",
    "                start_idx = int(rng.integers(0, max(1, n - duration)))\n",
    "                end_idx   = start_idx + duration\n",
    "                if not occupied[start_idx:end_idx].any():\n",
    "                    occupied[start_idx:end_idx] = True\n",
    "                    placed = True\n",
    "                    break\n",
    "\n",
    "            if not placed:\n",
    "                continue\n",
    "\n",
    "            # Extract window rows \n",
    "            window = sensor_data.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "            # Sample and apply fault parameters\n",
    "            cfg = fault_types[fault_type]\n",
    "            if fault_type == \"drift\":\n",
    "                param = float(rng.uniform(*cfg[\"rate_pct_per_min\"]))\n",
    "                window[sensor] = apply_drift(window[sensor], param)\n",
    "            elif fault_type == \"bias\":\n",
    "                param = float(rng.uniform(*cfg[\"offset_pct\"]))\n",
    "                window[sensor] = apply_bias(window[sensor], param)\n",
    "            elif fault_type == \"precision_degradation\":\n",
    "                param = float(rng.uniform(*cfg[\"noise_multiplier\"]))\n",
    "                window[sensor] = apply_precision_degradation(window[sensor], param)\n",
    "            elif fault_type == \"stuck_at\":\n",
    "                param = float(rng.uniform(*cfg[\"stuck_pct\"]))\n",
    "                window[sensor] = apply_stuck_at(window[sensor], param)\n",
    "            elif fault_type == \"intermittent_dropout\":\n",
    "                param = float(rng.uniform(*cfg[\"dropout_rate\"]))\n",
    "                window[sensor] = apply_intermittent_dropout(window[sensor], param)\n",
    "\n",
    "            # ── Assign fault metadata ─────────────────────────────────────────\n",
    "            window[\"label\"]          = 2\n",
    "            window[\"fault_type\"]     = fault_type\n",
    "            window[\"fault_sensor\"]   = sensor\n",
    "            window[\"fault_start\"]    = window[\"timestamp\"].iloc[0]\n",
    "            window[\"fault_end\"]      = window[\"timestamp\"].iloc[-1]\n",
    "            window[\"fault_severity\"] = round(float(param), 4)\n",
    "\n",
    "            fault_rows.append(window)\n",
    "\n",
    "    if not fault_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(fault_rows, ignore_index=True)\n",
    "\n",
    "print(\"Fault orchestrator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f0669-157b-4dd9-a93d-c152eaff7b1a",
   "metadata": {},
   "source": [
    "## 4.2 - Run Injection Per Split  \n",
    "Runs the fault orchestrator for the train split only.\n",
    "The train split gets its own random number generator seeded deterministically from the global seed, ensuring full reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd7a811b-3fbc-4184-8b0d-baa508677ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injecting into train split (1,209,601 normal rows)...\n",
      "  Fault events:  268\n",
      "  Fault rows:    97,606\n",
      "  Fault %:       8.07% of normal rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_seeds = {\"train\": RANDOM_SEED}\n",
    "\n",
    "fault_dfs         = {}\n",
    "injection_summary = {}\n",
    "\n",
    "for split in [\"train\"]:\n",
    "    rng = np.random.default_rng(split_seeds[split])\n",
    "\n",
    "    df_split_normal = df[\n",
    "        (df[\"label\"] == 0) & (df[\"split\"] == split)\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"Injecting into {split} split ({len(df_split_normal):,} normal rows)...\")\n",
    "\n",
    "    fault_df = inject_faults_for_split(\n",
    "        df_split_normal  = df_split_normal,\n",
    "        eligible_sensors = eligible,\n",
    "        fault_types      = FAULT_TYPES,\n",
    "        fault_appliers   = FAULT_APPLIERS,\n",
    "        target_fault_pct = TARGET_FAULT_PCT,\n",
    "        duration_min     = FAULT_DURATION_MIN,\n",
    "        duration_max     = FAULT_DURATION_MAX,\n",
    "        rng              = rng,\n",
    "    )\n",
    "\n",
    "    fault_dfs[split] = fault_df\n",
    "    n_fault_rows     = len(fault_df)\n",
    "    n_fault_events   = 0 if fault_df.empty else fault_df.groupby(\n",
    "        [\"fault_sensor\", \"fault_start\"]\n",
    "    ).ngroups\n",
    "\n",
    "    injection_summary[split] = {\n",
    "        \"normal_rows\":  len(df_split_normal),\n",
    "        \"fault_rows\":   n_fault_rows,\n",
    "        \"fault_events\": n_fault_events,\n",
    "        \"fault_pct\":    round(n_fault_rows / len(df_split_normal) * 100, 2),\n",
    "    }\n",
    "\n",
    "    print(f\"  Fault events:  {n_fault_events}\")\n",
    "    print(f\"  Fault rows:    {n_fault_rows:,}\")\n",
    "    print(f\"  Fault %:       {injection_summary[split]['fault_pct']:.2f}% of normal rows\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab93c02-752f-4087-8bdc-a0f9a7877f43",
   "metadata": {},
   "source": [
    "## 4.3 - Merge Fault Rows Into Main DataFrame\n",
    "Combines the original staged data with the injected fault rows from the train split.\n",
    "The result is a single DataFrame with all three classes: 0=normal, 1=attack, 2=fault.\n",
    "The test split remains attack-only, no synthetic faults are introduced there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d7661b-19bc-4f28-8517-8954ceeff0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after injection: (1480008, 108)\n",
      "\n",
      "Label distribution:\n",
      "  normal   (0): 1,209,601  (81.73%)\n",
      "  attack   (1):   172,801  (11.68%)\n",
      "  fault    (2):    97,606  (6.59%)\n",
      "\n",
      "Label distribution by split:\n",
      "\n",
      "  train:\n",
      "    normal   (0): 1,209,601  (92.53%)\n",
      "    attack   (1):         0  (0.00%)\n",
      "    fault    (2):    97,606  (7.47%)\n",
      "\n",
      "  test:\n",
      "    normal   (0):         0  (0.00%)\n",
      "    attack   (1):   172,801  (100.00%)\n",
      "    fault    (2):         0  (0.00%)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Ensure fault metadata columns exist in original df\n",
    "FAULT_META_COLS = [\"fault_type\", \"fault_sensor\", \"fault_start\", \"fault_end\", \"fault_severity\"]\n",
    "for col in FAULT_META_COLS:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Combine all fault rows across splits \n",
    "all_fault_rows = pd.concat(\n",
    "    [fdf for fdf in fault_dfs.values() if not fdf.empty],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Concatenate — suppress known pandas 2.x deprecation warning \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "    df_injected = pd.concat([df, all_fault_rows], ignore_index=True)\n",
    "\n",
    "df_injected = df_injected.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "df_injected[\"label\"] = df_injected[\"label\"].astype(\"int8\")\n",
    "\n",
    "print(f\"Shape after injection: {df_injected.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label_val, label_name in [(0, \"normal\"), (1, \"attack\"), (2, \"fault\")]:\n",
    "    n   = (df_injected[\"label\"] == label_val).sum()\n",
    "    pct = n / len(df_injected) * 100\n",
    "    print(f\"  {label_name:<8} ({label_val}): {n:>9,}  ({pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nLabel distribution by split:\")\n",
    "for split in [\"train\", \"test\"]:\n",
    "    split_df = df_injected[df_injected[\"split\"] == split]\n",
    "    print(f\"\\n  {split}:\")\n",
    "    for label_val, label_name in [(0, \"normal\"), (1, \"attack\"), (2, \"fault\")]:\n",
    "        n   = (split_df[\"label\"] == label_val).sum()\n",
    "        pct = n / len(split_df) * 100\n",
    "        print(f\"    {label_name:<8} ({label_val}): {n:>9,}  ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61f5cd-5fe0-443c-ae69-3bf5a332c1a6",
   "metadata": {},
   "source": [
    "# Stage 5 - Validate Injection  \n",
    "Runs validation checks on the injected dataset to confirm fault injection behaved correctly before writing anything to disk.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d1ebc-308f-4feb-8dc8-0eca911a034f",
   "metadata": {},
   "source": [
    "## 5.1 - Run Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6a19c1-5e21-494a-9d09-aec40a07cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation status: PASSED\n",
      "  Failures: 0\n",
      "  Warnings: 0\n",
      "\n",
      "All checks passed with no warnings.\n"
     ]
    }
   ],
   "source": [
    "failures = []\n",
    "warnings_list = []\n",
    "\n",
    "# Check 1: Expected labels present in each split\n",
    "expected = {\n",
    "    \"train\": [0, 2],   # normal operation + injected faults\n",
    "    \"test\":  [1],      # attack period only — no normal rows, no synthetic faults\n",
    "}\n",
    "\n",
    "for split, expected_labels in expected.items():\n",
    "    split_df = df_injected[df_injected[\"split\"] == split]\n",
    "    for label_val in expected_labels:\n",
    "        label_name = {0: \"normal\", 1: \"attack\", 2: \"fault\"}[label_val]\n",
    "        n = (split_df[\"label\"] == label_val).sum()\n",
    "        if n == 0:\n",
    "            failures.append(f\"Label {label_val} ({label_name}) missing from {split} split\")\n",
    "            \n",
    "# Check 2: No row has both attack and fault label \n",
    "overlap = df_injected[\n",
    "    (df_injected[\"label\"] == 1) &\n",
    "    (df_injected[\"fault_type\"].notna())\n",
    "]\n",
    "if len(overlap) > 0:\n",
    "    failures.append(f\"{len(overlap)} rows have both attack label and fault metadata\")\n",
    "\n",
    "# Check 3: All fault rows have fault metadata populated \n",
    "fault_rows = df_injected[df_injected[\"label\"] == 2]\n",
    "for col in [\"fault_type\", \"fault_sensor\", \"fault_start\", \"fault_end\", \"fault_severity\"]:\n",
    "    n_null = fault_rows[col].isna().sum()\n",
    "    if n_null > 0:\n",
    "        failures.append(f\"Fault metadata '{col}' has {n_null} NaN values in fault rows\")\n",
    "\n",
    "# Check 4: Normal and attack rows have no fault metadata \n",
    "non_fault = df_injected[df_injected[\"label\"] != 2]\n",
    "for col in [\"fault_type\", \"fault_sensor\"]:\n",
    "    n_populated = non_fault[col].notna().sum()\n",
    "    if n_populated > 0:\n",
    "        failures.append(\n",
    "            f\"Fault metadata '{col}' populated in {n_populated} non-fault rows\"\n",
    "        )\n",
    "\n",
    "# Check 5: Fault severity values are within expected ranges \n",
    "for fault_type, cfg in FAULT_TYPES.items():\n",
    "    ft_rows = fault_rows[fault_rows[\"fault_type\"] == fault_type]\n",
    "    if ft_rows.empty:\n",
    "        warnings_list.append(f\"No fault rows found for fault type '{fault_type}'\")\n",
    "        continue\n",
    "    param_key = list(cfg.keys())[1]\n",
    "    lo, hi    = cfg[param_key]\n",
    "    out_of_range = ft_rows[\n",
    "        (ft_rows[\"fault_severity\"] < lo) | (ft_rows[\"fault_severity\"] > hi)\n",
    "    ]\n",
    "    if len(out_of_range) > 0:\n",
    "        failures.append(\n",
    "            f\"Fault type '{fault_type}': {len(out_of_range)} rows have \"\n",
    "            f\"severity outside expected range [{lo}, {hi}]\"\n",
    "        )\n",
    "\n",
    "# Check 6: Fault rows only in normal-origin rows (no timestamp conflicts)\n",
    "attack_timestamps = set(df_injected[df_injected[\"label\"] == 1][\"timestamp\"].astype(str))\n",
    "fault_timestamps  = set(df_injected[df_injected[\"label\"] == 2][\"timestamp\"].astype(str))\n",
    "ts_overlap = attack_timestamps & fault_timestamps\n",
    "if ts_overlap:\n",
    "    warnings_list.append(\n",
    "        f\"{len(ts_overlap)} timestamps appear in both attack and fault rows\"\n",
    "    )\n",
    "\n",
    "# Report \n",
    "print(f\"Validation status: {'PASSED' if not failures else 'FAILED'}\")\n",
    "print(f\"  Failures: {len(failures)}\")\n",
    "print(f\"  Warnings: {len(warnings_list)}\")\n",
    "\n",
    "if failures:\n",
    "    print(\"\\nFailures:\")\n",
    "    for f in failures:\n",
    "        print(f\"  {f}\")\n",
    "\n",
    "if warnings_list:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for w in warnings_list:\n",
    "        print(f\"  {w}\")\n",
    "\n",
    "if not failures and not warnings_list:\n",
    "    print(\"\\nAll checks passed with no warnings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1491221-b856-456b-8135-38a75e88f8cd",
   "metadata": {},
   "source": [
    "# Stage 6 - Write Outputs  \n",
    "Write the injected parquet and run log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860df64-5808-4132-865f-5a031a295918",
   "metadata": {},
   "source": [
    "## 6.1 - Write Injected Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c92ce36-19e2-406d-acb2-3dafb1b5319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injected parquet written: work/wadi_A1/data/injected/wadi_injected_20260223_141149_utc.parquet\n",
      "Size:                     99.7 MB\n",
      "Shape:                    (1480008, 108)\n"
     ]
    }
   ],
   "source": [
    "injected_path = INJECTED_DIR / f\"wadi_injected_{RUN_ID}.parquet\"\n",
    "df_injected.to_parquet(injected_path, index=False)\n",
    "\n",
    "size_mb = injected_path.stat().st_size / 1e6\n",
    "print(f\"Injected parquet written: {injected_path}\")\n",
    "print(f\"Size:                     {size_mb:.1f} MB\")\n",
    "print(f\"Shape:                    {df_injected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a15d9c-4fb9-4cdb-a93e-b4d3757877a6",
   "metadata": {},
   "source": [
    "## 6.2 - Write Run Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f8d8376-a49b-4ecb-8d30-0679da079c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run log written: work/wadi_A1/data/reference/pipeline_runs/run_20260223_141149_utc.json\n"
     ]
    }
   ],
   "source": [
    "run_log = {\n",
    "    \"run_id\":          RUN_ID,\n",
    "    \"created_at_utc\":  utc_now_iso(),\n",
    "    \"stage\":           \"Notebook 2 — Fault Injection\",\n",
    "    \"dataset\":         DATASET_NAME,\n",
    "    \"random_seed\":     RANDOM_SEED,\n",
    "    \"inputs\": {\n",
    "        \"staged_parquet\": str(staged_path),\n",
    "        \"sensor_cols_ref\": str(sensor_cols_path),\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"injected_parquet\": str(injected_path),\n",
    "    },\n",
    "    \"configuration\": {\n",
    "        \"target_fault_pct\":  TARGET_FAULT_PCT,\n",
    "        \"fault_duration_min\": FAULT_DURATION_MIN,\n",
    "        \"fault_duration_max\": FAULT_DURATION_MAX,\n",
    "        \"min_coverage_ratio\": MIN_COVERAGE_RATIO,\n",
    "        \"binary_max_unique\":  BINARY_MAX_UNIQUE,\n",
    "        \"eligible_sensors\":   len(eligible),\n",
    "        \"ineligible_sensors\": len(ineligible),\n",
    "    },\n",
    "    \"fault_types\": {\n",
    "        name: cfg[\"description\"] for name, cfg in FAULT_TYPES.items()\n",
    "    },\n",
    "    \"injection_summary\": injection_summary,\n",
    "    \"dataset_summary\": {\n",
    "        \"total_rows\":   len(df_injected),\n",
    "        \"total_cols\":   len(df_injected.columns),\n",
    "        \"label_counts\": {\n",
    "            int(k): int(v)\n",
    "            for k, v in df_injected[\"label\"].value_counts().sort_index().items()\n",
    "        },\n",
    "        \"split_label_counts\": {\n",
    "            split: {\n",
    "                int(k): int(v)\n",
    "                for k, v in df_injected[df_injected[\"split\"] == split][\"label\"]\n",
    "                .value_counts().sort_index().items()\n",
    "            }\n",
    "            for split in [\"train\", \"test\"]\n",
    "        },\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"passed\":   True,\n",
    "        \"failures\": 0,\n",
    "        \"warnings\": 0,\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"Faults injected independently per split to prevent cross-split leakage.\",\n",
    "        \"Each split seeded deterministically from global RANDOM_SEED.\",\n",
    "        \"Binary and status columns excluded from injection.\",\n",
    "        \"Fault metadata columns retained for validation and error analysis.\",\n",
    "        \"Fault metadata must be excluded from feature matrix in Notebook 3.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "run_log_path = RUN_DIR / f\"run_{RUN_ID}.json\"\n",
    "write_json(run_log_path, run_log)\n",
    "print(f\"Run log written: {run_log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d5a11-0ec2-4adc-9e4b-a94f7bcade52",
   "metadata": {},
   "source": [
    "# Stage 7 - Reflection  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb13bdf-228a-43a4-9743-b5fc38bd637b",
   "metadata": {},
   "source": [
    "## 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d40d83f-c37a-455b-9ed1-d595d2719f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Reflection\n",
      "============================================================\n",
      "\n",
      "[Row definition]\n",
      "  Each fault row represents one second of sensor readings from a normal operation period with a synthetic fault signature applied to one sensor. Label 2=injected sensor fault. Labels 0=normal and 1=attack are unchanged from the staged parquet.\n",
      "\n",
      "[Eligible sensors]\n",
      "  67 of 98 sensors were eligible for injection. 31 were excluded as binary/status columns (<=5 unique values). Injecting continuous fault types into discrete status columns would produce physically implausible results.\n",
      "\n",
      "[Fault types]\n",
      "  Five fault types implemented with literature-grounded parameter ranges: drift (gradual linear deviation), bias (fixed offset), precision degradation (increased noise), stuck-at (frozen value), and intermittent dropout (NaN). Parameters sampled uniformly within ranges per injection event.\n",
      "\n",
      "[Fault injection scope]\n",
      "  Faults injected into train split only, seeded deterministically (seed=42). The test split contains only real attack rows — no synthetic faults. This matches the evaluation protocol: the model learns fault signatures during training and is tested on its ability to detect real attacks.\n",
      "\n",
      "[Fault density]\n",
      "  Train fault rows: 97,606 (8.1% of train normal rows). Fault density is sufficient for the model to learn fault signatures without overwhelming the normal class.\n",
      "\n",
      "[Fault metadata]\n",
      "  Columns fault_type, fault_sensor, fault_start, fault_end, fault_severity are retained in the injected parquet for validation and error analysis. These must be explicitly excluded from the feature matrix in Notebook 3 — they are injection metadata, not observable signals.\n",
      "\n",
      "[Leakage risks]\n",
      "  No normalization or rolling statistics computed here — deferred to feature engineering. Fault windows placed only within normal rows — no fault overlaps with attack rows confirmed by validation check 6.\n",
      "\n",
      "[Reproducibility]\n",
      "  Full injection reproduced by running this notebook with RANDOM_SEED=42 against staged parquet run ID 20260223_140105_utc. Different seed produces a valid alternative injection for sensitivity analysis.\n",
      "\n",
      "[Next step]\n",
      "  Notebook 3 (Curate/Validate) loads wadi_injected_*.parquet, applies curation, runs validation contracts, performs leakage audit, and writes the final warehouse parquet ready for feature engineering.\n"
     ]
    }
   ],
   "source": [
    "reflection = [\n",
    "    (\"Row definition\",\n",
    "     \"Each fault row represents one second of sensor readings from a normal \"\n",
    "     \"operation period with a synthetic fault signature applied to one sensor. \"\n",
    "     \"Label 2=injected sensor fault. Labels 0=normal and 1=attack are unchanged \"\n",
    "     \"from the staged parquet.\"),\n",
    "\n",
    "    (\"Eligible sensors\",\n",
    "     f\"{len(eligible)} of {len(SENSOR_COLS)} sensors were eligible for injection. \"\n",
    "     f\"{len(ineligible)} were excluded as binary/status columns (<=5 unique values). \"\n",
    "     \"Injecting continuous fault types into discrete status columns would produce \"\n",
    "     \"physically implausible results.\"),\n",
    "\n",
    "    (\"Fault types\",\n",
    "     \"Five fault types implemented with literature-grounded parameter ranges: \"\n",
    "     \"drift (gradual linear deviation), bias (fixed offset), precision degradation \"\n",
    "     \"(increased noise), stuck-at (frozen value), and intermittent dropout (NaN). \"\n",
    "     \"Parameters sampled uniformly within ranges per injection event.\"),\n",
    "\n",
    "    (\"Fault injection scope\",\n",
    "     \"Faults injected into train split only, seeded deterministically (seed=42). \"\n",
    "     \"The test split contains only real attack rows — no synthetic faults. \"\n",
    "     \"This matches the evaluation protocol: the model learns fault signatures \"\n",
    "     \"during training and is tested on its ability to detect real attacks.\"),\n",
    "\n",
    "    (\"Fault density\",\n",
    "     f\"Train fault rows: {injection_summary['train']['fault_rows']:,} \"\n",
    "     f\"({injection_summary['train']['fault_pct']:.1f}% of train normal rows). \"\n",
    "     \"Fault density is sufficient for the model to learn fault signatures \"\n",
    "     \"without overwhelming the normal class.\"),\n",
    "\n",
    "    (\"Fault metadata\",\n",
    "     \"Columns fault_type, fault_sensor, fault_start, fault_end, fault_severity \"\n",
    "     \"are retained in the injected parquet for validation and error analysis. \"\n",
    "     \"These must be explicitly excluded from the feature matrix in Notebook 3 \"\n",
    "     \"— they are injection metadata, not observable signals.\"),\n",
    "\n",
    "    (\"Leakage risks\",\n",
    "     \"No normalization or rolling statistics computed here — deferred to feature \"\n",
    "     \"engineering. Fault windows placed only within normal rows — no fault \"\n",
    "     \"overlaps with attack rows confirmed by validation check 6.\"),\n",
    "\n",
    "    (\"Reproducibility\",\n",
    "     f\"Full injection reproduced by running this notebook with RANDOM_SEED={RANDOM_SEED} \"\n",
    "     f\"against staged parquet run ID {sensor_ref['run_id']}. \"\n",
    "     \"Different seed produces a valid alternative injection for sensitivity analysis.\"),\n",
    "\n",
    "    (\"Next step\",\n",
    "     \"Notebook 3 (Curate/Validate) loads wadi_injected_*.parquet, applies curation, \"\n",
    "     \"runs validation contracts, performs leakage audit, and writes the final \"\n",
    "     \"warehouse parquet ready for feature engineering.\"),\n",
    "]\n",
    "\n",
    "print(\"Pipeline Reflection\")\n",
    "print(\"=\" * 60)\n",
    "for title, content in reflection:\n",
    "    print(f\"\\n[{title}]\")\n",
    "    print(f\"  {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc8491-4bbe-4938-a519-aba43f717789",
   "metadata": {},
   "source": [
    "# This continues in WaDi Pipeline Notebook 3 - Curate/Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd406e-932c-4622-9074-ae9c2778e31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
