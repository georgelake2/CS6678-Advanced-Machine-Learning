{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4537db7-2170-48cf-a58f-a437f44ddba2",
   "metadata": {},
   "source": [
    "# Data Pipeline Template  \n",
    "A repeatable pipeline for building analysis-ready datasets with validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824480f-789a-4443-83f9-b07d5a6febef",
   "metadata": {},
   "source": [
    "## Dataset Overview  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac4ae9-475f-4d51-88ff-960c3f008f58",
   "metadata": {},
   "source": [
    "**Describe the dataset here:**\n",
    "* What does a row represent?\n",
    "* What are the key variables?\n",
    "* What makes this data messy/realistic?\n",
    "* What is the time range?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c95863-e844-468e-a903-a776bc8c2985",
   "metadata": {},
   "source": [
    "## What this Pipeline Produces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52743c-c6a8-457d-9767-191b13325b82",
   "metadata": {},
   "source": [
    "**Artifacts**\n",
    "* `data/raw/` - raw data snapshots + metadata\n",
    "* `data/staged/` - parsed/normalized table (typed, missingness normalized)\n",
    "* `data/warehouse/` - curated table (Parquet; optionally partitioned)\n",
    "* `data/reference/validation_report.json` - contracts + anomaly rates + canaries\n",
    "* `data/reference/pipeline_runs/` - run logs for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a62af-bedb-47dd-9d33-5557204651ce",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc61df1-9476-427f-81da-7f10dd531a47",
   "metadata": {},
   "source": [
    "## 0.1 Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f0380-0e94-4b23-9e70-dcfb963b19e5",
   "metadata": {},
   "source": [
    "Create project directories for the pipeline layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b621c70-ee12-47e7-abd9-d72bdbf75a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: work/hai_21_03\n",
      "Raw: work/hai_21_03/data/raw\n",
      "Staged: work/hai_21_03/data/staged\n",
      "Warehouse: work/hai_21_03/data/warehouse\n",
      "Reference: work/hai_21_03/data/reference\n",
      "Runs: work/hai_21_03/data/reference/pipeline_runs\n"
     ]
    }
   ],
   "source": [
    "# Project structure setup\n",
    "\n",
    "# Import Libraries\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import json\n",
    "import hashlib\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 180)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# Define Paths\n",
    "WORK_DIR = Path(\"work\")\n",
    "PROJECT_DIR = WORK_DIR / \"hai_21_03\"\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGED_DIR = DATA_DIR / \"staged\"\n",
    "WH_DIR = DATA_DIR / \"warehouse\"\n",
    "REF_DIR = DATA_DIR / \"reference\"\n",
    "RUN_DIR = REF_DIR / \"pipeline_runs\"\n",
    "\n",
    "# Create Directories\n",
    "for p in [RAW_DIR, STAGED_DIR, WH_DIR, REF_DIR, RUN_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output\n",
    "print(\"Project:\", PROJECT_DIR)\n",
    "print(\"Raw:\", RAW_DIR)\n",
    "print(\"Staged:\", STAGED_DIR)\n",
    "print(\"Warehouse:\", WH_DIR)\n",
    "print(\"Reference:\", REF_DIR)\n",
    "print(\"Runs:\", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cbbaa-67bc-4d20-85dc-fc97e5330b64",
   "metadata": {},
   "source": [
    "## 0.2 Helper Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d0d49-c983-45ec-a821-7c071c34e5b2",
   "metadata": {},
   "source": [
    "Define reusable helper functions for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d47f2f74-5009-4303-ad75-e218b1ba6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready.\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions\n",
    "class PipelineError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def sha16(x: str) -> str:\n",
    "    return hashlib.sha256(x.encode('utf-8')).hexdigest()[:16]\n",
    "\n",
    "def write_json(path: Path, obj: dict) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, default=str))\n",
    "\n",
    "def read_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "def require_columns(df: pd.DataFrame, cols: list[str], context: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise PipelineError(f'[{context}] Missing required columns: {missing}')\n",
    "\n",
    "def require_unique(df: pd.DataFrame, key: str, context: str) -> None:\n",
    "    if key not in df.columns:\n",
    "        raise PipelineError(f'[{context}] Missing key column \"{key}\"')\n",
    "    dupes = int(df[key].duplicated().sum())\n",
    "    if dupes:\n",
    "        raise PipelineError(f'[{context}] Key \"{key}\" has {dupes} duplicates')\n",
    "\n",
    "print(\"Helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85948cc4-ac8b-4324-ad43-2738876807cd",
   "metadata": {},
   "source": [
    "## 0.3 Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b055d-23cd-4610-bb8a-8d2132af9170",
   "metadata": {},
   "source": [
    "Set pipeline configuration constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4716705-1093-43a6-9b77-27937b38f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Configuration\n",
    "\n",
    "DATASET_NAME = \"HAI-21.03\"\n",
    "DATASET_VERSION = \"21.03\"\n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S_utc\")\n",
    "\n",
    "TRAIN_FILES = [\"train1.csv.gz\", \"train2.csv.gz\", \"train3.csv.gz\"]\n",
    "TEST_FILES = [\"test1.csv.gz\", \"test2.csv.gz\", \"test3.csv.gz\", \"test4.csv.gz\", \"test5.csv.gz\"]\n",
    "ALL_FILES = TRAIN_FILES + TEST_FILES\n",
    "\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "ATTACK_LABEL_COLS = [\"attack\", \"attack_P1\", \"attack_P2\", \"attack_P3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1dafb4-1b61-443e-8959-e14e53f408d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Ingest: Acquire Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe790fff-5053-457e-9815-c003b34ad36e",
   "metadata": {},
   "source": [
    "Download or load raw data and save with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea66b5-de4f-445d-86b5-fe19200eeae3",
   "metadata": {},
   "source": [
    "## 1.1 Fetch Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfbdd2-99cc-4522-a35b-32513d66394e",
   "metadata": {},
   "source": [
    "Download or load the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d038200-e139-4c40-a92e-1184039bfb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [DOWNLOAD] train1.csv.gz ...\n",
      "  [DONE] saved to work/hai_21_03/data/raw/hai_train1_run_20260221_135249_utc.csv.gz\n",
      "  [DOWNLOAD] train2.csv.gz ...\n",
      "  [DONE] saved to work/hai_21_03/data/raw/hai_train2_run_20260221_135249_utc.csv.gz\n",
      "  [DOWNLOAD] train3.csv.gz ...\n",
      "  [DONE] saved to work/hai_21_03/data/raw/hai_train3_run_20260221_135249_utc.csv.gz\n",
      "  [DOWNLOAD] test1.csv.gz ...\n",
      "  [DONE] saved to work/hai_21_03/data/raw/hai_test1_run_20260221_135249_utc.csv.gz\n",
      "  [DOWNLOAD] test2.csv.gz ...\n",
      "  [DONE] saved to work/hai_21_03/data/raw/hai_test2_run_20260221_135249_utc.csv.gz\n",
      "  [DOWNLOAD] test3.csv.gz ...\n",
      "  [DONE] saved to work/hai_21_03/data/raw/hai_test3_run_20260221_135249_utc.csv.gz\n",
      "  [DOWNLOAD] test4.csv.gz ...\n",
      "  [DONE] saved to work/hai_21_03/data/raw/hai_test4_run_20260221_135249_utc.csv.gz\n",
      "  [DOWNLOAD] test5.csv.gz ...\n",
      "  [DONE] saved to work/hai_21_03/data/raw/hai_test5_run_20260221_135249_utc.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# Fetch Dataset\n",
    "\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/icsdataset/hai/master/hai-21.03/\"\n",
    "\n",
    "raw_file_meta = {}\n",
    "\n",
    "for file_name in ALL_FILES:\n",
    "    url = BASE_URL + file_name\n",
    "    dest = RAW_DIR / f\"hai_{file_name.replace('.csv.gz', '')}_run_{RUN_ID}.csv.gz\"\n",
    "\n",
    "    if dest.exists():\n",
    "        print(f\"  [SKIP] {file_name} already exists\")\n",
    "    else:\n",
    "        print(f\"  [DOWNLOAD] {file_name} ...\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(dest, 'wb') as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "        print(f\"  [DONE] saved to {dest}\")\n",
    "\n",
    "    raw_file_meta[file_name] = {\n",
    "        \"source_url\": url,\n",
    "        \"local_path\": str(dest),\n",
    "        \"size_bytes\": dest.stat().st_size,\n",
    "        \"format\": \"csv.gz\",\n",
    "        \"decompression\": \"handled automatically by pandas at read time\",\n",
    "        \"split\": \"train\" if file_name in TRAIN_FILES else \"test\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b18e2-a22a-455a-af46-47ea215eadbf",
   "metadata": {},
   "source": [
    "## 1.2 Write Raw Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1ccae-839f-403f-b773-b9aaa78c35ab",
   "metadata": {},
   "source": [
    "Document the raw data source and retrieval details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e7f45b-d70d-42a9-aa6b-b23bef09e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: work/hai_21_03/data/raw/hai_ingest_meta_run_20260221_135249_utc.json\n",
      "Total files: 8\n",
      "Total size: 187.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Create Raw Metadata\n",
    "\n",
    "ingest_meta = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"version\": DATASET_VERSION,\n",
    "    \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"source\": \"https://github.com/icsdataset/hai\",\n",
    "    \"base_url\": BASE_URL,\n",
    "    \"files\": raw_file_meta,   \n",
    "    \"summary\": {\n",
    "        \"total_files\": len(ALL_FILES),\n",
    "        \"train_files\": len(TRAIN_FILES),\n",
    "        \"test_files\": len(TEST_FILES),\n",
    "        \"total_size_bytes\": sum(v[\"size_bytes\"] for v in raw_file_meta.values())\n",
    "    }\n",
    "}\n",
    "\n",
    "meta_path = RAW_DIR / f\"hai_ingest_meta_run_{RUN_ID}.json\"\n",
    "write_json(meta_path, ingest_meta)\n",
    "\n",
    "print(f\"Saved: {meta_path}\")\n",
    "print(f\"Total files: {ingest_meta['summary']['total_files']}\")\n",
    "print(f\"Total size: {ingest_meta['summary']['total_size_bytes'] / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c0809-0bc3-406a-a4fe-855f3809d04a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Stage: Parse and Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618511b-a720-40cc-913c-aa30040b411e",
   "metadata": {},
   "source": [
    "Convert raw data into a clean, typed DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f996d0-b036-4912-90c6-63490a6856d5",
   "metadata": {},
   "source": [
    "## 2.1 Read Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9bef45-10de-4d6e-bf10-229883f48968",
   "metadata": {},
   "source": [
    "**How do we read it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86bf3757-7e0c-496c-874f-7bee46ec6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw data\n",
    "\n",
    "def read_raw_data(path: Path, split: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, compression='gzip')\n",
    "    df[\"file_source\"] = path.stem.replace(\".csv\", \"\")\n",
    "    df[\"split\"] = split\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6533bda2-cb86-4d27-8b88-c923e346f18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (1323608, 86)\n",
      "float64    57\n",
      "int64      26\n",
      "object      3\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>P1_B2004</th>\n",
       "      <th>P1_B2016</th>\n",
       "      <th>P1_B3004</th>\n",
       "      <th>P1_B3005</th>\n",
       "      <th>P1_B4002</th>\n",
       "      <th>P1_B4005</th>\n",
       "      <th>P1_B400B</th>\n",
       "      <th>P1_B4022</th>\n",
       "      <th>P1_FCV01D</th>\n",
       "      <th>P1_FCV01Z</th>\n",
       "      <th>P1_FCV02D</th>\n",
       "      <th>P1_FCV02Z</th>\n",
       "      <th>P1_FCV03D</th>\n",
       "      <th>P1_FCV03Z</th>\n",
       "      <th>P1_FT01</th>\n",
       "      <th>P1_FT01Z</th>\n",
       "      <th>P1_FT02</th>\n",
       "      <th>P1_FT02Z</th>\n",
       "      <th>P1_FT03</th>\n",
       "      <th>P1_FT03Z</th>\n",
       "      <th>P1_LCV01D</th>\n",
       "      <th>P1_LCV01Z</th>\n",
       "      <th>P1_LIT01</th>\n",
       "      <th>P1_PCV01D</th>\n",
       "      <th>P1_PCV01Z</th>\n",
       "      <th>P1_PCV02D</th>\n",
       "      <th>P1_PCV02Z</th>\n",
       "      <th>P1_PIT01</th>\n",
       "      <th>P1_PIT02</th>\n",
       "      <th>P1_PP01AD</th>\n",
       "      <th>P1_PP01AR</th>\n",
       "      <th>P1_PP01BD</th>\n",
       "      <th>P1_PP01BR</th>\n",
       "      <th>P1_PP02D</th>\n",
       "      <th>P1_PP02R</th>\n",
       "      <th>P1_STSP</th>\n",
       "      <th>P1_TIT01</th>\n",
       "      <th>P1_TIT02</th>\n",
       "      <th>P2_24Vdc</th>\n",
       "      <th>P2_ASD</th>\n",
       "      <th>P2_AutoGO</th>\n",
       "      <th>P2_CO_rpm</th>\n",
       "      <th>P2_Emerg</th>\n",
       "      <th>P2_HILout</th>\n",
       "      <th>P2_MSD</th>\n",
       "      <th>P2_ManualGO</th>\n",
       "      <th>P2_OnOff</th>\n",
       "      <th>P2_RTR</th>\n",
       "      <th>P2_SIT01</th>\n",
       "      <th>P2_SIT02</th>\n",
       "      <th>P2_TripEx</th>\n",
       "      <th>P2_VT01</th>\n",
       "      <th>P2_VTR01</th>\n",
       "      <th>P2_VTR02</th>\n",
       "      <th>P2_VTR03</th>\n",
       "      <th>P2_VTR04</th>\n",
       "      <th>P2_VXT02</th>\n",
       "      <th>P2_VXT03</th>\n",
       "      <th>P2_VYT02</th>\n",
       "      <th>P2_VYT03</th>\n",
       "      <th>P3_FIT01</th>\n",
       "      <th>P3_LCP01D</th>\n",
       "      <th>P3_LCV01D</th>\n",
       "      <th>P3_LH</th>\n",
       "      <th>P3_LIT01</th>\n",
       "      <th>P3_LL</th>\n",
       "      <th>P3_PIT01</th>\n",
       "      <th>P4_HT_FD</th>\n",
       "      <th>P4_HT_LD</th>\n",
       "      <th>P4_HT_PO</th>\n",
       "      <th>P4_HT_PS</th>\n",
       "      <th>P4_LD</th>\n",
       "      <th>P4_ST_FD</th>\n",
       "      <th>P4_ST_GOV</th>\n",
       "      <th>P4_ST_LD</th>\n",
       "      <th>P4_ST_PO</th>\n",
       "      <th>P4_ST_PS</th>\n",
       "      <th>P4_ST_PT01</th>\n",
       "      <th>P4_ST_TT01</th>\n",
       "      <th>attack</th>\n",
       "      <th>attack_P1</th>\n",
       "      <th>attack_P2</th>\n",
       "      <th>attack_P3</th>\n",
       "      <th>file_source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-11 00:00:00</td>\n",
       "      <td>0.10121</td>\n",
       "      <td>1.29784</td>\n",
       "      <td>397.63785</td>\n",
       "      <td>1001.99799</td>\n",
       "      <td>33.6555</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2847.02539</td>\n",
       "      <td>37.14706</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.87531</td>\n",
       "      <td>51.58201</td>\n",
       "      <td>52.80456</td>\n",
       "      <td>166.74039</td>\n",
       "      <td>808.29620</td>\n",
       "      <td>1973.19031</td>\n",
       "      <td>2847.02539</td>\n",
       "      <td>246.43968</td>\n",
       "      <td>1000.44769</td>\n",
       "      <td>8.79882</td>\n",
       "      <td>8.46252</td>\n",
       "      <td>395.19528</td>\n",
       "      <td>39.09198</td>\n",
       "      <td>40.49072</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.01782</td>\n",
       "      <td>1.36810</td>\n",
       "      <td>0.27786</td>\n",
       "      <td>540833</td>\n",
       "      <td>540833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.43700</td>\n",
       "      <td>35.74219</td>\n",
       "      <td>28.02645</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54074.0</td>\n",
       "      <td>0</td>\n",
       "      <td>712.07275</td>\n",
       "      <td>763.19324</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2880</td>\n",
       "      <td>780.0</td>\n",
       "      <td>779.59595</td>\n",
       "      <td>1</td>\n",
       "      <td>11.89504</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-3.0660</td>\n",
       "      <td>-1.2648</td>\n",
       "      <td>4.1758</td>\n",
       "      <td>6.0951</td>\n",
       "      <td>4795.0</td>\n",
       "      <td>10832.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>70</td>\n",
       "      <td>15454.0</td>\n",
       "      <td>20</td>\n",
       "      <td>815.0</td>\n",
       "      <td>-0.00072</td>\n",
       "      <td>0.06511</td>\n",
       "      <td>4.01474</td>\n",
       "      <td>0</td>\n",
       "      <td>301.01636</td>\n",
       "      <td>-0.00297</td>\n",
       "      <td>16495.0</td>\n",
       "      <td>301.35992</td>\n",
       "      <td>305.03113</td>\n",
       "      <td>0</td>\n",
       "      <td>10052.0</td>\n",
       "      <td>27610.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hai_train1_run_20260221_135249_utc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-11 00:00:01</td>\n",
       "      <td>0.10121</td>\n",
       "      <td>1.29692</td>\n",
       "      <td>397.63785</td>\n",
       "      <td>1001.99799</td>\n",
       "      <td>33.6555</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2839.58520</td>\n",
       "      <td>37.14477</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.88294</td>\n",
       "      <td>51.60648</td>\n",
       "      <td>52.78931</td>\n",
       "      <td>168.64778</td>\n",
       "      <td>819.16809</td>\n",
       "      <td>1975.47900</td>\n",
       "      <td>2839.58520</td>\n",
       "      <td>246.43968</td>\n",
       "      <td>1000.01270</td>\n",
       "      <td>8.78811</td>\n",
       "      <td>8.47015</td>\n",
       "      <td>395.14420</td>\n",
       "      <td>39.05680</td>\n",
       "      <td>40.49072</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.01782</td>\n",
       "      <td>1.36810</td>\n",
       "      <td>0.27634</td>\n",
       "      <td>540833</td>\n",
       "      <td>540833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.45227</td>\n",
       "      <td>35.74219</td>\n",
       "      <td>28.02473</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54089.0</td>\n",
       "      <td>0</td>\n",
       "      <td>708.52661</td>\n",
       "      <td>763.19324</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2880</td>\n",
       "      <td>781.0</td>\n",
       "      <td>780.67328</td>\n",
       "      <td>1</td>\n",
       "      <td>11.93421</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.9721</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>3.9259</td>\n",
       "      <td>5.9262</td>\n",
       "      <td>4835.0</td>\n",
       "      <td>10984.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>70</td>\n",
       "      <td>15461.0</td>\n",
       "      <td>20</td>\n",
       "      <td>883.0</td>\n",
       "      <td>-0.00051</td>\n",
       "      <td>0.04340</td>\n",
       "      <td>3.74347</td>\n",
       "      <td>0</td>\n",
       "      <td>297.43567</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>16402.0</td>\n",
       "      <td>297.43567</td>\n",
       "      <td>304.27161</td>\n",
       "      <td>0</td>\n",
       "      <td>10052.0</td>\n",
       "      <td>27610.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hai_train1_run_20260221_135249_utc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-11 00:00:02</td>\n",
       "      <td>0.10121</td>\n",
       "      <td>1.29631</td>\n",
       "      <td>397.63785</td>\n",
       "      <td>1001.99799</td>\n",
       "      <td>33.6555</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2833.26807</td>\n",
       "      <td>37.14325</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.88294</td>\n",
       "      <td>51.57790</td>\n",
       "      <td>52.79694</td>\n",
       "      <td>168.83849</td>\n",
       "      <td>823.51697</td>\n",
       "      <td>1972.42725</td>\n",
       "      <td>2833.26807</td>\n",
       "      <td>246.05821</td>\n",
       "      <td>1000.88245</td>\n",
       "      <td>8.81787</td>\n",
       "      <td>8.47015</td>\n",
       "      <td>395.14420</td>\n",
       "      <td>38.97124</td>\n",
       "      <td>40.49835</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.01782</td>\n",
       "      <td>1.36734</td>\n",
       "      <td>0.27634</td>\n",
       "      <td>540833</td>\n",
       "      <td>540833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.45227</td>\n",
       "      <td>35.74219</td>\n",
       "      <td>28.02817</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54124.0</td>\n",
       "      <td>0</td>\n",
       "      <td>709.15527</td>\n",
       "      <td>763.19324</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2880</td>\n",
       "      <td>780.0</td>\n",
       "      <td>780.06574</td>\n",
       "      <td>1</td>\n",
       "      <td>11.97030</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.9857</td>\n",
       "      <td>-1.4032</td>\n",
       "      <td>3.6489</td>\n",
       "      <td>5.8101</td>\n",
       "      <td>4961.0</td>\n",
       "      <td>11120.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>70</td>\n",
       "      <td>15462.0</td>\n",
       "      <td>20</td>\n",
       "      <td>956.0</td>\n",
       "      <td>-0.00043</td>\n",
       "      <td>0.04340</td>\n",
       "      <td>3.43603</td>\n",
       "      <td>0</td>\n",
       "      <td>298.84619</td>\n",
       "      <td>-0.00145</td>\n",
       "      <td>16379.0</td>\n",
       "      <td>298.66534</td>\n",
       "      <td>303.89179</td>\n",
       "      <td>0</td>\n",
       "      <td>10050.0</td>\n",
       "      <td>27617.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hai_train1_run_20260221_135249_utc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  P1_B2004  P1_B2016   P1_B3004    P1_B3005  P1_B4002  P1_B4005    P1_B400B  P1_B4022  P1_FCV01D  P1_FCV01Z  P1_FCV02D  P1_FCV02Z  P1_FCV03D  P1_FCV03Z  \\\n",
       "0  2020-07-11 00:00:00   0.10121   1.29784  397.63785  1001.99799   33.6555     100.0  2847.02539  37.14706      100.0      100.0        0.0   -1.87531   51.58201   52.80456   \n",
       "1  2020-07-11 00:00:01   0.10121   1.29692  397.63785  1001.99799   33.6555     100.0  2839.58520  37.14477      100.0      100.0        0.0   -1.88294   51.60648   52.78931   \n",
       "2  2020-07-11 00:00:02   0.10121   1.29631  397.63785  1001.99799   33.6555     100.0  2833.26807  37.14325      100.0      100.0        0.0   -1.88294   51.57790   52.79694   \n",
       "\n",
       "     P1_FT01   P1_FT01Z     P1_FT02    P1_FT02Z    P1_FT03    P1_FT03Z  P1_LCV01D  P1_LCV01Z   P1_LIT01  P1_PCV01D  P1_PCV01Z  P1_PCV02D  P1_PCV02Z  P1_PIT01  P1_PIT02  \\\n",
       "0  166.74039  808.29620  1973.19031  2847.02539  246.43968  1000.44769    8.79882    8.46252  395.19528   39.09198   40.49072       12.0   12.01782   1.36810   0.27786   \n",
       "1  168.64778  819.16809  1975.47900  2839.58520  246.43968  1000.01270    8.78811    8.47015  395.14420   39.05680   40.49072       12.0   12.01782   1.36810   0.27634   \n",
       "2  168.83849  823.51697  1972.42725  2833.26807  246.05821  1000.88245    8.81787    8.47015  395.14420   38.97124   40.49835       12.0   12.01782   1.36734   0.27634   \n",
       "\n",
       "   P1_PP01AD  P1_PP01AR  P1_PP01BD  P1_PP01BR  P1_PP02D  P1_PP02R  P1_STSP  P1_TIT01  P1_TIT02  P2_24Vdc  P2_ASD  P2_AutoGO  P2_CO_rpm  P2_Emerg  P2_HILout     P2_MSD  \\\n",
       "0     540833     540833          0          0         1         1        1  35.43700  35.74219  28.02645       0          1    54074.0         0  712.07275  763.19324   \n",
       "1     540833     540833          0          0         1         1        1  35.45227  35.74219  28.02473       0          1    54089.0         0  708.52661  763.19324   \n",
       "2     540833     540833          0          0         1         1        1  35.45227  35.74219  28.02817       0          1    54124.0         0  709.15527  763.19324   \n",
       "\n",
       "   P2_ManualGO  P2_OnOff  P2_RTR  P2_SIT01   P2_SIT02  P2_TripEx   P2_VT01  P2_VTR01  P2_VTR02  P2_VTR03  P2_VTR04  P2_VXT02  P2_VXT03  P2_VYT02  P2_VYT03  P3_FIT01  P3_LCP01D  \\\n",
       "0            0         1    2880     780.0  779.59595          1  11.89504        10        10        10        10   -3.0660   -1.2648    4.1758    6.0951    4795.0    10832.0   \n",
       "1            0         1    2880     781.0  780.67328          1  11.93421        10        10        10        10   -2.9721   -1.3147    3.9259    5.9262    4835.0    10984.0   \n",
       "2            0         1    2880     780.0  780.06574          1  11.97030        10        10        10        10   -2.9857   -1.4032    3.6489    5.8101    4961.0    11120.0   \n",
       "\n",
       "   P3_LCV01D  P3_LH  P3_LIT01  P3_LL  P3_PIT01  P4_HT_FD  P4_HT_LD  P4_HT_PO  P4_HT_PS      P4_LD  P4_ST_FD  P4_ST_GOV   P4_ST_LD   P4_ST_PO  P4_ST_PS  P4_ST_PT01  P4_ST_TT01  \\\n",
       "0      608.0     70   15454.0     20     815.0  -0.00072   0.06511   4.01474         0  301.01636  -0.00297    16495.0  301.35992  305.03113         0     10052.0     27610.0   \n",
       "1      528.0     70   15461.0     20     883.0  -0.00051   0.04340   3.74347         0  297.43567   0.00072    16402.0  297.43567  304.27161         0     10052.0     27610.0   \n",
       "2      464.0     70   15462.0     20     956.0  -0.00043   0.04340   3.43603         0  298.84619  -0.00145    16379.0  298.66534  303.89179         0     10050.0     27617.0   \n",
       "\n",
       "   attack  attack_P1  attack_P2  attack_P3                         file_source  split  \n",
       "0       0          0          0          0  hai_train1_run_20260221_135249_utc  train  \n",
       "1       0          0          0          0  hai_train1_run_20260221_135249_utc  train  \n",
       "2       0          0          0          0  hai_train1_run_20260221_135249_utc  train  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load an concatenate all 8 files\n",
    "\n",
    "dfs = []\n",
    "for file_name in TRAIN_FILES:\n",
    "    path = RAW_DIR / f\"hai_{file_name.replace('.csv.gz', '')}_run_{RUN_ID}.csv.gz\"\n",
    "    dfs.append(read_raw_data(path, split=\"train\"))\n",
    "\n",
    "for file_name in TEST_FILES:\n",
    "    path = RAW_DIR / f\"hai_{file_name.replace('.csv.gz', '')}_run_{RUN_ID}.csv.gz\"\n",
    "    dfs.append(read_raw_data(path, split=\"test\"))\n",
    "\n",
    "df_raw = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Combined shape: {df_raw.shape}\")\n",
    "print(df_raw.dtypes.value_counts())\n",
    "display(df_raw.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5af61f-a2ee-4cfb-9cbc-0beb96f79cc3",
   "metadata": {},
   "source": [
    "## 2.2 Create Staged DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089406e-811e-4fae-a71a-f4d5d9c2d3e5",
   "metadata": {},
   "source": [
    "**How do we transform it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0725f44-2005-4e66-adeb-86861c4e43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create staged DataFrame\n",
    "\n",
    "def staged_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Parse timestamp\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"time\"], format=\"%Y-%m-%d %H:%M:%S\", utc=True)\n",
    "    df = df.drop(columns=[\"time\"])\n",
    "\n",
    "    # 2. Sort by file_source then timestamp (preserve time order within each file)\n",
    "    df = df.sort_values([\"file_source\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    # 3. Derive observation_day (for partitioning and canary checks later)\n",
    "    df[\"observation_day\"] = df[\"timestamp\"].dt.date\n",
    "\n",
    "    # 4. Normalize attack label cols to int8\n",
    "    for col in ATTACK_LABEL_COLS:\n",
    "        df[col] = df[col].astype(\"int8\")\n",
    "\n",
    "    # 5. Create unified attack label (1 if ANY subsystem under attack)\n",
    "    df[\"is_attack\"] = (df[ATTACK_LABEL_COLS].any(axis=1)).astype(\"int8\")\n",
    "\n",
    "    # 6. Cast sensor columnst to float 32\n",
    "    sensor_cols = [c for c in df.columns\n",
    "                    if c not in ATTACK_LABEL_COLS + \n",
    "                    [\"timestamp\", \"observation_day\", \"file_source\", \"split\", \"is_attack\"]]\n",
    "    df[sensor_cols] = df[sensor_cols].astype(\"float32\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1236c66-1f14-4801-9a8c-cd4f3fbfe277",
   "metadata": {},
   "source": [
    "## 2.3 Test Staging Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe17b4-f43c-457c-a296-38fcf6a6a7f4",
   "metadata": {},
   "source": [
    "**Does it work?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e24d076-11e9-4e5f-bc36-fa88052c9d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged_shape: (1323608, 88)\n",
      "float32                79\n",
      "int8                    5\n",
      "object                  3\n",
      "datetime64[ns, UTC]     1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1_B2004</th>\n",
       "      <th>P1_B2016</th>\n",
       "      <th>P1_B3004</th>\n",
       "      <th>P1_B3005</th>\n",
       "      <th>P1_B4002</th>\n",
       "      <th>P1_B4005</th>\n",
       "      <th>P1_B400B</th>\n",
       "      <th>P1_B4022</th>\n",
       "      <th>P1_FCV01D</th>\n",
       "      <th>P1_FCV01Z</th>\n",
       "      <th>P1_FCV02D</th>\n",
       "      <th>P1_FCV02Z</th>\n",
       "      <th>P1_FCV03D</th>\n",
       "      <th>P1_FCV03Z</th>\n",
       "      <th>P1_FT01</th>\n",
       "      <th>P1_FT01Z</th>\n",
       "      <th>P1_FT02</th>\n",
       "      <th>P1_FT02Z</th>\n",
       "      <th>P1_FT03</th>\n",
       "      <th>P1_FT03Z</th>\n",
       "      <th>P1_LCV01D</th>\n",
       "      <th>P1_LCV01Z</th>\n",
       "      <th>P1_LIT01</th>\n",
       "      <th>P1_PCV01D</th>\n",
       "      <th>P1_PCV01Z</th>\n",
       "      <th>P1_PCV02D</th>\n",
       "      <th>P1_PCV02Z</th>\n",
       "      <th>P1_PIT01</th>\n",
       "      <th>P1_PIT02</th>\n",
       "      <th>P1_PP01AD</th>\n",
       "      <th>P1_PP01AR</th>\n",
       "      <th>P1_PP01BD</th>\n",
       "      <th>P1_PP01BR</th>\n",
       "      <th>P1_PP02D</th>\n",
       "      <th>P1_PP02R</th>\n",
       "      <th>P1_STSP</th>\n",
       "      <th>P1_TIT01</th>\n",
       "      <th>P1_TIT02</th>\n",
       "      <th>P2_24Vdc</th>\n",
       "      <th>P2_ASD</th>\n",
       "      <th>P2_AutoGO</th>\n",
       "      <th>P2_CO_rpm</th>\n",
       "      <th>P2_Emerg</th>\n",
       "      <th>P2_HILout</th>\n",
       "      <th>P2_MSD</th>\n",
       "      <th>P2_ManualGO</th>\n",
       "      <th>P2_OnOff</th>\n",
       "      <th>P2_RTR</th>\n",
       "      <th>P2_SIT01</th>\n",
       "      <th>P2_SIT02</th>\n",
       "      <th>P2_TripEx</th>\n",
       "      <th>P2_VT01</th>\n",
       "      <th>P2_VTR01</th>\n",
       "      <th>P2_VTR02</th>\n",
       "      <th>P2_VTR03</th>\n",
       "      <th>P2_VTR04</th>\n",
       "      <th>P2_VXT02</th>\n",
       "      <th>P2_VXT03</th>\n",
       "      <th>P2_VYT02</th>\n",
       "      <th>P2_VYT03</th>\n",
       "      <th>P3_FIT01</th>\n",
       "      <th>P3_LCP01D</th>\n",
       "      <th>P3_LCV01D</th>\n",
       "      <th>P3_LH</th>\n",
       "      <th>P3_LIT01</th>\n",
       "      <th>P3_LL</th>\n",
       "      <th>P3_PIT01</th>\n",
       "      <th>P4_HT_FD</th>\n",
       "      <th>P4_HT_LD</th>\n",
       "      <th>P4_HT_PO</th>\n",
       "      <th>P4_HT_PS</th>\n",
       "      <th>P4_LD</th>\n",
       "      <th>P4_ST_FD</th>\n",
       "      <th>P4_ST_GOV</th>\n",
       "      <th>P4_ST_LD</th>\n",
       "      <th>P4_ST_PO</th>\n",
       "      <th>P4_ST_PS</th>\n",
       "      <th>P4_ST_PT01</th>\n",
       "      <th>P4_ST_TT01</th>\n",
       "      <th>attack</th>\n",
       "      <th>attack_P1</th>\n",
       "      <th>attack_P2</th>\n",
       "      <th>attack_P3</th>\n",
       "      <th>file_source</th>\n",
       "      <th>split</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>observation_day</th>\n",
       "      <th>is_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10178</td>\n",
       "      <td>1.58771</td>\n",
       "      <td>403.788544</td>\n",
       "      <td>985.373535</td>\n",
       "      <td>32.595268</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2839.585205</td>\n",
       "      <td>36.810101</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.916077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.86768</td>\n",
       "      <td>50.907261</td>\n",
       "      <td>51.950069</td>\n",
       "      <td>176.086426</td>\n",
       "      <td>845.695496</td>\n",
       "      <td>1978.721558</td>\n",
       "      <td>2843.375488</td>\n",
       "      <td>243.388016</td>\n",
       "      <td>989.141174</td>\n",
       "      <td>10.89290</td>\n",
       "      <td>10.8429</td>\n",
       "      <td>402.709473</td>\n",
       "      <td>40.741249</td>\n",
       "      <td>41.32233</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.26196</td>\n",
       "      <td>1.34293</td>\n",
       "      <td>0.27557</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.887699</td>\n",
       "      <td>35.147099</td>\n",
       "      <td>28.03162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>725.213623</td>\n",
       "      <td>763.193237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>789.765076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.91040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.8687</td>\n",
       "      <td>-1.0189</td>\n",
       "      <td>3.7751</td>\n",
       "      <td>5.6330</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>15888.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>18082.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>76.801208</td>\n",
       "      <td>73.585808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>464.066101</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>386.266663</td>\n",
       "      <td>380.316833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10044.0</td>\n",
       "      <td>27567.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hai_test1_run_20260221_135249_utc</td>\n",
       "      <td>test</td>\n",
       "      <td>2020-07-07 15:00:00+00:00</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10178</td>\n",
       "      <td>1.58725</td>\n",
       "      <td>403.788544</td>\n",
       "      <td>985.373535</td>\n",
       "      <td>32.595268</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2843.375488</td>\n",
       "      <td>36.808949</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.916077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.86768</td>\n",
       "      <td>50.746071</td>\n",
       "      <td>51.965328</td>\n",
       "      <td>173.797562</td>\n",
       "      <td>840.477051</td>\n",
       "      <td>1986.923218</td>\n",
       "      <td>2845.060059</td>\n",
       "      <td>243.006561</td>\n",
       "      <td>992.620178</td>\n",
       "      <td>10.80512</td>\n",
       "      <td>10.8429</td>\n",
       "      <td>402.811737</td>\n",
       "      <td>40.861240</td>\n",
       "      <td>41.32233</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.26196</td>\n",
       "      <td>1.34216</td>\n",
       "      <td>0.27710</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.887699</td>\n",
       "      <td>35.147099</td>\n",
       "      <td>28.02301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>721.740723</td>\n",
       "      <td>763.193237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>789.0</td>\n",
       "      <td>789.131470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.98856</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.9842</td>\n",
       "      <td>-1.2637</td>\n",
       "      <td>3.1689</td>\n",
       "      <td>5.4158</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>15952.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>18043.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>76.924187</td>\n",
       "      <td>73.893250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>464.228882</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>20489.0</td>\n",
       "      <td>386.302856</td>\n",
       "      <td>380.027466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>27564.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hai_test1_run_20260221_135249_utc</td>\n",
       "      <td>test</td>\n",
       "      <td>2020-07-07 15:00:01+00:00</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10178</td>\n",
       "      <td>1.59519</td>\n",
       "      <td>403.788544</td>\n",
       "      <td>985.373535</td>\n",
       "      <td>32.595268</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2845.060059</td>\n",
       "      <td>36.828789</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.916077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.86768</td>\n",
       "      <td>50.662289</td>\n",
       "      <td>51.965328</td>\n",
       "      <td>174.560516</td>\n",
       "      <td>835.258423</td>\n",
       "      <td>1978.721558</td>\n",
       "      <td>2837.339111</td>\n",
       "      <td>242.815857</td>\n",
       "      <td>993.924683</td>\n",
       "      <td>10.80029</td>\n",
       "      <td>10.8429</td>\n",
       "      <td>402.760620</td>\n",
       "      <td>41.029060</td>\n",
       "      <td>41.32233</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.26196</td>\n",
       "      <td>1.34369</td>\n",
       "      <td>0.27710</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.887699</td>\n",
       "      <td>35.147099</td>\n",
       "      <td>28.02993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>718.157959</td>\n",
       "      <td>763.193237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>785.816528</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.97400</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-3.4939</td>\n",
       "      <td>-1.5398</td>\n",
       "      <td>2.9615</td>\n",
       "      <td>5.5532</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>18024.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>77.047150</td>\n",
       "      <td>74.200684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>466.905334</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>20604.0</td>\n",
       "      <td>389.738831</td>\n",
       "      <td>381.528503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10037.0</td>\n",
       "      <td>27565.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hai_test1_run_20260221_135249_utc</td>\n",
       "      <td>test</td>\n",
       "      <td>2020-07-07 15:00:02+00:00</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   P1_B2004  P1_B2016    P1_B3004    P1_B3005   P1_B4002  P1_B4005     P1_B400B   P1_B4022  P1_FCV01D  P1_FCV01Z  P1_FCV02D  P1_FCV02Z  P1_FCV03D  P1_FCV03Z     P1_FT01  \\\n",
       "0   0.10178   1.58771  403.788544  985.373535  32.595268     100.0  2839.585205  36.810101      100.0  99.916077        0.0   -1.86768  50.907261  51.950069  176.086426   \n",
       "1   0.10178   1.58725  403.788544  985.373535  32.595268     100.0  2843.375488  36.808949      100.0  99.916077        0.0   -1.86768  50.746071  51.965328  173.797562   \n",
       "2   0.10178   1.59519  403.788544  985.373535  32.595268     100.0  2845.060059  36.828789      100.0  99.916077        0.0   -1.86768  50.662289  51.965328  174.560516   \n",
       "\n",
       "     P1_FT01Z      P1_FT02     P1_FT02Z     P1_FT03    P1_FT03Z  P1_LCV01D  P1_LCV01Z    P1_LIT01  P1_PCV01D  P1_PCV01Z  P1_PCV02D  P1_PCV02Z  P1_PIT01  P1_PIT02  P1_PP01AD  \\\n",
       "0  845.695496  1978.721558  2843.375488  243.388016  989.141174   10.89290    10.8429  402.709473  40.741249   41.32233       12.0   12.26196   1.34293   0.27557   540833.0   \n",
       "1  840.477051  1986.923218  2845.060059  243.006561  992.620178   10.80512    10.8429  402.811737  40.861240   41.32233       12.0   12.26196   1.34216   0.27710   540833.0   \n",
       "2  835.258423  1978.721558  2837.339111  242.815857  993.924683   10.80029    10.8429  402.760620  41.029060   41.32233       12.0   12.26196   1.34369   0.27710   540833.0   \n",
       "\n",
       "   P1_PP01AR  P1_PP01BD  P1_PP01BR  P1_PP02D  P1_PP02R  P1_STSP   P1_TIT01   P1_TIT02  P2_24Vdc  P2_ASD  P2_AutoGO  P2_CO_rpm  P2_Emerg   P2_HILout      P2_MSD  P2_ManualGO  \\\n",
       "0   540833.0        0.0        0.0       1.0       1.0      1.0  34.887699  35.147099  28.03162     0.0        1.0    54116.0       0.0  725.213623  763.193237          0.0   \n",
       "1   540833.0        0.0        0.0       1.0       1.0      1.0  34.887699  35.147099  28.02301     0.0        1.0    54114.0       0.0  721.740723  763.193237          0.0   \n",
       "2   540833.0        0.0        0.0       1.0       1.0      1.0  34.887699  35.147099  28.02993     0.0        1.0    54082.0       0.0  718.157959  763.193237          0.0   \n",
       "\n",
       "   P2_OnOff  P2_RTR  P2_SIT01    P2_SIT02  P2_TripEx   P2_VT01  P2_VTR01  P2_VTR02  P2_VTR03  P2_VTR04  P2_VXT02  P2_VXT03  P2_VYT02  P2_VYT03  P3_FIT01  P3_LCP01D  P3_LCV01D  \\\n",
       "0       1.0  2880.0     790.0  789.765076        1.0  11.91040      10.0      10.0      10.0      10.0   -2.8687   -1.0189    3.7751    5.6330     -25.0      688.0    15888.0   \n",
       "1       1.0  2880.0     789.0  789.131470        1.0  11.98856      10.0      10.0      10.0      10.0   -2.9842   -1.2637    3.1689    5.4158     -25.0      648.0    15952.0   \n",
       "2       1.0  2880.0     786.0  785.816528        1.0  11.97400      10.0      10.0      10.0      10.0   -3.4939   -1.5398    2.9615    5.5532     -25.0      616.0    16000.0   \n",
       "\n",
       "   P3_LH  P3_LIT01  P3_LL  P3_PIT01  P4_HT_FD   P4_HT_LD   P4_HT_PO  P4_HT_PS       P4_LD  P4_ST_FD  P4_ST_GOV    P4_ST_LD    P4_ST_PO  P4_ST_PS  P4_ST_PT01  P4_ST_TT01  attack  \\\n",
       "0   70.0   18082.0   20.0     -23.0   0.00029  76.801208  73.585808       0.0  464.066101    0.0047    20469.0  386.266663  380.316833       0.0     10044.0     27567.0       0   \n",
       "1   70.0   18043.0   20.0     -23.0   0.00051  76.924187  73.893250       0.0  464.228882    0.0021    20489.0  386.302856  380.027466       0.0     10040.0     27564.0       0   \n",
       "2   70.0   18024.0   20.0     -23.0   0.00022  77.047150  74.200684       0.0  466.905334    0.0013    20604.0  389.738831  381.528503       0.0     10037.0     27565.0       0   \n",
       "\n",
       "   attack_P1  attack_P2  attack_P3                        file_source split                 timestamp observation_day  is_attack  \n",
       "0          0          0          0  hai_test1_run_20260221_135249_utc  test 2020-07-07 15:00:00+00:00      2020-07-07          0  \n",
       "1          0          0          0  hai_test1_run_20260221_135249_utc  test 2020-07-07 15:00:01+00:00      2020-07-07          0  \n",
       "2          0          0          0  hai_test1_run_20260221_135249_utc  test 2020-07-07 15:00:02+00:00      2020-07-07          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test staged function\n",
    "\n",
    "df_staged = staged_dataframe(df_raw)\n",
    "print(f\"Staged_shape: {df_staged.shape}\")\n",
    "print(df_staged.dtypes.value_counts())\n",
    "display(df_staged.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59e382-653a-4b5b-9f32-9a6ac7a08d1d",
   "metadata": {},
   "source": [
    "## 2.4 Write Staged Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344bc0f6-55f9-426f-8fe6-93b12a403eca",
   "metadata": {},
   "source": [
    "**How do we save it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6405a27-d74b-42aa-8b9c-ca70233a2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved staged: work/hai_21_03/data/staged/hai_staged_run_20260221_135249_utc.parquet\n",
      "File size: 119.9 MB\n",
      "Saved metadata: work/hai_21_03/data/staged/hai_staged_meta_run_20260221_135249_utc.json\n"
     ]
    }
   ],
   "source": [
    "# Write staged outputs\n",
    "\n",
    "\n",
    "# Save staged Parquet\n",
    "staged_path = STAGED_DIR / f\"hai_staged_run_{RUN_ID}.parquet\"\n",
    "df_staged.to_parquet(staged_path, index=False)\n",
    "print(f\"Saved staged: {staged_path}\")\n",
    "print(f\"File size: {staged_path.stat().st_size / 1e6:.1f} MB\")\n",
    "\n",
    "# Write staged metadata\n",
    "staged_meta = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"staged_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"source_files\": len(ALL_FILES),\n",
    "    \"staged_path\": str(staged_path),\n",
    "    \"shape\": {\n",
    "        \"rows\": len(df_staged),\n",
    "        \"cols\": len(df_staged.columns)\n",
    "    },\n",
    "    \"splits\": df_staged[\"split\"].value_counts().to_dict(),\n",
    "    \"is_attack_counts\": df_staged[\"is_attack\"].value_counts().to_dict(),\n",
    "    \"dtypes\": df_staged.dtypes.astype(str).to_dict(),\n",
    "    \"columns\": df_staged.columns.tolist()\n",
    "}\n",
    "\n",
    "staged_meta_path = STAGED_DIR / f\"hai_staged_meta_run_{RUN_ID}.json\"\n",
    "write_json(staged_meta_path, staged_meta)\n",
    "print(f\"Saved metadata: {staged_meta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01633f6f-4d7c-4d68-bbce-096e76151a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f96fdce6-0010-4874-8ee2-2c234319eb4d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Curate: Analysis-Ready Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9779a-105b-429b-a356-e19a9e1e0e53",
   "metadata": {},
   "source": [
    "Add engineered features and data quality flags.  \n",
    "\n",
    "We will be using the dataset after injecting sensor failures.  \n",
    "It now contains normal data, malicious attacks, and sensor failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fd4a765-999b-49ee-8a67-d83725313566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: work/hai_21_03/data/injected/hai_21_03_injected_20260221_135449_utc.parquet\n",
      "Loaded injected data: (1323608, 94)\n",
      "Sensor columns identified: 79\n"
     ]
    }
   ],
   "source": [
    "# Get Injected Files\n",
    "INJECTED_DIR = DATA_DIR / \"injected\"\n",
    "injected_files = sorted(INJECTED_DIR.glob(\"hai_21_03_*.parquet\"))\n",
    "\n",
    "if not injected_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No injected parquet found in {INJECTED_DIR}. \"\n",
    "        \"Run the data failure injection notebook through completion\"\n",
    "    )\n",
    "\n",
    "# Use the most recent injected file (last alphabetically = latest RUN_ID)\n",
    "injected_path = injected_files[-1]\n",
    "print(f\"Loading: {injected_path}\")\n",
    "\n",
    "# Load into dataframe\n",
    "df_injected = pd.read_parquet(injected_path)\n",
    "print(f\"Loaded injected data: {df_injected.shape}\")\n",
    "\n",
    "# Identify sensor columns: all float32 columns that are not metadata\n",
    "SENSOR_COLS = [c for c in df_injected.columns\n",
    "              if c not in [\n",
    "                  'attack', 'attack_P1', 'attack_P2', 'attack_P3',\n",
    "                  'file_source', 'split', 'timestamp', 'observation_day',\n",
    "                  'is_attack', 'label',\n",
    "                  'fault_type', 'fault_sensor', 'fault_start', 'fault_end', 'fault_severity'\n",
    "              ]]\n",
    "\n",
    "print(f\"Sensor columns identified: {len(SENSOR_COLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07127c-a7cf-4073-8070-c490f475a669",
   "metadata": {},
   "source": [
    "## 3.1 Create Curated DataFrame   \n",
    "Transforms the injected staged DataFrame into a warehouse-ready curated DataFrame by adding time-derived columns and enforcing schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce0eae4d-017b-444c-93f9-0f72f7dcb477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curate data function ready\n"
     ]
    }
   ],
   "source": [
    "def curate_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Confirm required columns are present\n",
    "    required = (\n",
    "        ['timestamp', 'label', 'is_attack',\n",
    "         'fault_type', 'fault_sensor', 'fault_start', 'fault_end', 'fault_severity']\n",
    "        + SENSOR_COLS\n",
    "    )\n",
    "    require_columns(df, required, context=\"curate_data\")\n",
    "\n",
    "    # 2. Confirm label is correctly typed and has no NaNs\n",
    "    if df['label'].isna().any():\n",
    "        raise PipelineError(\"curate_data: label column contains NaN values\")\n",
    "    df['label'] = df['label'].astype('int8')\n",
    "\n",
    "    # 3. Add seconds_since_start - Captures system startup vs steady-state behavior\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "\n",
    "    t_min = df['timestamp'].min()\n",
    "    df['seconds_since_start'] = (\n",
    "        (df['timestamp'] - t_min).dt.total_seconds().astype('float32')\n",
    "    )\n",
    "\n",
    "    # 4. Drop raw HAI attack flags (replaced by unified label column)\n",
    "    df = df.drop(columns=['attack', 'attack_P1', 'attack_P2', 'attack_P3'])\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"curate data function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc68184-a470-43c0-9737-c50266b5b1f3",
   "metadata": {},
   "source": [
    "## 3.2 Test Curation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a7a1373-2942-421f-b923-585707df908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (1323608, 94)\n",
      "Output shape: (1323608, 91)\n",
      "\n",
      "New columns added:\n",
      "  ['seconds_since_start']\n",
      "\n",
      "Dropped columns:\n",
      "  ['attack', 'attack_P1', 'attack_P2', 'attack_P3']\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "normal    1178988\n",
      "attack       8947\n",
      "fault      135673\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dtype check:\n",
      "label                     int8\n",
      "seconds_since_start    float32\n",
      "dtype: object\n",
      "\n",
      "Seconds since start range: 0  2923200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1_B2004</th>\n",
       "      <th>P1_B2016</th>\n",
       "      <th>P1_B3004</th>\n",
       "      <th>P1_B3005</th>\n",
       "      <th>P1_B4002</th>\n",
       "      <th>P1_B4005</th>\n",
       "      <th>P1_B400B</th>\n",
       "      <th>P1_B4022</th>\n",
       "      <th>P1_FCV01D</th>\n",
       "      <th>P1_FCV01Z</th>\n",
       "      <th>P1_FCV02D</th>\n",
       "      <th>P1_FCV02Z</th>\n",
       "      <th>P1_FCV03D</th>\n",
       "      <th>P1_FCV03Z</th>\n",
       "      <th>P1_FT01</th>\n",
       "      <th>P1_FT01Z</th>\n",
       "      <th>P1_FT02</th>\n",
       "      <th>P1_FT02Z</th>\n",
       "      <th>P1_FT03</th>\n",
       "      <th>P1_FT03Z</th>\n",
       "      <th>P1_LCV01D</th>\n",
       "      <th>P1_LCV01Z</th>\n",
       "      <th>P1_LIT01</th>\n",
       "      <th>P1_PCV01D</th>\n",
       "      <th>P1_PCV01Z</th>\n",
       "      <th>P1_PCV02D</th>\n",
       "      <th>P1_PCV02Z</th>\n",
       "      <th>P1_PIT01</th>\n",
       "      <th>P1_PIT02</th>\n",
       "      <th>P1_PP01AD</th>\n",
       "      <th>P1_PP01AR</th>\n",
       "      <th>P1_PP01BD</th>\n",
       "      <th>P1_PP01BR</th>\n",
       "      <th>P1_PP02D</th>\n",
       "      <th>P1_PP02R</th>\n",
       "      <th>P1_STSP</th>\n",
       "      <th>P1_TIT01</th>\n",
       "      <th>P1_TIT02</th>\n",
       "      <th>P2_24Vdc</th>\n",
       "      <th>P2_ASD</th>\n",
       "      <th>P2_AutoGO</th>\n",
       "      <th>P2_CO_rpm</th>\n",
       "      <th>P2_Emerg</th>\n",
       "      <th>P2_HILout</th>\n",
       "      <th>P2_MSD</th>\n",
       "      <th>P2_ManualGO</th>\n",
       "      <th>P2_OnOff</th>\n",
       "      <th>P2_RTR</th>\n",
       "      <th>P2_SIT01</th>\n",
       "      <th>P2_SIT02</th>\n",
       "      <th>P2_TripEx</th>\n",
       "      <th>P2_VT01</th>\n",
       "      <th>P2_VTR01</th>\n",
       "      <th>P2_VTR02</th>\n",
       "      <th>P2_VTR03</th>\n",
       "      <th>P2_VTR04</th>\n",
       "      <th>P2_VXT02</th>\n",
       "      <th>P2_VXT03</th>\n",
       "      <th>P2_VYT02</th>\n",
       "      <th>P2_VYT03</th>\n",
       "      <th>P3_FIT01</th>\n",
       "      <th>P3_LCP01D</th>\n",
       "      <th>P3_LCV01D</th>\n",
       "      <th>P3_LH</th>\n",
       "      <th>P3_LIT01</th>\n",
       "      <th>P3_LL</th>\n",
       "      <th>P3_PIT01</th>\n",
       "      <th>P4_HT_FD</th>\n",
       "      <th>P4_HT_LD</th>\n",
       "      <th>P4_HT_PO</th>\n",
       "      <th>P4_HT_PS</th>\n",
       "      <th>P4_LD</th>\n",
       "      <th>P4_ST_FD</th>\n",
       "      <th>P4_ST_GOV</th>\n",
       "      <th>P4_ST_LD</th>\n",
       "      <th>P4_ST_PO</th>\n",
       "      <th>P4_ST_PS</th>\n",
       "      <th>P4_ST_PT01</th>\n",
       "      <th>P4_ST_TT01</th>\n",
       "      <th>file_source</th>\n",
       "      <th>split</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>observation_day</th>\n",
       "      <th>is_attack</th>\n",
       "      <th>label</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_sensor</th>\n",
       "      <th>fault_start</th>\n",
       "      <th>fault_end</th>\n",
       "      <th>fault_severity</th>\n",
       "      <th>seconds_since_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10178</td>\n",
       "      <td>1.58771</td>\n",
       "      <td>403.788544</td>\n",
       "      <td>985.373535</td>\n",
       "      <td>32.595268</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2839.585205</td>\n",
       "      <td>36.810101</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.916077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.86768</td>\n",
       "      <td>50.907261</td>\n",
       "      <td>51.950069</td>\n",
       "      <td>176.086426</td>\n",
       "      <td>845.695496</td>\n",
       "      <td>1978.721558</td>\n",
       "      <td>2843.375488</td>\n",
       "      <td>243.388016</td>\n",
       "      <td>989.141174</td>\n",
       "      <td>10.89290</td>\n",
       "      <td>10.8429</td>\n",
       "      <td>402.709473</td>\n",
       "      <td>40.741249</td>\n",
       "      <td>41.32233</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.26196</td>\n",
       "      <td>1.34293</td>\n",
       "      <td>0.27557</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.887699</td>\n",
       "      <td>35.147099</td>\n",
       "      <td>28.03162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>725.213623</td>\n",
       "      <td>763.193237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>789.765076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.91040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.8687</td>\n",
       "      <td>-1.0189</td>\n",
       "      <td>3.7751</td>\n",
       "      <td>5.6330</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>15888.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>18082.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>76.801208</td>\n",
       "      <td>73.585808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>464.066101</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>386.266663</td>\n",
       "      <td>380.316833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10044.0</td>\n",
       "      <td>27567.0</td>\n",
       "      <td>hai_test1_run_20260221_135249_utc</td>\n",
       "      <td>test</td>\n",
       "      <td>2020-07-07 15:00:00+00:00</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10178</td>\n",
       "      <td>1.58725</td>\n",
       "      <td>403.788544</td>\n",
       "      <td>985.373535</td>\n",
       "      <td>32.595268</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2843.375488</td>\n",
       "      <td>36.808949</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.916077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.86768</td>\n",
       "      <td>50.746071</td>\n",
       "      <td>51.965328</td>\n",
       "      <td>173.797562</td>\n",
       "      <td>840.477051</td>\n",
       "      <td>1986.923218</td>\n",
       "      <td>2845.060059</td>\n",
       "      <td>243.006561</td>\n",
       "      <td>992.620178</td>\n",
       "      <td>10.80512</td>\n",
       "      <td>10.8429</td>\n",
       "      <td>402.811737</td>\n",
       "      <td>40.861240</td>\n",
       "      <td>41.32233</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.26196</td>\n",
       "      <td>1.34216</td>\n",
       "      <td>0.27710</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.887699</td>\n",
       "      <td>35.147099</td>\n",
       "      <td>28.02301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>721.740723</td>\n",
       "      <td>763.193237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>789.0</td>\n",
       "      <td>789.131470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.98856</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.9842</td>\n",
       "      <td>-1.2637</td>\n",
       "      <td>3.1689</td>\n",
       "      <td>5.4158</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>15952.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>18043.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>76.924187</td>\n",
       "      <td>73.893250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>464.228882</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>20489.0</td>\n",
       "      <td>386.302856</td>\n",
       "      <td>380.027466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>27564.0</td>\n",
       "      <td>hai_test1_run_20260221_135249_utc</td>\n",
       "      <td>test</td>\n",
       "      <td>2020-07-07 15:00:01+00:00</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10178</td>\n",
       "      <td>1.59519</td>\n",
       "      <td>403.788544</td>\n",
       "      <td>985.373535</td>\n",
       "      <td>32.595268</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2845.060059</td>\n",
       "      <td>36.828789</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.916077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.86768</td>\n",
       "      <td>50.662289</td>\n",
       "      <td>51.965328</td>\n",
       "      <td>174.560516</td>\n",
       "      <td>835.258423</td>\n",
       "      <td>1978.721558</td>\n",
       "      <td>2837.339111</td>\n",
       "      <td>242.815857</td>\n",
       "      <td>993.924683</td>\n",
       "      <td>10.80029</td>\n",
       "      <td>10.8429</td>\n",
       "      <td>402.760620</td>\n",
       "      <td>41.029060</td>\n",
       "      <td>41.32233</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.26196</td>\n",
       "      <td>1.34369</td>\n",
       "      <td>0.27710</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>540833.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.887699</td>\n",
       "      <td>35.147099</td>\n",
       "      <td>28.02993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>718.157959</td>\n",
       "      <td>763.193237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>785.816528</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.97400</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-3.4939</td>\n",
       "      <td>-1.5398</td>\n",
       "      <td>2.9615</td>\n",
       "      <td>5.5532</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>18024.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>77.047150</td>\n",
       "      <td>74.200684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>466.905334</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>20604.0</td>\n",
       "      <td>389.738831</td>\n",
       "      <td>381.528503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10037.0</td>\n",
       "      <td>27565.0</td>\n",
       "      <td>hai_test1_run_20260221_135249_utc</td>\n",
       "      <td>test</td>\n",
       "      <td>2020-07-07 15:00:02+00:00</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   P1_B2004  P1_B2016    P1_B3004    P1_B3005   P1_B4002  P1_B4005     P1_B400B   P1_B4022  P1_FCV01D  P1_FCV01Z  P1_FCV02D  P1_FCV02Z  P1_FCV03D  P1_FCV03Z     P1_FT01  \\\n",
       "0   0.10178   1.58771  403.788544  985.373535  32.595268     100.0  2839.585205  36.810101      100.0  99.916077        0.0   -1.86768  50.907261  51.950069  176.086426   \n",
       "1   0.10178   1.58725  403.788544  985.373535  32.595268     100.0  2843.375488  36.808949      100.0  99.916077        0.0   -1.86768  50.746071  51.965328  173.797562   \n",
       "2   0.10178   1.59519  403.788544  985.373535  32.595268     100.0  2845.060059  36.828789      100.0  99.916077        0.0   -1.86768  50.662289  51.965328  174.560516   \n",
       "\n",
       "     P1_FT01Z      P1_FT02     P1_FT02Z     P1_FT03    P1_FT03Z  P1_LCV01D  P1_LCV01Z    P1_LIT01  P1_PCV01D  P1_PCV01Z  P1_PCV02D  P1_PCV02Z  P1_PIT01  P1_PIT02  P1_PP01AD  \\\n",
       "0  845.695496  1978.721558  2843.375488  243.388016  989.141174   10.89290    10.8429  402.709473  40.741249   41.32233       12.0   12.26196   1.34293   0.27557   540833.0   \n",
       "1  840.477051  1986.923218  2845.060059  243.006561  992.620178   10.80512    10.8429  402.811737  40.861240   41.32233       12.0   12.26196   1.34216   0.27710   540833.0   \n",
       "2  835.258423  1978.721558  2837.339111  242.815857  993.924683   10.80029    10.8429  402.760620  41.029060   41.32233       12.0   12.26196   1.34369   0.27710   540833.0   \n",
       "\n",
       "   P1_PP01AR  P1_PP01BD  P1_PP01BR  P1_PP02D  P1_PP02R  P1_STSP   P1_TIT01   P1_TIT02  P2_24Vdc  P2_ASD  P2_AutoGO  P2_CO_rpm  P2_Emerg   P2_HILout      P2_MSD  P2_ManualGO  \\\n",
       "0   540833.0        0.0        0.0       1.0       1.0      1.0  34.887699  35.147099  28.03162     0.0        1.0    54116.0       0.0  725.213623  763.193237          0.0   \n",
       "1   540833.0        0.0        0.0       1.0       1.0      1.0  34.887699  35.147099  28.02301     0.0        1.0    54114.0       0.0  721.740723  763.193237          0.0   \n",
       "2   540833.0        0.0        0.0       1.0       1.0      1.0  34.887699  35.147099  28.02993     0.0        1.0    54082.0       0.0  718.157959  763.193237          0.0   \n",
       "\n",
       "   P2_OnOff  P2_RTR  P2_SIT01    P2_SIT02  P2_TripEx   P2_VT01  P2_VTR01  P2_VTR02  P2_VTR03  P2_VTR04  P2_VXT02  P2_VXT03  P2_VYT02  P2_VYT03  P3_FIT01  P3_LCP01D  P3_LCV01D  \\\n",
       "0       1.0  2880.0     790.0  789.765076        1.0  11.91040      10.0      10.0      10.0      10.0   -2.8687   -1.0189    3.7751    5.6330     -25.0      688.0    15888.0   \n",
       "1       1.0  2880.0     789.0  789.131470        1.0  11.98856      10.0      10.0      10.0      10.0   -2.9842   -1.2637    3.1689    5.4158     -25.0      648.0    15952.0   \n",
       "2       1.0  2880.0     786.0  785.816528        1.0  11.97400      10.0      10.0      10.0      10.0   -3.4939   -1.5398    2.9615    5.5532     -25.0      616.0    16000.0   \n",
       "\n",
       "   P3_LH  P3_LIT01  P3_LL  P3_PIT01  P4_HT_FD   P4_HT_LD   P4_HT_PO  P4_HT_PS       P4_LD  P4_ST_FD  P4_ST_GOV    P4_ST_LD    P4_ST_PO  P4_ST_PS  P4_ST_PT01  P4_ST_TT01  \\\n",
       "0   70.0   18082.0   20.0     -23.0   0.00029  76.801208  73.585808       0.0  464.066101    0.0047    20469.0  386.266663  380.316833       0.0     10044.0     27567.0   \n",
       "1   70.0   18043.0   20.0     -23.0   0.00051  76.924187  73.893250       0.0  464.228882    0.0021    20489.0  386.302856  380.027466       0.0     10040.0     27564.0   \n",
       "2   70.0   18024.0   20.0     -23.0   0.00022  77.047150  74.200684       0.0  466.905334    0.0013    20604.0  389.738831  381.528503       0.0     10037.0     27565.0   \n",
       "\n",
       "                         file_source split                 timestamp observation_day  is_attack  label fault_type fault_sensor fault_start fault_end  fault_severity  \\\n",
       "0  hai_test1_run_20260221_135249_utc  test 2020-07-07 15:00:00+00:00      2020-07-07          0      0       None         None         NaT       NaT             NaN   \n",
       "1  hai_test1_run_20260221_135249_utc  test 2020-07-07 15:00:01+00:00      2020-07-07          0      0       None         None         NaT       NaT             NaN   \n",
       "2  hai_test1_run_20260221_135249_utc  test 2020-07-07 15:00:02+00:00      2020-07-07          0      0       None         None         NaT       NaT             NaN   \n",
       "\n",
       "   seconds_since_start  \n",
       "0                  0.0  \n",
       "1                  1.0  \n",
       "2                  2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Curation Function\n",
    "\n",
    "df_curated = curate_data(df_injected)\n",
    "\n",
    "print(f\"Input shape:  {df_injected.shape}\")\n",
    "print(f\"Output shape: {df_curated.shape}\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "new_cols = [c for c in df_curated.columns if c not in df_injected.columns]\n",
    "print(f\"  {new_cols}\")\n",
    "print(f\"\\nDropped columns:\")\n",
    "dropped_cols = [c for c in df_injected.columns if c not in df_curated.columns]\n",
    "print(f\"  {dropped_cols}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df_curated['label'].value_counts().sort_index().rename({0: 'normal', 1: 'attack', 2: 'fault'}))\n",
    "print(f\"\\nDtype check:\")\n",
    "print(df_curated[['label', 'seconds_since_start']].dtypes)\n",
    "print(f\"\\nSeconds since start range: {df_curated['seconds_since_start'].min():.0f}  {df_curated['seconds_since_start'].max():.0f}\")\n",
    "display(df_curated.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff741e1a-7219-4431-94ca-be567f4709fe",
   "metadata": {},
   "source": [
    "## 3.3 Choose Curated Columns and Write  \n",
    "Define the curated column list  \n",
    "Injection metadata columns are retained for validation and error analysis  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bad7128-e91c-4d02-9283-ecb1c7e7e343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated shape: (1323608, 91)\n",
      "Saved: work/hai_21_03/data/warehouse/hai_curated_20260221_135249_utc.parquet\n",
      "File size: 127.1 MB\n",
      "\n",
      "Column groups:\n",
      "  Time features:      3\n",
      "  Provenance:         2\n",
      "  Sensor readings:    79\n",
      "  Label columns:      2\n",
      "  Fault metadata:     5\n",
      "  Total:              91\n"
     ]
    }
   ],
   "source": [
    "CURATED_COLS = (\n",
    "    # Timestamp and time features\n",
    "    ['timestamp', 'observation_day', 'seconds_since_start']\n",
    "    # Provenance\n",
    "    + ['file_source', 'split']\n",
    "    # All 79 sensor readings\n",
    "    + SENSOR_COLS\n",
    "    # Unified label (0=normal, 1=attack, 2=fault)\n",
    "    + ['label', 'is_attack']\n",
    "    # Fault injection metadata  retained for validation, excluded from features later\n",
    "    + ['fault_type', 'fault_sensor', 'fault_start', 'fault_end', 'fault_severity']\n",
    ")\n",
    "\n",
    "# Verify all columns exist\n",
    "missing = [c for c in CURATED_COLS if c not in df_curated.columns]\n",
    "if missing:\n",
    "    raise PipelineError(f\"Curated column selection: missing columns {missing}\")\n",
    "\n",
    "df_final = df_curated[CURATED_COLS].copy()\n",
    "\n",
    "# Write to warehouse\n",
    "curated_path = WH_DIR / f\"hai_curated_{RUN_ID}.parquet\"\n",
    "df_final.to_parquet(curated_path, index=False)\n",
    "\n",
    "print(f\"Curated shape: {df_final.shape}\")\n",
    "print(f\"Saved: {curated_path}\")\n",
    "print(f\"File size: {curated_path.stat().st_size / 1e6:.1f} MB\")\n",
    "print(f\"\\nColumn groups:\")\n",
    "print(f\"  Time features:      3\")\n",
    "print(f\"  Provenance:         2\")\n",
    "print(f\"  Sensor readings:    {len(SENSOR_COLS)}\")\n",
    "print(f\"  Label columns:      2\")\n",
    "print(f\"  Fault metadata:     5\")\n",
    "print(f\"  Total:              {len(CURATED_COLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec8fd5-612b-43c1-8ec7-de5e5efe3bad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Validate: Contracts + Anomalies + Canaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae89945-4d5b-430e-a792-eb419c5efbbe",
   "metadata": {},
   "source": [
    "Define validation rules and check data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c7223-82c1-4640-b358-8a51d49b72bf",
   "metadata": {},
   "source": [
    "## 4.1 Define Contracts  \n",
    "Define validation contracts for the curated HAI dataset  \n",
    "* `required_cols`: must exist and have zero NaN values\n",
    "* `range_checks`: physical plausibility of sensor values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e3b2c72-31f4-411f-9ee0-bce1243f949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range checks computed for 79 sensors\n",
      "  (derived from 1,178,988 normal operation rows, 10% margin)\n",
      "\n",
      "Sample bounds:\n",
      "  P1_FT01             : [    -96.23,     873.55]\n",
      "  P1_LIT01            : [    302.08,     674.84]\n",
      "  P1_PIT01            : [     -0.13,       2.65]\n",
      "  P1_TIT01            : [     34.38,      37.18]\n",
      "  P2_CO_rpm           : [  51337.40,   55064.60]\n"
     ]
    }
   ],
   "source": [
    "REQUIRED_COLS = [\n",
    "    'timestamp', 'observation_day', 'seconds_since_start',\n",
    "    'file_source', 'split', 'label', 'is_attack'\n",
    "]\n",
    "SENSOR_REQUIRED_NONFAULT = SENSOR_COLS \n",
    "\n",
    "\n",
    "# Compute physical plausibility bounds from normal operation rows only.\n",
    "# Using observed min/max on label==0 rows with a 10% margin to allow for\n",
    "# legitimate excursions near the boundary of normal operation.\n",
    "# Injected faults and attacks may legitimately exceed these bounds.\n",
    "\n",
    "df_normal = df_final[df_final['label'] == 0]\n",
    "\n",
    "RANGE_CHECKS = {}\n",
    "for col in SENSOR_COLS:\n",
    "    col_min = float(df_normal[col].min())\n",
    "    col_max = float(df_normal[col].max())\n",
    "    margin = (col_max - col_min) * 0.10\n",
    "    RANGE_CHECKS[col] = (col_min - margin, col_max + margin)\n",
    "\n",
    "print(f\"Range checks computed for {len(RANGE_CHECKS)} sensors\")\n",
    "print(f\"  (derived from {len(df_normal):,} normal operation rows, 10% margin)\")\n",
    "print(f\"\\nSample bounds:\")\n",
    "for col in ['P1_FT01', 'P1_LIT01', 'P1_PIT01', 'P1_TIT01', 'P2_CO_rpm']:\n",
    "    lo, hi = RANGE_CHECKS[col]\n",
    "    print(f\"  {col:20s}: [{lo:10.2f}, {hi:10.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc66f1e-b2ba-475d-bcd8-8995ac06aa32",
   "metadata": {},
   "source": [
    "## 4.2 Implement Validation Checks  \n",
    "Run contract checks on the curated DataFrame  \n",
    "\n",
    "Checks:  \n",
    "1. Required columns exist\n",
    "2. Required columns have no NaN values\n",
    "3. Range violations < 5% per sensor (warning) or > 20% (failure)\n",
    "4. Label column contains only valid values {0, 1, 2}\n",
    "5. No rows carry both attack and fault labels simultaneously\n",
    "6. Timestamps are monotonically non-decreasing within each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce463f8e-bb8e-46c2-940f-bab05e8d3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data function ready\n"
     ]
    }
   ],
   "source": [
    "failures = []\n",
    "warnings = []\n",
    "\n",
    "def validate_data(df: pd.DataFrame, required_cols: list, range_checks: dict) -> dict:\n",
    "\n",
    "    # 1. Required columns exist\n",
    "    missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "    if missing_cols:\n",
    "        failures.append(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # 2. Required columns have no NaNs, except for sensors in fault rows. Dropout faults cause NaN.\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        n_null = int(df[col].isna().sum())\n",
    "        if n_null > 0:\n",
    "            failures.append(f\"Required column '{col}' has {n_null:,} NaN values\")\n",
    "\n",
    "    non_fault_mask = df['label'] != 2\n",
    "    for col in SENSOR_COLS:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        n_null = int(df.loc[non_fault_mask, col].isna().sum())\n",
    "        if n_null > 0:\n",
    "            failures.append(\n",
    "                f\"Sensor '{col}' has {n_null:,} NaN values in non-fault rows \"\n",
    "                f\"(label != 2)  unexpected data quality issue\"\n",
    "            )\n",
    "    \n",
    "    # 3. Range violations\n",
    "    for col, (lo, hi) in range_checks.items():\n",
    "        if col not in df.columns:\n",
    "            warnings.append(f\"Range check skipped  '{col}' not in DataFrame\")\n",
    "            continue\n",
    "        n_total = len(df)\n",
    "        n_violation = int(((df[col] < lo) | (df[col] > hi)).sum())\n",
    "        pct = n_violation / n_total * 100\n",
    "        if pct > 20.0:\n",
    "            failures.append(\n",
    "                f\"Range violation FAILURE '{col}': {n_violation:,} rows ({pct:.1f}%) \"\n",
    "                f\"outside [{lo:.3f}, {hi:.3f}]\"\n",
    "            )\n",
    "        elif pct > 5.0:\n",
    "            warnings.append(\n",
    "                f\"Range violation WARNING '{col}': {n_violation:,} rows ({pct:.1f}%) \"\n",
    "                f\"outside [{lo:.3f}, {hi:.3f}]\"\n",
    "            )\n",
    "    \n",
    "    # 4. Label values are valid\n",
    "    valid_labels = {0, 1, 2}\n",
    "    actual_labels = set(df['label'].unique())\n",
    "    invalid_labels = actual_labels - valid_labels\n",
    "    if invalid_labels:\n",
    "        failures.append(f\"Label column contains invalid values: {invalid_labels}\")\n",
    "    \n",
    "    # 5. No row carries bot attack and fault labels\n",
    "    overlap = int(((df['label'] == 1) & (df['is_attack'] == 0)).sum())\n",
    "    if overlap > 0:\n",
    "        failures.append(\n",
    "            f\"Label/is_attack mismatch: {overlap:,} rows have label=1 but is_attack=0\"\n",
    "        )\n",
    "    attack_fault_overlap = int(((df['is_attack'] == 1) & (df['label'] == 2)).sum())\n",
    "    if attack_fault_overlap > 0:\n",
    "        failures.append(\n",
    "            f\"Attack/fault overlap: {attack_fault_overlap:,} rows carry both \"\n",
    "            f\"is_attack=1 and label=2\"\n",
    "        )\n",
    "    \n",
    "    # 6. Time stamps non-decreasing within each split\n",
    "    for split_name, split_df in df.groupby('split'):\n",
    "        n_decreasing = int((split_df['timestamp'].diff().dt.total_seconds() < 0).sum())\n",
    "        if n_decreasing > 0:\n",
    "            failures.append(\n",
    "                f\"Timestamp ordering violation in split='{split_name}': \"\n",
    "                f\"{n_decreasing:,} out-of-order rows\"\n",
    "            )\n",
    "    \n",
    "    passed = len(failures) == 0\n",
    "    return {'passed': passed, 'failures': failures, 'warnings': warnings}\n",
    "\n",
    "print(\"validation data function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604da8a-7c1d-4f13-958a-9ce13935362d",
   "metadata": {},
   "source": [
    "## 4.3 Run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5459b19a-10cb-47c5-a7ab-eda55324d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts in df_injected vs df_final:\n",
      "\n",
      "Column                 injected      final      split      label\n",
      "--------------------------------------------------------------\n",
      "P1_B2016                     16         16 {'train': 16}    {2: 16}\n",
      "P1_B4002                     48         48 {'train': 48}    {2: 48}\n",
      "P1_B4005                     71         71 {'train': 71}    {2: 71}\n",
      "P1_FCV01D                    90         90 {'train': 90}    {2: 90}\n",
      "P1_FCV01Z                    70         70 {'train': 70}    {2: 70}\n",
      "P1_FT01Z                     60         60 {'train': 60}    {2: 60}\n",
      "P1_FT02Z                     84         84 {'train': 84}    {2: 84}\n",
      "P1_LCV01D                   144        144 {'test': 72, 'train': 72}   {2: 144}\n",
      "P1_LCV01Z                    10         10 {'train': 10}    {2: 10}\n",
      "P1_PIT01                     25         25 {'train': 25}    {2: 25}\n",
      "P1_TIT02                     45         45 {'train': 45}    {2: 45}\n",
      "P2_VXT03                    200        200 {'train': 200}   {2: 200}\n",
      "P2_VYT02                     72         72 {'train': 72}    {2: 72}\n",
      "P2_VYT03                    104        104 {'train': 104}   {2: 104}\n",
      "P3_LIT01                     32         32 {'train': 32}    {2: 32}\n",
      "P3_PIT01                     56         56 {'train': 56}    {2: 56}\n"
     ]
    }
   ],
   "source": [
    "# Investigate: are these NaNs present in the original injected data\n",
    "# or were they introduced during curation?\n",
    "print(\"NaN counts in df_injected vs df_final:\\n\")\n",
    "nan_cols = [\n",
    "    'P1_B2016', 'P1_B4002', 'P1_B4005', 'P1_FCV01D', 'P1_FCV01Z',\n",
    "    'P1_FT01Z', 'P1_FT02Z', 'P1_LCV01D', 'P1_LCV01Z', 'P1_PIT01',\n",
    "    'P1_TIT02', 'P2_VXT03', 'P2_VYT02', 'P2_VYT03', 'P3_LIT01', 'P3_PIT01'\n",
    "]\n",
    "\n",
    "print(f\"{'Column':<20} {'injected':>10} {'final':>10} {'split':>10} {'label':>10}\")\n",
    "print(\"-\" * 62)\n",
    "for col in nan_cols:\n",
    "    n_injected = int(df_injected[col].isna().sum())\n",
    "    n_final    = int(df_final[col].isna().sum())\n",
    "    # Which split do the NaNs fall in?\n",
    "    null_mask = df_final[col].isna()\n",
    "    split_counts = df_final.loc[null_mask, 'split'].value_counts().to_dict()\n",
    "    label_counts = df_final.loc[null_mask, 'label'].value_counts().to_dict()\n",
    "    print(f\"{col:<20} {n_injected:>10} {n_final:>10} {str(split_counts):>10} {str(label_counts):>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ca6d976-7dc7-4187-aaeb-6e33871b26a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation status: PASSED\n",
      "  Failures: 0\n",
      "  Warnings: 0\n",
      "\n",
      "All contracts passed with no warnings.\n"
     ]
    }
   ],
   "source": [
    "# Run validation against the curated dataset\n",
    "validation_report = validate_data(df_final, REQUIRED_COLS, RANGE_CHECKS)\n",
    "\n",
    "# Print results\n",
    "status = \"PASSED\" if validation_report['passed'] else \"FAILED\"\n",
    "print(f\"Validation status: {status}\")\n",
    "print(f\"  Failures: {len(validation_report['failures'])}\")\n",
    "print(f\"  Warnings: {len(validation_report['warnings'])}\")\n",
    "\n",
    "if validation_report['failures']:\n",
    "    print(f\"\\nFailures:\")\n",
    "    for f in validation_report['failures']:\n",
    "        print(f\"  {f}\")\n",
    "\n",
    "if validation_report['warnings']:\n",
    "    print(f\"\\nWarnings:\")\n",
    "    for w in validation_report['warnings']:\n",
    "        print(f\"  {w}\")\n",
    "\n",
    "if validation_report['passed'] and not validation_report['warnings']:\n",
    "    print(\"\\nAll contracts passed with no warnings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24907376-d655-4c9a-941c-bca8c6608122",
   "metadata": {},
   "source": [
    "## 4.4 Anomaly Flags and Investigation  \n",
    "Computes anomaly statistics across the curated dataset.  \n",
    "\n",
    "Calculates, but does not modify the DataFrame.  \n",
    "Results are used in final validation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0035c41f-f038-4455-a924-9caf3b3bcb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly summary function ready\n"
     ]
    }
   ],
   "source": [
    "def create_anomaly_summary(df: pd.DataFrame, range_checks: dict) -> dict:\n",
    "    n_total = len(df)\n",
    "\n",
    "    # 1. Missingness by column \n",
    "    missingness = {}\n",
    "    for col in SENSOR_COLS:\n",
    "        n_null = int(df[col].isna().sum())\n",
    "        if n_null > 0:\n",
    "            missingness[col] = {\n",
    "                'n_null': n_null,\n",
    "                'pct_null': round(n_null / n_total * 100, 4),\n",
    "                'in_fault_rows': int(df.loc[df['label'] == 2, col].isna().sum()),\n",
    "                'in_normal_rows': int(df.loc[df['label'] == 0, col].isna().sum()),\n",
    "                'in_attack_rows': int(df.loc[df['label'] == 1, col].isna().sum()),\n",
    "            }\n",
    "\n",
    "    # 2. Out-of-range counts by column\n",
    "    range_violations = {}\n",
    "    for col, (lo, hi) in range_checks.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        mask = (df[col] < lo) | (df[col] > hi)\n",
    "        n_violation = int(mask.sum())\n",
    "        if n_violation > 0:\n",
    "            label_breakdown = df.loc[mask, 'label'].value_counts().to_dict()\n",
    "            range_violations[col] = {\n",
    "                'n_violations': n_violation,\n",
    "                'pct_violations': round(n_violation / n_total * 100, 4),\n",
    "                'by_label': label_breakdown\n",
    "            }\n",
    "\n",
    "    # 3. Suspicious rows: NaN in non-fault sensor rows \n",
    "    non_fault_nan_rows = int(\n",
    "        df.loc[df['label'] != 2, SENSOR_COLS].isna().any(axis=1).sum()\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'total_rows': n_total,\n",
    "        'missing_sensor_cols': len(missingness),\n",
    "        'missingness': missingness,\n",
    "        'range_violation_cols': len(range_violations),\n",
    "        'range_violations': range_violations,\n",
    "        'non_fault_nan_rows': non_fault_nan_rows,\n",
    "    }\n",
    "\n",
    "print(\"anomaly summary function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd7f36-a384-4270-90dc-9667dc7d3562",
   "metadata": {},
   "source": [
    "## 4.5 Run Anomaly Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42218d63-1004-475e-a7a6-68b6c8ab058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:                    1,323,608\n",
      "Sensors with missingness:      16\n",
      "Sensors with range violations: 29\n",
      "Non-fault rows with NaN:       0\n",
      "\n",
      "Missingness detail (fault rows only, as expected):\n",
      "  P1_B2016               16 NaN  (normal=0, attack=0, fault=16)\n",
      "  P1_B4002               48 NaN  (normal=0, attack=0, fault=48)\n",
      "  P1_B4005               71 NaN  (normal=0, attack=0, fault=71)\n",
      "  P1_FCV01D              90 NaN  (normal=0, attack=0, fault=90)\n",
      "  P1_FCV01Z              70 NaN  (normal=0, attack=0, fault=70)\n",
      "  P1_FT01Z               60 NaN  (normal=0, attack=0, fault=60)\n",
      "  P1_FT02Z               84 NaN  (normal=0, attack=0, fault=84)\n",
      "  P1_LCV01D             144 NaN  (normal=0, attack=0, fault=144)\n",
      "  P1_LCV01Z              10 NaN  (normal=0, attack=0, fault=10)\n",
      "  P1_PIT01               25 NaN  (normal=0, attack=0, fault=25)\n",
      "  P1_TIT02               45 NaN  (normal=0, attack=0, fault=45)\n",
      "  P2_VXT03              200 NaN  (normal=0, attack=0, fault=200)\n",
      "  P2_VYT02               72 NaN  (normal=0, attack=0, fault=72)\n",
      "  P2_VYT03              104 NaN  (normal=0, attack=0, fault=104)\n",
      "  P3_LIT01               32 NaN  (normal=0, attack=0, fault=32)\n",
      "  P3_PIT01               56 NaN  (normal=0, attack=0, fault=56)\n",
      "\n",
      "Range violations:\n",
      "  P1_B4002               1811 rows (0.137%)  by label: {2: 1811}\n",
      "  P1_B4022                584 rows (0.044%)  by label: {2: 584}\n",
      "  P1_FCV01Z              1090 rows (0.082%)  by label: {2: 1090}\n",
      "  P1_FCV02D              1328 rows (0.100%)  by label: {2: 1328}\n",
      "  P1_FCV03D               320 rows (0.024%)  by label: {1: 320}\n",
      "  P1_FCV03Z               103 rows (0.008%)  by label: {1: 103}\n",
      "  P1_FT01                 109 rows (0.008%)  by label: {1: 74, 2: 35}\n",
      "  P1_FT01Z                101 rows (0.008%)  by label: {2: 71, 1: 30}\n",
      "  P1_FT02                 868 rows (0.066%)  by label: {2: 868}\n",
      "  P1_FT03Z                446 rows (0.034%)  by label: {2: 446}\n",
      "  P1_LCV01D               545 rows (0.041%)  by label: {1: 545}\n",
      "  P1_LCV01Z               527 rows (0.040%)  by label: {1: 527}\n",
      "  P1_LIT01                670 rows (0.051%)  by label: {2: 345, 1: 325}\n",
      "  P1_PCV01D               109 rows (0.008%)  by label: {1: 109}\n",
      "  P1_PCV01Z               672 rows (0.051%)  by label: {2: 542, 1: 130}\n",
      "  P1_PCV02D               216 rows (0.016%)  by label: {1: 216}\n",
      "  P1_PCV02Z               240 rows (0.018%)  by label: {1: 240}\n",
      "  P1_PIT01                  1 rows (0.000%)  by label: {2: 1}\n",
      "  P1_PIT02                805 rows (0.061%)  by label: {2: 805}\n",
      "  P2_CO_rpm               420 rows (0.032%)  by label: {2: 420}\n",
      "  P2_VXT02                  3 rows (0.000%)  by label: {1: 3}\n",
      "  P2_VXT03                547 rows (0.041%)  by label: {2: 542, 1: 5}\n",
      "  P2_VYT02                  3 rows (0.000%)  by label: {1: 3}\n",
      "  P2_VYT03                156 rows (0.012%)  by label: {2: 153, 1: 3}\n",
      "  P3_FIT01                 23 rows (0.002%)  by label: {1: 23}\n",
      "  P3_LCP01D              2390 rows (0.181%)  by label: {2: 2350, 1: 40}\n",
      "  P3_LCV01D               301 rows (0.023%)  by label: {2: 301}\n",
      "  P3_PIT01                 32 rows (0.002%)  by label: {1: 32}\n",
      "  P4_ST_TT01                2 rows (0.000%)  by label: {1: 2}\n"
     ]
    }
   ],
   "source": [
    "anomaly_summary = create_anomaly_summary(df_final, RANGE_CHECKS)\n",
    "\n",
    "print(f\"Total rows:                    {anomaly_summary['total_rows']:,}\")\n",
    "print(f\"Sensors with missingness:      {anomaly_summary['missing_sensor_cols']}\")\n",
    "print(f\"Sensors with range violations: {anomaly_summary['range_violation_cols']}\")\n",
    "print(f\"Non-fault rows with NaN:       {anomaly_summary['non_fault_nan_rows']}\")\n",
    "\n",
    "if anomaly_summary['missingness']:\n",
    "    print(f\"\\nMissingness detail (fault rows only, as expected):\")\n",
    "    for col, stats in anomaly_summary['missingness'].items():\n",
    "        print(f\"  {col:<20s} {stats['n_null']:>4} NaN  \"\n",
    "              f\"(normal={stats['in_normal_rows']}, \"\n",
    "              f\"attack={stats['in_attack_rows']}, \"\n",
    "              f\"fault={stats['in_fault_rows']})\")\n",
    "\n",
    "if anomaly_summary['range_violations']:\n",
    "    print(f\"\\nRange violations:\")\n",
    "    for col, stats in anomaly_summary['range_violations'].items():\n",
    "        print(f\"  {col:<20s} {stats['n_violations']:>6} rows \"\n",
    "              f\"({stats['pct_violations']:.3f}%)  by label: {stats['by_label']}\")\n",
    "else:\n",
    "    print(f\"\\nNo range violations detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4cf66-701a-4181-b0c0-2d91c237c6b8",
   "metadata": {},
   "source": [
    "**Interpretation:**  \n",
    "* **All missingness is fault only** - dropout faults working correctly\n",
    "* **Non-fault NaN rows = 0** - no data quality issues in normal or attack rows\n",
    "* **Range violations are small** - worst case is `P3_LCP01D` at 0.181%\n",
    "* **Range violations split cleanly by label** - mostly label 2, faults pushing sensors out of bounds or label 1, attacks manipulating process values. Zero label 0 violations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fcfd5-1f1f-4395-90bd-b8359bba4eed",
   "metadata": {},
   "source": [
    "## 4.6 Canary Checks  \n",
    "\n",
    "Monitor dataset-level health metrics that should remain stable.  \n",
    "Flag days with anomalous row counts or high sensor missingness that could silently degrade model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26e5343b-2284-4825-ae99-7cce179f7cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canary summary function ready\n"
     ]
    }
   ],
   "source": [
    "def create_canary_summary(df: pd.DataFrame) -> dict:\n",
    "    \n",
    "    # 1. Observations per day \n",
    "    daily_counts = df.groupby('observation_day').size()\n",
    "    median_daily = float(daily_counts.median())\n",
    "    low_threshold = median_daily * 0.50  # days with < 50% of median\n",
    "\n",
    "    low_days = daily_counts[daily_counts < low_threshold]\n",
    "\n",
    "    # 2. Class distribution \n",
    "    label_counts = df['label'].value_counts().sort_index().to_dict()\n",
    "    total = len(df)\n",
    "    label_pcts = {int(k): round(v / total * 100, 3) for k, v in label_counts.items()}\n",
    "\n",
    "    # 3. Per-column missingness across all rows\n",
    "    overall_missingness = {}\n",
    "    for col in SENSOR_COLS:\n",
    "        n_null = int(df[col].isna().sum())\n",
    "        if n_null > 0:\n",
    "            overall_missingness[col] = round(n_null / total * 100, 4)\n",
    "\n",
    "    # 4. Days with high missingness (any sensor > 30% null that day) \n",
    "    high_miss_days = []\n",
    "    for day, day_df in df.groupby('observation_day'):\n",
    "        for col in SENSOR_COLS:\n",
    "            pct = day_df[col].isna().mean() * 100\n",
    "            if pct > 30.0:\n",
    "                high_miss_days.append({\n",
    "                    'day': str(day),\n",
    "                    'col': col,\n",
    "                    'pct_null': round(pct, 2)\n",
    "                })\n",
    "\n",
    "    return {\n",
    "        'daily_counts': {\n",
    "            'min':    int(daily_counts.min()),\n",
    "            'median': int(median_daily),\n",
    "            'max':    int(daily_counts.max()),\n",
    "            'n_days': int(len(daily_counts)),\n",
    "            'low_count_days': {str(k): int(v) for k, v in low_days.items()},\n",
    "        },\n",
    "        'class_distribution': {\n",
    "            'counts': {int(k): int(v) for k, v in label_counts.items()},\n",
    "            'pct':    label_pcts,\n",
    "        },\n",
    "        'overall_missingness_pct': overall_missingness,\n",
    "        'high_missingness_days':   high_miss_days,\n",
    "    }\n",
    "\n",
    "print(\"canary summary function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da339f4c-593b-4f89-b547-c0842a84f080",
   "metadata": {},
   "source": [
    "## 4.7 Run Canary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00a983fc-0cf3-442a-8810-1f6a4ba256f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily row counts:\n",
      "  Days in dataset:  21\n",
      "  Min rows/day:     7,200\n",
      "  Median rows/day:  86,400\n",
      "  Max rows/day:     129,601\n",
      "  Low count days (< 50% of median):\n",
      "    2020-07-07: 32,400 rows\n",
      "    2020-07-08: 10,801 rows\n",
      "    2020-07-09: 32,400 rows\n",
      "    2020-07-14: 21,601 rows\n",
      "    2020-07-28: 39,601 rows\n",
      "    2020-08-04: 7,200 rows\n",
      "    2020-08-10: 39,601 rows\n",
      "\n",
      "Class distribution:\n",
      "  Normal  (0): 1,178,988  (89.07%)\n",
      "  Attack  (1):     8,947  (0.68%)\n",
      "  Fault   (2):   135,673  (10.25%)\n",
      "\n",
      "Sensors with any missingness: 16\n",
      "No high missingness days detected.\n"
     ]
    }
   ],
   "source": [
    "# Run canary analysis\n",
    "canary_summary = create_canary_summary(df_final)\n",
    "\n",
    "dc = canary_summary['daily_counts']\n",
    "print(f\"Daily row counts:\")\n",
    "print(f\"  Days in dataset:  {dc['n_days']}\")\n",
    "print(f\"  Min rows/day:     {dc['min']:,}\")\n",
    "print(f\"  Median rows/day:  {dc['median']:,}\")\n",
    "print(f\"  Max rows/day:     {dc['max']:,}\")\n",
    "\n",
    "if dc['low_count_days']:\n",
    "    print(f\"  Low count days (< 50% of median):\")\n",
    "    for day, count in dc['low_count_days'].items():\n",
    "        print(f\"    {day}: {count:,} rows\")\n",
    "else:\n",
    "    print(f\"  No low count days detected.\")\n",
    "\n",
    "cd = canary_summary['class_distribution']\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Normal  (0): {cd['counts'][0]:>9,}  ({cd['pct'][0]:.2f}%)\")\n",
    "print(f\"  Attack  (1): {cd['counts'][1]:>9,}  ({cd['pct'][1]:.2f}%)\")\n",
    "print(f\"  Fault   (2): {cd['counts'][2]:>9,}  ({cd['pct'][2]:.2f}%)\")\n",
    "\n",
    "print(f\"\\nSensors with any missingness: {len(canary_summary['overall_missingness_pct'])}\")\n",
    "\n",
    "if canary_summary['high_missingness_days']:\n",
    "    print(f\"\\nHigh missingness days (>30% null):\")\n",
    "    for entry in canary_summary['high_missingness_days']:\n",
    "        print(f\"  {entry['day']}  {entry['col']:<20s}  {entry['pct_null']:.1f}%\")\n",
    "else:\n",
    "    print(f\"No high missingness days detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf79db5-c221-4227-9fa9-0e8c2e7ccc75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Leakage Audit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddbc382-275c-4873-a7a3-fa05cbc137f9",
   "metadata": {},
   "source": [
    "Document potential temporal leakage risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7df30-8575-42d6-a6e0-86bfdb107267",
   "metadata": {},
   "source": [
    "## 5.1 Write Leakage Checklist  \n",
    "Confirms that no future-looking information contaminates the feature matrix.\n",
    "Evaluated at curation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17f37019-5f01-4f89-9f26-e4a208d82f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leakage Audit Checklist\n",
      "============================================================\n",
      "\n",
      "[DEFERRED] Normalization stats computed on train only\n",
      "  StandardScaler will be fit on train split only and applied to val/test in the Feature Engineering notebook.\n",
      "\n",
      "[DEFERRED] Rolling window features use only past observations\n",
      "  Rolling windows will use closed='left' or shift(1) to ensure each row sees only its own history. Computed in Feature Engineering.\n",
      "\n",
      "[CONFIRMED] fault_start and fault_end excluded from feature matrix\n",
      "  These columns are injection metadata. They are retained in the warehouse for validation and error analysis but will be explicitly excluded from the feature matrix during Feature Engineering.\n",
      "\n",
      "[CONFIRMED] fault_type and fault_sensor excluded from feature matrix\n",
      "  These are injection metadata columns, not observable signals. A real ICS deployment would not have access to these values.\n",
      "\n",
      "[CONFIRMED] is_attack excluded from feature matrix\n",
      "  is_attack is derived from the HAI attack flags and is part of the label definition. Including it as a feature would be direct label leakage.\n",
      "\n",
      "[CONFIRMED] Train/val/test split is time-based, not random\n",
      "  Split will be performed by timestamp order (60/20/20) in the Feature Engineering notebook. Random splitting is not used  it would allow future readings to inform past predictions.\n",
      "\n",
      "[ACCEPTABLE] seconds_since_start computed on full dataset before split\n",
      "  seconds_since_start uses the global dataset minimum timestamp as its reference point. This is a fixed constant (dataset start), not a statistic derived from labels or future rows. No leakage risk.\n"
     ]
    }
   ],
   "source": [
    "leakage_checklist = [\n",
    "    {\n",
    "        \"check\": \"Normalization stats computed on train only\",\n",
    "        \"status\": \"DEFERRED\",\n",
    "        \"notes\": \"StandardScaler will be fit on train split only and applied to \"\n",
    "                 \"val/test in the Feature Engineering notebook.\"\n",
    "    },\n",
    "    {\n",
    "        \"check\": \"Rolling window features use only past observations\",\n",
    "        \"status\": \"DEFERRED\",\n",
    "        \"notes\": \"Rolling windows will use closed='left' or shift(1) to ensure \"\n",
    "                 \"each row sees only its own history. Computed in Feature Engineering.\"\n",
    "    },\n",
    "    {\n",
    "        \"check\": \"fault_start and fault_end excluded from feature matrix\",\n",
    "        \"status\": \"CONFIRMED\",\n",
    "        \"notes\": \"These columns are injection metadata. They are retained in the \"\n",
    "                 \"warehouse for validation and error analysis but will be explicitly \"\n",
    "                 \"excluded from the feature matrix during Feature Engineering.\"\n",
    "    },\n",
    "    {\n",
    "        \"check\": \"fault_type and fault_sensor excluded from feature matrix\",\n",
    "        \"status\": \"CONFIRMED\",\n",
    "        \"notes\": \"These are injection metadata columns, not observable signals. \"\n",
    "                 \"A real ICS deployment would not have access to these values.\"\n",
    "    },\n",
    "    {\n",
    "        \"check\": \"is_attack excluded from feature matrix\",\n",
    "        \"status\": \"CONFIRMED\",\n",
    "        \"notes\": \"is_attack is derived from the HAI attack flags and is part of \"\n",
    "                 \"the label definition. Including it as a feature would be direct \"\n",
    "                 \"label leakage.\"\n",
    "    },\n",
    "    {\n",
    "        \"check\": \"Train/val/test split is time-based, not random\",\n",
    "        \"status\": \"CONFIRMED\",\n",
    "        \"notes\": \"Split will be performed by timestamp order (60/20/20) in the \"\n",
    "                 \"Feature Engineering notebook. Random splitting is not used  it \"\n",
    "                 \"would allow future readings to inform past predictions.\"\n",
    "    },\n",
    "    {\n",
    "        \"check\": \"seconds_since_start computed on full dataset before split\",\n",
    "        \"status\": \"ACCEPTABLE\",\n",
    "        \"notes\": \"seconds_since_start uses the global dataset minimum timestamp \"\n",
    "                 \"as its reference point. This is a fixed constant (dataset start), \"\n",
    "                 \"not a statistic derived from labels or future rows. No leakage risk.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Leakage Audit Checklist\")\n",
    "print(\"=\" * 60)\n",
    "for item in leakage_checklist:\n",
    "    print(f\"\\n[{item['status']}] {item['check']}\")\n",
    "    print(f\"  {item['notes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc781862-0716-46c4-b27c-de7136e7716e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Write Final Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317aaf9-377a-430e-9937-7b6b39c65431",
   "metadata": {},
   "source": [
    "Consolidate validation results and create run log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ae05e-aa2c-4984-aeb5-7d88220ffdad",
   "metadata": {},
   "source": [
    "## 6.1 Write Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccd8fa57-d889-44d4-b64d-ab1d5f6e8027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: work/hai_21_03/data/reference/validation_report_20260221_135249_utc.json\n",
      "Size:  10.5 KB\n"
     ]
    }
   ],
   "source": [
    "def write_validation_report(\n",
    "    validation_report: dict,\n",
    "    anomaly_summary: dict,\n",
    "    canary_summary: dict,\n",
    "    leakage_checklist: list,\n",
    "    path: Path\n",
    ") -> None:\n",
    "\n",
    "    report = {\n",
    "        \"run_id\":            RUN_ID,\n",
    "        \"generated_at_utc\":  datetime.now(timezone.utc).isoformat(),\n",
    "        \"dataset\":           DATASET_NAME,\n",
    "        \"curated_path\":      str(WH_DIR / f\"hai_curated_{RUN_ID}.parquet\"),\n",
    "        \"contracts\":         validation_report,\n",
    "        \"anomalies\":         anomaly_summary,\n",
    "        \"canaries\":          canary_summary,\n",
    "        \"leakage_checklist\": leakage_checklist,\n",
    "    }\n",
    "    write_json(path, report)\n",
    "    print(f\"Saved: {path}\")\n",
    "    print(f\"Size:  {path.stat().st_size / 1e3:.1f} KB\")\n",
    "\n",
    "report_path = REF_DIR / f\"validation_report_{RUN_ID}.json\"\n",
    "write_validation_report(\n",
    "    validation_report,\n",
    "    anomaly_summary,\n",
    "    canary_summary,\n",
    "    leakage_checklist,\n",
    "    report_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e038a-7f13-441e-8ff1-2e68286c7a07",
   "metadata": {},
   "source": [
    "## 6.2 Create Run Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc411cf2-2cbe-4896-8ada-368a653c3f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: work/hai_21_03/data/reference/pipeline_runs/run_20260221_135249_utc.json\n",
      "Size:  1.2 KB\n",
      "\n",
      "Validation passed: True\n"
     ]
    }
   ],
   "source": [
    "def create_run_log(injected_path: Path, curated_path: Path, report_path: Path) -> dict:\n",
    "    return {\n",
    "        \"run_id\":           RUN_ID,\n",
    "        \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"stage\":            \"Stage 3-6: Curation and Validation\",\n",
    "        \"row_definition\":   \"One row = one second of ICS sensor readings. \"\n",
    "                            \"Label 0=normal, 1=cyber attack, 2=injected sensor fault.\",\n",
    "        \"inputs\": {\n",
    "            \"injected_parquet\": {\n",
    "                \"path\":       str(injected_path),\n",
    "                \"size_bytes\": injected_path.stat().st_size,\n",
    "            }\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"curated_parquet\": {\n",
    "                \"path\":       str(curated_path),\n",
    "                \"size_bytes\": curated_path.stat().st_size,\n",
    "                \"rows\":       len(df_final),\n",
    "                \"cols\":       len(df_final.columns),\n",
    "            },\n",
    "            \"validation_report\": {\n",
    "                \"path\": str(report_path),\n",
    "            }\n",
    "        },\n",
    "        \"validation_passed\": validation_report['passed'],\n",
    "        \"notes\": [\n",
    "            \"Sensor NaNs in fault rows (label=2) are expected  intermittent dropout faults.\",\n",
    "            \"Range violations are confined to label=1 and label=2 rows  correct behavior.\",\n",
    "            \"Low row-count days are dataset boundary artifacts, not data quality issues.\",\n",
    "            \"Leakage audit has two DEFERRED items to be confirmed in Feature Engineering.\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "run_log = create_run_log(\n",
    "    injected_path  = INJECTED_DIR / sorted(INJECTED_DIR.glob(\"hai_21_03_*.parquet\"))[-1].name,\n",
    "    curated_path   = WH_DIR / f\"hai_curated_{RUN_ID}.parquet\",\n",
    "    report_path    = report_path\n",
    ")\n",
    "\n",
    "run_log_path = RUN_DIR / f\"run_{RUN_ID}.json\"\n",
    "write_json(run_log_path, run_log)\n",
    "print(f\"Saved: {run_log_path}\")\n",
    "print(f\"Size:  {run_log_path.stat().st_size / 1e3:.1f} KB\")\n",
    "print(f\"\\nValidation passed: {run_log['validation_passed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef52461-ed33-4f8e-aacb-29709fa33515",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Self-Check and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75a1ad-87b7-4b4b-a2c6-d862d4a5f30e",
   "metadata": {},
   "source": [
    "Document insights and potential issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe15bf-5d9d-40b5-88af-ff58ee1626b8",
   "metadata": {},
   "source": [
    "## 7.1 Pipeline Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79e23e42-6972-4488-9288-c874dc63fb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Reflection\n",
      "============================================================\n",
      "\n",
      "1. Row definition: One row = one second of ICS sensor readings from the HAI-21.03 dataset. Label 0=normal operation, 1=cyber attack (original HAI labels), 2=injected sensor fault (synthetic).\n",
      "\n",
      "2. Required columns: timestamp, label, is_attack, and all 79 sensor columns are required. Fault metadata columns (fault_type, fault_sensor, fault_start, fault_end, fault_severity) are retained for validation but excluded from the feature matrix.\n",
      "\n",
      "3. NaN policy: Sensor NaNs are only permitted in fault rows (label=2) where they represent intermittent dropout fault signatures. 16 sensors carry dropout NaNs, all confined to fault rows. Zero NaNs in normal or attack rows.\n",
      "\n",
      "4. Range checks: Physical plausibility bounds derived empirically from 1,178,988 normal operation rows with a 10% margin. 29 sensors show out-of-range violations, all confined to label=1 or label=2 rows  confirming bounds correctly characterize normal operation and that faults/attacks produce distinguishable sensor excursions.\n",
      "\n",
      "5. Biggest anomaly found: P3_LCP01D has the highest range violation count at 2,390 rows (0.181%), split across label=2 (2,350) and label=1 (40). This sensor appears sensitive to both fault injection and cyber attack manipulation.\n",
      "\n",
      "6. Likely breakage scenario: A future pipeline run with a different injection seed could produce fault rows in the test split, violating the train-only injection contract. The injected parquet path is pinned to a specific RUN_ID to prevent this.\n",
      "\n",
      "7. Temporal leakage risk: seconds_since_start is computed from the global dataset minimum before splitting  acceptable since it references a fixed constant, not a label-derived statistic. All other leakage risks are deferred to and documented in the Feature Engineering notebook.\n",
      "\n",
      "8. Data quality insight: Low row-count days (7 days below 50% of median) are dataset boundary artifacts where HAI source files start or end mid-day. 2020-08-04 is the thinnest at 7,200 rows (2 hours). Split boundaries in Feature Engineering should avoid these partial days.\n",
      "\n",
      "9. Pipeline reproducibility: All artifacts are timestamped with RUN_ID (20260221_135249_utc). The run log links the injected parquet input to the curated parquet and validation report outputs. Re-running this notebook with the same injected parquet will produce identical outputs.\n"
     ]
    }
   ],
   "source": [
    "reflection = [\n",
    "    \"Row definition: One row = one second of ICS sensor readings from the HAI-21.03 \"\n",
    "    \"dataset. Label 0=normal operation, 1=cyber attack (original HAI labels), \"\n",
    "    \"2=injected sensor fault (synthetic).\",\n",
    "\n",
    "    \"Required columns: timestamp, label, is_attack, and all 79 sensor columns are \"\n",
    "    \"required. Fault metadata columns (fault_type, fault_sensor, fault_start, \"\n",
    "    \"fault_end, fault_severity) are retained for validation but excluded from the \"\n",
    "    \"feature matrix.\",\n",
    "\n",
    "    \"NaN policy: Sensor NaNs are only permitted in fault rows (label=2) where they \"\n",
    "    \"represent intermittent dropout fault signatures. 16 sensors carry dropout NaNs, \"\n",
    "    \"all confined to fault rows. Zero NaNs in normal or attack rows.\",\n",
    "\n",
    "    \"Range checks: Physical plausibility bounds derived empirically from 1,178,988 \"\n",
    "    \"normal operation rows with a 10% margin. 29 sensors show out-of-range violations, \"\n",
    "    \"all confined to label=1 or label=2 rows  confirming bounds correctly characterize \"\n",
    "    \"normal operation and that faults/attacks produce distinguishable sensor excursions.\",\n",
    "\n",
    "    \"Biggest anomaly found: P3_LCP01D has the highest range violation count at 2,390 \"\n",
    "    \"rows (0.181%), split across label=2 (2,350) and label=1 (40). This sensor appears \"\n",
    "    \"sensitive to both fault injection and cyber attack manipulation.\",\n",
    "\n",
    "    \"Likely breakage scenario: A future pipeline run with a different injection seed \"\n",
    "    \"could produce fault rows in the test split, violating the train-only injection \"\n",
    "    \"contract. The injected parquet path is pinned to a specific RUN_ID to prevent this.\",\n",
    "\n",
    "    \"Temporal leakage risk: seconds_since_start is computed from the global dataset \"\n",
    "    \"minimum before splitting  acceptable since it references a fixed constant, not \"\n",
    "    \"a label-derived statistic. All other leakage risks are deferred to and documented \"\n",
    "    \"in the Feature Engineering notebook.\",\n",
    "\n",
    "    \"Data quality insight: Low row-count days (7 days below 50% of median) are \"\n",
    "    \"dataset boundary artifacts where HAI source files start or end mid-day. \"\n",
    "    \"2020-08-04 is the thinnest at 7,200 rows (2 hours). Split boundaries in \"\n",
    "    \"Feature Engineering should avoid these partial days.\",\n",
    "\n",
    "    \"Pipeline reproducibility: All artifacts are timestamped with RUN_ID \"\n",
    "    f\"({RUN_ID}). The run log links the injected parquet input to the curated \"\n",
    "    \"parquet and validation report outputs. Re-running this notebook with the \"\n",
    "    \"same injected parquet will produce identical outputs.\",\n",
    "]\n",
    "\n",
    "print(\"Pipeline Reflection\")\n",
    "print(\"=\" * 60)\n",
    "for i, note in enumerate(reflection, 1):\n",
    "    print(f\"\\n{i}. {note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890b52b-0042-4d96-b815-2567c4a1f797",
   "metadata": {},
   "source": [
    "# From here, move to the Feature Engineering Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7dfeb-d209-4c5f-9957-233fe7eb882b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
