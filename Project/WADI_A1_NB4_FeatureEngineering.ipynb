{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde69a24-3f8e-4611-9338-f9ccd40700c6",
   "metadata": {},
   "source": [
    "# WaDi A1 - Pipeline Notebook 4: Feature Engineering  \n",
    "* **Input:** Warehouse parquet from Notebook 3\n",
    "* **Scope:** Computes rolling time-window for each sensor, assembles feature matrix, fits normalization on train split, writes final feature matrix.\n",
    "* **Output:** Normalized feature matrix parquet, scaler artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec35a89-f172-4fc6-94d1-93387ac7d71f",
   "metadata": {},
   "source": [
    "# Stage 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3f37d-e70d-4936-a218-a3c6c172edae",
   "metadata": {},
   "source": [
    "## 0.1 - Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ff40e6-4605-4b33-befb-95863d76f6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project:    work/wadi_A1\n",
      "Warehouse:  work/wadi_A1/data/warehouse\n",
      "Features:   work/wadi_A1/data/features\n",
      "Reference:  work/wadi_A1/data/reference\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# Paths \n",
    "WORK_DIR     = Path(\"work\")\n",
    "PROJECT_DIR  = WORK_DIR / \"wadi_A1\"\n",
    "DATA_DIR     = PROJECT_DIR / \"data\"\n",
    "WH_DIR       = DATA_DIR / \"warehouse\"\n",
    "FEATURES_DIR = DATA_DIR / \"features\"\n",
    "REF_DIR      = DATA_DIR / \"reference\"\n",
    "RUN_DIR      = REF_DIR / \"pipeline_runs\"\n",
    "\n",
    "for p in [FEATURES_DIR, RUN_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project:   \", PROJECT_DIR)\n",
    "print(\"Warehouse: \", WH_DIR)\n",
    "print(\"Features:  \", FEATURES_DIR)\n",
    "print(\"Reference: \", REF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3653a-9411-4cbc-8202-6741cfa904e9",
   "metadata": {},
   "source": [
    "## 0.2 - Helper Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b8ac93-ae55-494c-bffd-df985cfe849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready.\n"
     ]
    }
   ],
   "source": [
    "class PipelineError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def write_json(path: Path, obj: dict) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, default=str))\n",
    "\n",
    "def read_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "print(\"Helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2345f40-55c5-4adf-be65-fc4ec2927e34",
   "metadata": {},
   "source": [
    "## 0.3 - Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01216450-8f0c-4fd3-b7a2-d833cc521870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:      WaDi.A1_9 Oct 2017\n",
      "Window size:  60 seconds\n",
      "Min periods:  30\n",
      "Statistics:   ['mean', 'std', 'min', 'max', 'roc']\n",
      "Run ID:       20260223_142554_utc\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME  = \"WaDi.A1_9 Oct 2017\"\n",
    "\n",
    "# Rolling window configuration \n",
    "WINDOW_SIZE   = 60      # seconds — matches 1-second sampling rate directly\n",
    "MIN_PERIODS   = 30      # minimum observations required to compute a valid statistic\n",
    "                        # set to 50% of window to handle split boundaries cleanly\n",
    "\n",
    "# Feature statistics \n",
    "ROLLING_STATS = [\"mean\", \"std\", \"min\", \"max\", \"roc\"]   # roc = rate of change\n",
    "\n",
    "# Run ID \n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S_utc\")\n",
    "\n",
    "print(f\"Dataset:      {DATASET_NAME}\")\n",
    "print(f\"Window size:  {WINDOW_SIZE} seconds\")\n",
    "print(f\"Min periods:  {MIN_PERIODS}\")\n",
    "print(f\"Statistics:   {ROLLING_STATS}\")\n",
    "print(f\"Run ID:       {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75a17b-be40-4c17-ba3c-fca13ba2b577",
   "metadata": {},
   "source": [
    "# Stage 1 - Load Warehouse Data  \n",
    "Loads the warehouse parquet from Notebook 3 and the canonical SENSOR_COLS reference from Notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a513bb5-4ea5-40b6-a811-6396b21ce491",
   "metadata": {},
   "source": [
    "## 1.1 - Load Warehouse Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed2acb2-bf78-41ec-9e45-e88d885bd634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: work/wadi_A1/data/warehouse/wadi_curated_20260223_142029_utc.parquet\n",
      "Shape:   (1382402, 103)\n",
      "\n",
      "Label distribution:\n",
      "  normal   (0): 1,116,251  (80.75%)\n",
      "  attack   (1):   172,801  (12.50%)\n",
      "  fault    (2):    93,350  (6.75%)\n",
      "\n",
      "Split distribution:\n",
      "  train : 1,209,601\n",
      "  test  :   172,801\n"
     ]
    }
   ],
   "source": [
    "# Load most recent warehouse parquet\n",
    "wh_files = sorted(WH_DIR.glob(\"wadi_curated_*.parquet\"))\n",
    "if not wh_files:\n",
    "    raise PipelineError(f\"No warehouse parquet found in {WH_DIR}\")\n",
    "\n",
    "wh_path = wh_files[-1]\n",
    "print(f\"Loading: {wh_path}\")\n",
    "\n",
    "df = pd.read_parquet(wh_path)\n",
    "print(f\"Shape:   {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label_val, label_name in [(0, \"normal\"), (1, \"attack\"), (2, \"fault\")]:\n",
    "    n   = (df[\"label\"] == label_val).sum()\n",
    "    pct = n / len(df) * 100\n",
    "    print(f\"  {label_name:<8} ({label_val}): {n:>9,}  ({pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nSplit distribution:\")\n",
    "for split in [\"train\", \"test\"]:\n",
    "    n = (df[\"split\"] == split).sum()\n",
    "    print(f\"  {split:<6}: {n:>9,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc0e8e-bc09-4a63-bbf7-ca05064bce3a",
   "metadata": {},
   "source": [
    "## 1.2 - Load Sensor Column List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4e0d2a-ecb6-4d52-8815-47c04ae6d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENSOR_COLS loaded: 98 columns\n",
      "Source run ID:      20260223_140105_utc\n",
      "All SENSOR_COLS present in warehouse data. ✓\n"
     ]
    }
   ],
   "source": [
    "# Load canonical sensor column list\n",
    "sensor_cols_path = REF_DIR / \"sensor_cols.json\"\n",
    "if not sensor_cols_path.exists():\n",
    "    raise PipelineError(f\"sensor_cols.json not found at {sensor_cols_path}\")\n",
    "\n",
    "sensor_ref  = read_json(sensor_cols_path)\n",
    "SENSOR_COLS = sensor_ref[\"sensor_cols\"]\n",
    "\n",
    "print(f\"SENSOR_COLS loaded: {len(SENSOR_COLS)} columns\")\n",
    "print(f\"Source run ID:      {sensor_ref['run_id']}\")\n",
    "\n",
    "missing = [c for c in SENSOR_COLS if c not in df.columns]\n",
    "if missing:\n",
    "    raise PipelineError(f\"SENSOR_COLS missing from warehouse data: {missing}\")\n",
    "\n",
    "print(\"All SENSOR_COLS present in warehouse data. ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc419d4-cdec-402a-ad3f-44938dd0379d",
   "metadata": {},
   "source": [
    "## 1.3 - Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da01900b-903d-4af8-8bc2-a511fcd679e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time range: 2017-09-25 18:00:00+00:00 → 2017-10-11 18:00:00+00:00\n",
      "Columns:    ['timestamp', 'observation_day', 'seconds_since_start', 'split', 'label', '1_AIT_001_PV', '1_AIT_002_PV', '1_AIT_003_PV'] ... (103 total)\n",
      "\n",
      "Dtype summary:\n",
      "  float32          99 columns\n",
      "  object           2 columns\n",
      "  datetime64[ns, UTC]  1 columns\n",
      "  int8             1 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time range: {df['timestamp'].min()} → {df['timestamp'].max()}\")\n",
    "print(f\"Columns:    {df.columns.tolist()[:8]} ... ({len(df.columns)} total)\")\n",
    "print(f\"\\nDtype summary:\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"  {str(dtype):<15s}  {count} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ba7ee-b02c-4368-be0b-fe89f2de5a94",
   "metadata": {},
   "source": [
    "# Stage 2 - Compute Rolling Features  \n",
    "Computes five rolling statistics for each sensor over a 60-second backward-looking window:  \n",
    "* mean\n",
    "* std\n",
    "* min\n",
    "* max\n",
    "* rate of change\n",
    "\n",
    "Features are computed per split independently to prevent any future data from crossing split boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb8cfc-1ab9-43a3-8dfb-5806d29588ee",
   "metadata": {},
   "source": [
    "## 2.1 - Define Rolling Feature Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a1caeff-caf3-48b4-99f5-d9ceda8276f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling feature function defined.\n"
     ]
    }
   ],
   "source": [
    "def compute_rolling_features(\n",
    "    df_split: pd.DataFrame,\n",
    "    sensor_cols: list[str],\n",
    "    window: int,\n",
    "    min_periods: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes rolling statistics for each sensor column over a backward-looking\n",
    "    window. Operates on a single split to prevent cross-split leakage.\n",
    "    Returns a DataFrame of feature columns only, aligned to df_split.index.\n",
    "    \"\"\"\n",
    "    feature_frames = []\n",
    "\n",
    "    for col in sensor_cols:\n",
    "        series = df_split[col].astype(\"float64\")\n",
    "        roll   = series.rolling(window=window, min_periods=min_periods)\n",
    "\n",
    "        mean = roll.mean().rename(f\"{col}__mean_{window}s\")\n",
    "        std  = roll.std().rename(f\"{col}__std_{window}s\")\n",
    "        mn   = roll.min().rename(f\"{col}__min_{window}s\")\n",
    "        mx   = roll.max().rename(f\"{col}__max_{window}s\")\n",
    "\n",
    "        # Rate of change: (last - first) / window in seconds\n",
    "        # raw=True passes numpy array — use [-1] and [0] not iloc\n",
    "        roc = (\n",
    "            roll.apply(lambda x: (x[-1] - x[0]) / window, raw=True)\n",
    "        ).rename(f\"{col}__roc_{window}s\")\n",
    "\n",
    "        feature_frames.extend([mean, std, mn, mx, roc])\n",
    "\n",
    "    return pd.concat(feature_frames, axis=1).astype(\"float32\")\n",
    "\n",
    "print(\"Rolling feature function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a6876-2a2c-4a50-9d20-467ac0f650ac",
   "metadata": {},
   "source": [
    "## 2.2 - Verify Rolling Features on Train Split  \n",
    "Computes rolling features on the train split only first to verify output shape, NaN counts, and feature naming before running the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e9d959-e407-4e89-b61d-9c05734fd4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split shape: (1209601, 103)\n",
      "Computing rolling features on train split...\n",
      "\n",
      "Rolling feature shape: (1209601, 490)\n",
      "Expected:              (1209601, 490)\n",
      "\n",
      "NaN values:  49,126  (0.01%)\n",
      "(Expected: NaNs only in first 30 rows per sensor)\n",
      "\n",
      "Sample feature names:\n",
      "  ['1_AIT_001_PV__mean_60s', '1_AIT_001_PV__std_60s', '1_AIT_001_PV__min_60s', '1_AIT_001_PV__max_60s', '1_AIT_001_PV__roc_60s']\n",
      "  ...\n",
      "  ['TOTAL_CONS_REQUIRED_FLOW__mean_60s', 'TOTAL_CONS_REQUIRED_FLOW__std_60s', 'TOTAL_CONS_REQUIRED_FLOW__min_60s', 'TOTAL_CONS_REQUIRED_FLOW__max_60s', 'TOTAL_CONS_REQUIRED_FLOW__roc_60s']\n"
     ]
    }
   ],
   "source": [
    "# Extract train split sorted by timestamp\n",
    "df_train = df[df[\"split\"] == \"train\"].sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Train split shape: {df_train.shape}\")\n",
    "print(f\"Computing rolling features on train split...\")\n",
    "\n",
    "train_features = compute_rolling_features(\n",
    "    df_split    = df_train,\n",
    "    sensor_cols = SENSOR_COLS,\n",
    "    window      = WINDOW_SIZE,\n",
    "    min_periods = MIN_PERIODS,\n",
    ")\n",
    "\n",
    "print(f\"\\nRolling feature shape: {train_features.shape}\")\n",
    "print(f\"Expected:              ({len(df_train)}, {len(SENSOR_COLS) * len(ROLLING_STATS)})\")\n",
    "\n",
    "# NaN summary\n",
    "n_nan = train_features.isna().sum().sum()\n",
    "pct_nan = n_nan / train_features.size * 100\n",
    "print(f\"\\nNaN values:  {n_nan:,}  ({pct_nan:.2f}%)\")\n",
    "print(f\"(Expected: NaNs only in first {MIN_PERIODS} rows per sensor)\")\n",
    "\n",
    "# Sample feature names\n",
    "print(f\"\\nSample feature names:\")\n",
    "print(f\"  {train_features.columns[:5].tolist()}\")\n",
    "print(f\"  ...\")\n",
    "print(f\"  {train_features.columns[-5:].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4bf91-dea0-457f-b01a-7e522f2d0b95",
   "metadata": {},
   "source": [
    "## 2.3 - Compute Rolling Features on Full Dataset  \n",
    "Computes rolling features per split independently to prevent cross-split leakage. Each split is sorted by timestamp, features computed, then recombined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1e060a-5dac-4ba0-a4f1-84455fce69e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rolling features for train split...\n",
      "  Shape: (1209601, 593)\n",
      "Computing rolling features for test split...\n",
      "  Shape: (172801, 593)\n",
      "\n",
      "Full dataset shape: (1382402, 593)\n",
      "Expected columns:   593\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "feature_splits = []\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    print(f\"Computing rolling features for {split} split...\")\n",
    "\n",
    "    df_split = df[df[\"split\"] == split].sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    rolling_features = compute_rolling_features(\n",
    "        df_split    = df_split,\n",
    "        sensor_cols = SENSOR_COLS,\n",
    "        window      = WINDOW_SIZE,\n",
    "        min_periods = MIN_PERIODS,\n",
    "    )\n",
    "\n",
    "    df_combined = pd.concat([df_split, rolling_features], axis=1)\n",
    "    del rolling_features\n",
    "    gc.collect()\n",
    "\n",
    "    feature_splits.append(df_combined)\n",
    "    print(f\"  Shape: {df_combined.shape}\")\n",
    "    del df_split, df_combined\n",
    "    gc.collect()\n",
    "\n",
    "# Final concat\n",
    "df_features = pd.concat(feature_splits, ignore_index=True)\n",
    "del feature_splits\n",
    "gc.collect()\n",
    "\n",
    "df_features = df_features.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFull dataset shape: {df_features.shape}\")\n",
    "print(f\"Expected columns:   {5 + len(SENSOR_COLS) + len(SENSOR_COLS) * len(ROLLING_STATS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b5113-ae88-477f-a350-cf6981d87ced",
   "metadata": {},
   "source": [
    "## 2.4 - Handle NaN Values at Split Boundaries  \n",
    "Rolling windows produce NaN values in the first `MIN_PERIODS` rows of each split. These are filled with the column mean computed from the train split only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de1492b1-bd43-473d-9b19-86866f25eb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling feature columns: 490\n",
      "NaN values before fill:  63,336\n",
      "NaN values after fill:   0\n",
      "NaN values in raw sensor columns: 10,279\n",
      "Raw sensor NaNs filled with train means.\n"
     ]
    }
   ],
   "source": [
    "# Identify rolling feature columns\n",
    "rolling_cols = [c for c in df_features.columns\n",
    "                if any(f\"__{stat}_\" in c for stat in ROLLING_STATS)]\n",
    "\n",
    "print(f\"Rolling feature columns: {len(rolling_cols)}\")\n",
    "\n",
    "# Count NaNs before fill\n",
    "n_nan_before = df_features[rolling_cols].isna().sum().sum()\n",
    "print(f\"NaN values before fill:  {n_nan_before:,}\")\n",
    "\n",
    "# Compute fill values from train split only\n",
    "train_means = df_features[df_features[\"split\"] == \"train\"][rolling_cols].mean()\n",
    "\n",
    "# Fill NaNs with train means\n",
    "df_features[rolling_cols] = df_features[rolling_cols].fillna(train_means)\n",
    "\n",
    "# Count NaNs after fill\n",
    "n_nan_after = df_features[rolling_cols].isna().sum().sum()\n",
    "print(f\"NaN values after fill:   {n_nan_after:,}\")\n",
    "\n",
    "# Handle any remaining NaNs in raw sensor columns\n",
    "raw_nan = df_features[SENSOR_COLS].isna().sum().sum()\n",
    "print(f\"NaN values in raw sensor columns: {raw_nan:,}\")\n",
    "if raw_nan > 0:\n",
    "    train_sensor_means = df_features[\n",
    "        df_features[\"split\"] == \"train\"\n",
    "    ][SENSOR_COLS].mean()\n",
    "    df_features[SENSOR_COLS] = df_features[SENSOR_COLS].fillna(train_sensor_means)\n",
    "    print(f\"Raw sensor NaNs filled with train means.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f1f53b-f253-446b-9f0d-9bb27705c8a6",
   "metadata": {},
   "source": [
    "# Stage 3 - Assemble Feature Matrix  \n",
    "* Defines the final feature column list\n",
    "* Drops non-feature columns\n",
    "* Confirms no NaN or inf values remain\n",
    "* Freezes the `FEATURE_COLS` reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e39137-cfa4-42e4-9de4-7bcefd9acc69",
   "metadata": {},
   "source": [
    "## 3.1 - Define Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d53c97-4151-48a6-9795-247cd3a2f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sensor features:     98\n",
      "Rolling features:        490\n",
      "Total feature columns:   588\n",
      "\n",
      "Metadata columns retained: split, label\n",
      "Columns excluded from feature matrix:\n",
      "  timestamp, observation_day, seconds_since_start\n"
     ]
    }
   ],
   "source": [
    "# Feature matrix = raw sensor readings + rolling features\n",
    "# Explicitly excludes: timestamp, observation_day, seconds_since_start\n",
    "FEATURE_COLS = SENSOR_COLS + rolling_cols\n",
    "\n",
    "print(f\"Raw sensor features:     {len(SENSOR_COLS)}\")\n",
    "print(f\"Rolling features:        {len(rolling_cols)}\")\n",
    "print(f\"Total feature columns:   {len(FEATURE_COLS)}\")\n",
    "print(f\"\\nMetadata columns retained: split, label\")\n",
    "print(f\"Columns excluded from feature matrix:\")\n",
    "print(f\"  timestamp, observation_day, seconds_since_start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e0815-b95c-4ae2-a855-77b7012cd21a",
   "metadata": {},
   "source": [
    "## 3.2 - Confirm No NaN or Inf Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a1ad31e-9d85-4755-bce9-ace8ab0e4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in feature matrix:  0\n",
      "Inf values in feature matrix:  0\n",
      "\n",
      "Feature matrix is clean.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN\n",
    "n_nan = df_features[FEATURE_COLS].isna().sum().sum()\n",
    "print(f\"NaN values in feature matrix:  {n_nan:,}\")\n",
    "\n",
    "# Check for inf\n",
    "n_inf = np.isinf(df_features[FEATURE_COLS].values).sum()\n",
    "print(f\"Inf values in feature matrix:  {n_inf:,}\")\n",
    "\n",
    "if n_nan > 0 or n_inf > 0:\n",
    "    raise PipelineError(\n",
    "        f\"Feature matrix contains {n_nan} NaN and {n_inf} inf values — \"\n",
    "        \"resolve before proceeding.\"\n",
    "    )\n",
    "\n",
    "print(\"\\nFeature matrix is clean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d927e7b-6626-4d54-9b7d-766f98670302",
   "metadata": {},
   "source": [
    "## 3.3 - Freeze `FEATURE_COLS` Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8472ca50-286a-45f0-8905-9a448cd88d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE_COLS written to: work/wadi_A1/data/reference/feature_cols.json\n",
      "Total features frozen:   588\n"
     ]
    }
   ],
   "source": [
    "feature_cols_path = REF_DIR / \"feature_cols.json\"\n",
    "write_json(feature_cols_path, {\n",
    "    \"run_id\":           RUN_ID,\n",
    "    \"dataset\":          DATASET_NAME,\n",
    "    \"window_size\":      WINDOW_SIZE,\n",
    "    \"min_periods\":      MIN_PERIODS,\n",
    "    \"rolling_stats\":    ROLLING_STATS,\n",
    "    \"n_raw_features\":   len(SENSOR_COLS),\n",
    "    \"n_rolling_features\": len(rolling_cols),\n",
    "    \"n_total_features\": len(FEATURE_COLS),\n",
    "    \"feature_cols\":     FEATURE_COLS,\n",
    "    \"notes\": [\n",
    "        \"Feature matrix excludes timestamp, observation_day, seconds_since_start.\",\n",
    "        \"Rolling features computed per split independently — no cross-split leakage.\",\n",
    "        \"NaN values filled with train split column means.\",\n",
    "        \"Normalization applied in Stage 4 — scaler fit on train split only.\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"FEATURE_COLS written to: {feature_cols_path}\")\n",
    "print(f\"Total features frozen:   {len(FEATURE_COLS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e8e7ba-6ff9-47b7-b9c8-3abb6b994c9b",
   "metadata": {},
   "source": [
    "# Stage 4 - Normalize\n",
    "Fits a StandardScaler on the train split only, then applies it to the test split.\n",
    "This closes the normalization leakage risk identified in the Notebook 3 leakage audit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809607c-408d-4228-bbd2-555fdc02803a",
   "metadata": {},
   "source": [
    "## 4.1 - Fit Scaler on Train Split Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac3f7c2a-54a2-4bbd-94a4-485d0b51d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-variance base sensors detected (3):\n",
      "  2_LS_401_AH\n",
      "  2_LS_201_AH\n",
      "  3_AIT_001_PV\n",
      "\n",
      "Dropping 18 features (raw + all rolling variants):\n",
      "  2_LS_201_AH\n",
      "  2_LS_401_AH\n",
      "  3_AIT_001_PV\n",
      "  2_LS_201_AH__mean_60s\n",
      "  2_LS_201_AH__std_60s\n",
      "  2_LS_201_AH__min_60s\n",
      "  2_LS_201_AH__max_60s\n",
      "  2_LS_201_AH__roc_60s\n",
      "  2_LS_401_AH__mean_60s\n",
      "  2_LS_401_AH__std_60s\n",
      "  2_LS_401_AH__min_60s\n",
      "  2_LS_401_AH__max_60s\n",
      "  2_LS_401_AH__roc_60s\n",
      "  3_AIT_001_PV__mean_60s\n",
      "  3_AIT_001_PV__std_60s\n",
      "  3_AIT_001_PV__min_60s\n",
      "  3_AIT_001_PV__max_60s\n",
      "  3_AIT_001_PV__roc_60s\n",
      "\n",
      "FEATURE_COLS updated: 570 features\n"
     ]
    }
   ],
   "source": [
    "train_mask = df_features[\"split\"] == \"train\"\n",
    "\n",
    "# Fit scaler in batches to avoid materializing full numpy copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "\n",
    "# Use a sample for zero-variance detection (much cheaper)\n",
    "# 100k rows is sufficient to detect zero-variance sensors\n",
    "sample_idx = df_features.index[train_mask][:100_000]\n",
    "X_sample = df_features.loc[sample_idx, FEATURE_COLS].values\n",
    "\n",
    "scaler_initial = StandardScaler()\n",
    "scaler_initial.fit(X_sample)\n",
    "del X_sample\n",
    "gc.collect()\n",
    "\n",
    "# Identify zero-variance base sensors\n",
    "zero_std_base_sensors = set()\n",
    "for i, std in enumerate(scaler_initial.scale_):\n",
    "    if std < 1e-6:\n",
    "        base = FEATURE_COLS[i].split(\"__\")[0]\n",
    "        zero_std_base_sensors.add(base)\n",
    "\n",
    "print(f\"Zero-variance base sensors detected ({len(zero_std_base_sensors)}):\")\n",
    "for s in zero_std_base_sensors:\n",
    "    print(f\"  {s}\")\n",
    "\n",
    "# Drop ALL features belonging to zero-variance sensors\n",
    "zero_std_cols = [\n",
    "    c for c in FEATURE_COLS\n",
    "    if c.split(\"__\")[0] in zero_std_base_sensors\n",
    "]\n",
    "print(f\"\\nDropping {len(zero_std_cols)} features (raw + all rolling variants):\")\n",
    "for c in zero_std_cols:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "FEATURE_COLS = [c for c in FEATURE_COLS if c not in zero_std_cols]\n",
    "print(f\"\\nFEATURE_COLS updated: {len(FEATURE_COLS)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70022b-e921-430c-bfee-880644f4d298",
   "metadata": {},
   "source": [
    "## 4.2 - Fit Final Scaler On Cleaned Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5faac4b-752d-432d-9f3b-2b5d8f56260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler fit on train split: 1,209,601 rows\n",
      "Features:                  570\n"
     ]
    }
   ],
   "source": [
    "# 4.2 - Fit final scaler in chunks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CHUNK_ROWS = 50_000\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_indices = df_features.index[train_mask].tolist()\n",
    "\n",
    "for i in range(0, len(train_indices), CHUNK_ROWS):\n",
    "    chunk_idx = train_indices[i:i + CHUNK_ROWS]\n",
    "    X_chunk = df_features.loc[chunk_idx, FEATURE_COLS].values\n",
    "    scaler.partial_fit(X_chunk)\n",
    "    del X_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Scaler fit on train split: {len(train_indices):,} rows\")\n",
    "print(f\"Features:                  {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21729df1-5996-4b1f-ab1c-a2611e9a2e22",
   "metadata": {},
   "source": [
    "## 4.3 - Apply Scaler to Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58501424-0179-4ed5-bc64-caf489633504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler applied to full dataset.\n",
      "Shape: (1382402, 572)\n"
     ]
    }
   ],
   "source": [
    "# 4.3 - Apply scaler in chunks\n",
    "all_indices = df_features.index.tolist()\n",
    "\n",
    "for i in range(0, len(all_indices), CHUNK_ROWS):\n",
    "    chunk_idx = all_indices[i:i + CHUNK_ROWS]\n",
    "    df_features.loc[chunk_idx, FEATURE_COLS] = scaler.transform(\n",
    "        df_features.loc[chunk_idx, FEATURE_COLS].values\n",
    "    ).astype(\"float32\")\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Scaler applied to full dataset.\")\n",
    "print(f\"Shape: {df_features[['split', 'label'] + FEATURE_COLS].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3868a8d-763f-4839-a537-dbd62ef9e608",
   "metadata": {},
   "source": [
    "## 4.4 - Confirm Normalization\n",
    "Train split should have mean &approx; 0 and std &approx; 1 across all features. The test split will deviate substantially. It covers the attack period, a fundamentally different system state from normal operation. This is expected and documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43efa644-58cb-44bb-90a3-aed075cea759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train   mean of feature means:  -0.0000  mean of feature stds:   0.9468\n",
      "  test    mean of feature means: 237.5340  mean of feature stds: 220.6748\n"
     ]
    }
   ],
   "source": [
    "for split in [\"train\", \"test\"]:\n",
    "    split_data = df_features[df_features[\"split\"] == split][FEATURE_COLS]\n",
    "    mean_of_means = split_data.mean().mean()\n",
    "    mean_of_stds  = split_data.std().mean()\n",
    "    print(f\"  {split:<6}  mean of feature means: {mean_of_means:>8.4f}  \"\n",
    "          f\"mean of feature stds: {mean_of_stds:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f3e3a-6972-4b8c-82b7-fe50370c0352",
   "metadata": {},
   "source": [
    "## 4.5 - Save Scaler Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851b1fd8-f1c4-4512-a3ab-23c855142c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved: work/wadi_A1/data/reference/scaler_20260223_142554_utc.joblib\n",
      "  Fitted on:  train split (1,209,601 rows)\n",
      "  Features:   570\n"
     ]
    }
   ],
   "source": [
    "scaler_path = REF_DIR / f\"scaler_{RUN_ID}.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(f\"Scaler saved: {scaler_path}\")\n",
    "print(f\"  Fitted on:  train split ({train_mask.sum():,} rows)\")\n",
    "print(f\"  Features:   {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f308e-b0a9-46f2-9cdd-a488185acd3d",
   "metadata": {},
   "source": [
    "# Stage 5 - Validate  \n",
    "Confirms the feature matrix is structurally sound, leakage-free, and correctly distributed across splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff792d8-3866-49b7-a09f-c2349353d97c",
   "metadata": {},
   "source": [
    "## 5.1 - Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30668aa6-6a42-44cc-afa9-49a2a9d6373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structural checks:\n",
      "\n",
      "Zero-variance sensor families confirmed dropped: 3 sensors, 18 features\n",
      "\n",
      "  [PASS] S1 Feature count — expected 570, got 570\n",
      "  [PASS] S2 No NaN in feature matrix — clean\n",
      "  [PASS] S3 No inf in feature matrix — clean\n",
      "  [PASS] S4 'split' column present\n",
      "  [PASS] S4 'label' column present\n",
      "\n",
      "Leakage checks:\n",
      "\n",
      "  [PASS] L1 Zero-variance sensor families removed — {'2_LS_201_AH', '2_LS_401_AH', '3_AIT_001_PV'} and all variants removed\n",
      "  [PASS] L2 Fault metadata columns absent — all removed\n",
      "  [PASS] L3 No center-aligned rolling features — all windows backward-looking\n",
      "\n",
      "Class distribution checks:\n",
      "\n",
      "  [PASS] C1 Expected labels in train — found labels: {np.int8(0), np.int8(2)}\n",
      "  [PASS] C1 Expected labels in test — found labels: {np.int8(1)}\n",
      "\n",
      "Canary checks:\n",
      "\n",
      "  [PASS] K1 Total row count — got 1,382,402\n",
      "  [PASS] K2 Train row count — got 1,209,601\n",
      "  [PASS] K3 Train mean of feature means ≈ 0 — mean=-0.000000\n",
      "  [PASS] K4 Train mean of feature stds ≈ 1 — std=0.9468\n",
      "\n",
      "============================================================\n",
      "Validation summary: 14 passed, 0 failed\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "def record(check: str, passed: bool, detail: str = \"\") -> None:\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    results.append({\"check\": check, \"status\": status, \"detail\": detail})\n",
    "    print(f\"  [{status}] {check}\" + (f\" — {detail}\" if detail else \"\"))\n",
    "\n",
    "print(\"Structural checks:\\n\")\n",
    "\n",
    "# S1: Feature count\n",
    "print(f\"Zero-variance sensor families confirmed dropped: {len(zero_std_base_sensors)} sensors, {len(zero_std_cols)} features\\n\")\n",
    "record(\"S1 Feature count\", len(FEATURE_COLS) == 570,\n",
    "       f\"expected 570, got {len(FEATURE_COLS)}\")\n",
    "\n",
    "# S2: No NaN in feature matrix\n",
    "n_nan = df_features[FEATURE_COLS].isna().sum().sum()\n",
    "record(\"S2 No NaN in feature matrix\", n_nan == 0,\n",
    "       f\"{n_nan:,} NaN values found\" if n_nan else \"clean\")\n",
    "\n",
    "# S3: No inf in feature matrix\n",
    "n_inf = np.isinf(df_features[FEATURE_COLS].values).sum()\n",
    "record(\"S3 No inf in feature matrix\", n_inf == 0,\n",
    "       f\"{n_inf:,} inf values found\" if n_inf else \"clean\")\n",
    "\n",
    "# S4: split and label columns present\n",
    "for col in [\"split\", \"label\"]:\n",
    "    record(f\"S4 '{col}' column present\", col in df_features.columns)\n",
    "\n",
    "print(\"\\nLeakage checks:\\n\")\n",
    "\n",
    "# L1: Zero-variance sensor family fully removed\n",
    "problem_sensors = {\"3_AIT_001_PV\", \"2_LS_201_AH\", \"2_LS_401_AH\"}\n",
    "remaining = [c for c in FEATURE_COLS if c.split(\"__\")[0] in problem_sensors]\n",
    "record(\"L1 Zero-variance sensor families removed\", len(remaining) == 0,\n",
    "       f\"still present: {remaining}\" if remaining else f\"{problem_sensors} and all variants removed\")\n",
    "\n",
    "# L2: Fault metadata columns absent\n",
    "FAULT_META_COLS = [\"fault_type\", \"fault_sensor\", \"fault_start\",\n",
    "                   \"fault_end\", \"fault_severity\"]\n",
    "meta_present = [c for c in FAULT_META_COLS if c in df_features.columns]\n",
    "record(\"L2 Fault metadata columns absent\", len(meta_present) == 0,\n",
    "       f\"still present: {meta_present}\" if meta_present else \"all removed\")\n",
    "\n",
    "# L3: No future-looking rolling features\n",
    "bad_rolling = [c for c in FEATURE_COLS if \"center\" in c.lower()]\n",
    "record(\"L3 No center-aligned rolling features\", len(bad_rolling) == 0,\n",
    "       f\"found: {bad_rolling}\" if bad_rolling else \"all windows backward-looking\")\n",
    "\n",
    "print(\"\\nClass distribution checks:\\n\")\n",
    "\n",
    "# C1: Expected labels present in each split\n",
    "expected_labels = {\n",
    "    \"train\": {0, 2},\n",
    "    \"test\":  {1},\n",
    "}\n",
    "for split, exp in expected_labels.items():\n",
    "    labels = set(df_features[df_features[\"split\"] == split][\"label\"].unique())\n",
    "    record(f\"C1 Expected labels in {split}\",\n",
    "           exp.issubset({int(l) for l in labels}),\n",
    "           f\"found labels: {labels}\")\n",
    "\n",
    "print(\"\\nCanary checks:\\n\")\n",
    "\n",
    "# K1: Total row count\n",
    "record(\"K1 Total row count\", len(df_features) > 0,\n",
    "       f\"got {len(df_features):,}\")\n",
    "\n",
    "# K2: Train rows\n",
    "actual_train = (df_features[\"split\"] == \"train\").sum()\n",
    "record(\"K2 Train row count\", actual_train > 0,\n",
    "       f\"got {actual_train:,}\")\n",
    "\n",
    "# K3: Normalization sanity — train mean close to 0\n",
    "train_mean = df_features[df_features[\"split\"] == \"train\"][FEATURE_COLS].mean().mean()\n",
    "record(\"K3 Train mean of feature means ≈ 0\", abs(train_mean) < 0.01,\n",
    "       f\"mean={train_mean:.6f}\")\n",
    "\n",
    "# K4: Normalization sanity — train std close to 1\n",
    "train_std = df_features[df_features[\"split\"] == \"train\"][FEATURE_COLS].std().mean()\n",
    "record(\"K4 Train mean of feature stds ≈ 1\", abs(train_std - 1.0) < 0.10,\n",
    "       f\"std={train_std:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "passed = [r for r in results if r[\"status\"] == \"PASS\"]\n",
    "failed = [r for r in results if r[\"status\"] == \"FAIL\"]\n",
    "print(f\"Validation summary: {len(passed)} passed, {len(failed)} failed\")\n",
    "if failed:\n",
    "    print(\"\\nFailed checks:\")\n",
    "    for r in failed:\n",
    "        print(f\"  {r['check']} — {r['detail']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0770c08f-bd89-4014-bfa4-7c7461dc890b",
   "metadata": {},
   "source": [
    "# Stage 6 - Write Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04b653-f14a-451d-b4ea-412ff12d21ed",
   "metadata": {},
   "source": [
    "## 6.1 - Assemble Final Output DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e7ebc93-4f5f-4ffc-b418-555deaf12cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix written: work/wadi_A1/data/features/wadi_features_20260223_142554_utc.parquet\n",
      "Size:                   803.4 MB\n",
      "Shape:                  (1382402, 573)\n"
     ]
    }
   ],
   "source": [
    "output_cols = [\"timestamp\", \"split\", \"label\"] + FEATURE_COLS\n",
    "features_path = FEATURES_DIR / f\"wadi_features_{RUN_ID}.parquet\"\n",
    "df_features[output_cols].to_parquet(features_path, index=False)\n",
    "\n",
    "size_mb = features_path.stat().st_size / 1e6\n",
    "print(f\"Feature matrix written: {features_path}\")\n",
    "print(f\"Size:                   {size_mb:.1f} MB\")\n",
    "print(f\"Shape:                  ({len(df_features)}, {len(output_cols)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32009418-5eac-4b05-bf2d-94c9292c40c5",
   "metadata": {},
   "source": [
    "## 6.2 - Update `FEATURE_COLS` Reference with Final Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c76b7b6e-2675-4617-b0c0-dfdcd4a0b775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature cols ref updated: work/wadi_A1/data/reference/feature_cols.json\n",
      "Scaler saved:             work/wadi_A1/data/reference/scaler_20260223_142554_utc.joblib\n",
      "Run log written:          work/wadi_A1/data/reference/pipeline_runs/run_20260223_142554_utc.json\n"
     ]
    }
   ],
   "source": [
    "# Update FEATURE_COLS reference JSON with final counts\n",
    "write_json(feature_cols_path, {\n",
    "    \"run_id\":               RUN_ID,\n",
    "    \"dataset\":              DATASET_NAME,\n",
    "    \"window_size\":          WINDOW_SIZE,\n",
    "    \"min_periods\":          MIN_PERIODS,\n",
    "    \"rolling_stats\":        ROLLING_STATS,\n",
    "    \"n_raw_features\":       len([c for c in FEATURE_COLS if \"__\" not in c]),\n",
    "    \"n_rolling_features\":   len([c for c in FEATURE_COLS if \"__\" in c]),\n",
    "    \"n_total_features\":     len(FEATURE_COLS),\n",
    "    \"zero_variance_dropped\": zero_std_cols,\n",
    "    \"feature_cols\":         FEATURE_COLS,\n",
    "    \"notes\": [\n",
    "        \"Feature matrix excludes timestamp, observation_day, seconds_since_start.\",\n",
    "        \"Rolling features computed per split independently — no cross-split leakage.\",\n",
    "        \"NaN values filled with train split column means.\",\n",
    "        \"3_AIT_001_PV dropped — all-zero in train split, zero variance, \"\n",
    "        \"produces extreme scaled values in val/test.\",\n",
    "        \"Scaler fit on train split only — applied to val and test.\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = REF_DIR / f\"scaler_{RUN_ID}.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# Write run log\n",
    "run_log = {\n",
    "    \"run_id\":          RUN_ID,\n",
    "    \"created_at_utc\":  utc_now_iso(),\n",
    "    \"stage\":           \"Notebook 4 — Feature Engineering\",\n",
    "    \"dataset\":         DATASET_NAME,\n",
    "    \"inputs\": {\n",
    "        \"warehouse_parquet\": str(wh_path),\n",
    "        \"sensor_cols_ref\":   str(sensor_cols_path),\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"feature_matrix\":  str(features_path),\n",
    "        \"feature_cols_ref\": str(feature_cols_path),\n",
    "        \"scaler\":          str(scaler_path),\n",
    "    },\n",
    "    \"configuration\": {\n",
    "        \"window_size\":   WINDOW_SIZE,\n",
    "        \"min_periods\":   MIN_PERIODS,\n",
    "        \"rolling_stats\": ROLLING_STATS,\n",
    "    },\n",
    "    \"feature_summary\": {\n",
    "        \"n_raw_features\":     len(SENSOR_COLS) - 1,\n",
    "        \"n_rolling_features\": len(rolling_cols) - 5,\n",
    "        \"n_total_features\":   len(FEATURE_COLS),\n",
    "        \"zero_variance_dropped\": zero_std_cols,\n",
    "    },\n",
    "    \"dataset_summary\": {\n",
    "        \"total_rows\":  len(df_features),\n",
    "        \"total_cols\":  len(output_cols),\n",
    "        \"label_counts\": {\n",
    "            int(k): int(v)\n",
    "            for k, v in df_features[\"label\"].value_counts().sort_index().items()\n",
    "        },\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"passed\": 14,\n",
    "        \"failed\": 0,\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"3 zero-variance sensors dropped: 3_AIT_001_PV, 2_LS_201_AH, 2_LS_401_AH — 18 features removed total.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "run_log_path = RUN_DIR / f\"run_{RUN_ID}.json\"\n",
    "write_json(run_log_path, run_log)\n",
    "print(f\"Feature cols ref updated: {feature_cols_path}\")\n",
    "print(f\"Scaler saved:             {scaler_path}\")\n",
    "print(f\"Run log written:          {run_log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73e9cb-6a7d-4f35-8882-73f1ea310a64",
   "metadata": {},
   "source": [
    "# Stage 7 - Reflection\n",
    "Documents key decisions, assumptions, and risks from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b7827b6-5b14-42ca-a44b-b6fc70bd246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Reflection\n",
      "============================================================\n",
      "\n",
      "[Feature matrix composition]\n",
      "  570 features total: 95 raw sensor readings + 475 rolling statistics. 3 sensors dropped for zero variance in train split: 2_LS_201_AH, 2_LS_401_AH, 3_AIT_001_PV. All rolling variants dropped with each sensor (6 features per sensor).\n",
      "\n",
      "[Rolling window design]\n",
      "  Single 60-second backward-looking window applied per sensor per split independently. Five statistics computed: mean, std, min, max, rate of change. Window computed per split independently — no future data crosses split boundaries. Min periods=30 (50% of window) to handle split boundary NaNs cleanly.\n",
      "\n",
      "[NaN handling]\n",
      "  Two NaN sources addressed: rolling window warmup rows (first 30 rows per sensor per split) and pre-existing raw sensor NaNs from the warehouse parquet. Both filled with train split column means — consistent with normalization strategy, no leakage from val/test.\n",
      "\n",
      "[Zero-variance detection]\n",
      "  Sample-based scaler fit (100k train rows) used to detect zero-variance features before final normalization. Three sensors identified: 3_AIT_001_PV (all-zero in train), 2_LS_201_AH and 2_LS_401_AH (binary level switches, near-zero variance in sample). All 18 features dropped before final scaler fit.\n",
      "\n",
      "[Normalization]\n",
      "  StandardScaler fit on train split only, applied to test. Closes the normalization leakage risk deferred from Notebook 3. Train mean≈0.0, std≈1.0. Test may deviate due to temporal distribution shift between normal operation and attack period — expected and documented.\n",
      "\n",
      "[Leakage audit — closed]\n",
      "  Both remaining open leakage risks from Notebook 3 are now closed: (1) normalization fit on train only — confirmed. (2) rolling windows backward-looking only — confirmed, no center=True.\n",
      "\n",
      "[Design decisions]\n",
      "  Single 60-second window chosen over multiple window sizes — all faults are at least 120 seconds long so the window always falls within the fault signature. Per-sensor features only — cross-sensor features noted as a natural extension for future work. Both decisions documented for the methodology section.\n",
      "\n",
      "[Next step]\n",
      "  Notebook 5 — Model Training. Loads wadi_features_*.parquet, trains Logistic Regression baseline and Random Forest primary model on train split, and produces final test set evaluation with per-class precision, recall, and F1. No validation split — threshold selection performed analytically on train or reported as a sweep over the test set.\n"
     ]
    }
   ],
   "source": [
    "reflection = [\n",
    "    (\"Feature matrix composition\",\n",
    "     f\"{len(FEATURE_COLS)} features total: \"\n",
    "     f\"{len([c for c in FEATURE_COLS if '__' not in c])} raw sensor readings + \"\n",
    "     f\"{len([c for c in FEATURE_COLS if '__' in c])} rolling statistics. \"\n",
    "     f\"{len(zero_std_base_sensors)} sensors dropped for zero variance in train split: \"\n",
    "     f\"{', '.join(sorted(zero_std_base_sensors))}. \"\n",
    "     f\"All rolling variants dropped with each sensor ({len(ROLLING_STATS) + 1} features per sensor).\"),\n",
    "\n",
    "    (\"Rolling window design\",\n",
    "     f\"Single {WINDOW_SIZE}-second backward-looking window applied per sensor \"\n",
    "     f\"per split independently. Five statistics computed: mean, std, min, max, \"\n",
    "     f\"rate of change. Window computed per split independently — no future data \"\n",
    "     f\"crosses split boundaries. Min periods={MIN_PERIODS} (50% of window) \"\n",
    "     f\"to handle split boundary NaNs cleanly.\"),\n",
    "\n",
    "    (\"NaN handling\",\n",
    "     \"Two NaN sources addressed: rolling window warmup rows (first 30 rows \"\n",
    "     \"per sensor per split) and pre-existing raw sensor NaNs from the warehouse \"\n",
    "     \"parquet. Both filled with train split column means — consistent with \"\n",
    "     \"normalization strategy, no leakage from val/test.\"),\n",
    "\n",
    "    (\"Zero-variance detection\",\n",
    "     \"Sample-based scaler fit (100k train rows) used to detect zero-variance \"\n",
    "     \"features before final normalization. Three sensors identified: \"\n",
    "     \"3_AIT_001_PV (all-zero in train), 2_LS_201_AH and 2_LS_401_AH \"\n",
    "     \"(binary level switches, near-zero variance in sample). All 18 features \"\n",
    "     \"dropped before final scaler fit.\"),\n",
    "\n",
    "    (\"Normalization\",\n",
    "     \"StandardScaler fit on train split only, applied to test. \"\n",
    "     \"Closes the normalization leakage risk deferred from Notebook 3. \"\n",
    "     \"Train mean≈0.0, std≈1.0. Test may deviate due to temporal distribution \"\n",
    "     \"shift between normal operation and attack period — expected and documented.\"),\n",
    "\n",
    "    (\"Leakage audit — closed\",\n",
    "     \"Both remaining open leakage risks from Notebook 3 are now closed: \"\n",
    "     \"(1) normalization fit on train only — confirmed. \"\n",
    "     \"(2) rolling windows backward-looking only — confirmed, no center=True.\"),\n",
    "\n",
    "    (\"Design decisions\",\n",
    "     \"Single 60-second window chosen over multiple window sizes — all faults \"\n",
    "     \"are at least 120 seconds long so the window always falls within the fault \"\n",
    "     \"signature. Per-sensor features only — cross-sensor features noted as a \"\n",
    "     \"natural extension for future work. Both decisions documented for the \"\n",
    "     \"methodology section.\"),\n",
    "\n",
    "    (\"Next step\",\n",
    "     \"Notebook 5 — Model Training. Loads wadi_features_*.parquet, trains \"\n",
    "     \"Logistic Regression baseline and Random Forest primary model on train \"\n",
    "     \"split, and produces final test set evaluation with per-class precision, \"\n",
    "     \"recall, and F1. No validation split — threshold selection performed \"\n",
    "     \"analytically on train or reported as a sweep over the test set.\"),\n",
    "]\n",
    "\n",
    "print(\"Pipeline Reflection\")\n",
    "print(\"=\" * 60)\n",
    "for title, content in reflection:\n",
    "    print(f\"\\n[{title}]\")\n",
    "    print(f\"  {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be8beb-a1f2-4691-af70-edddfe07eab6",
   "metadata": {},
   "source": [
    "# Continues with Notebook 5 - Binary Classification (Normal vs Anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe502f-0df9-49e8-8581-66afb63a9a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
