{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089bde07-efd3-45ba-b2fc-46daae2fa5b3",
   "metadata": {},
   "source": [
    "# WaDi A1 - Pipeline Notebook 1: Ingest & Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e1801-dfc2-4a95-aceb-0ece69ad06e2",
   "metadata": {},
   "source": [
    "The notebook will cover the following:  \n",
    "* **Dataset:** WaDi.A1_9 Oct 2017 - Water Distribution testbed, iTrust Labs, SUTD\n",
    "* **Scope:** Stages 0-2. Ingests raw CSV files, cleans and types the data, drops uninformative sensor columns, and assigns a temporal train/test split following the standard WaDi evaluation protocol.\n",
    "* **Split strategy:** Normal operation rows (label=0) -> train. Attack period rows (label=1) -> test. This matches the convention used in the ICS anomaly detection literature\n",
    "* **Output:** A staged Parquet file with a `split` column, ready for fault injection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fdde4d-e63a-4170-bee4-d3277ba2996b",
   "metadata": {},
   "source": [
    "# Stage 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6c403-bdcf-4bb5-901b-0b34c571d15a",
   "metadata": {},
   "source": [
    "## 0.1 - Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4edffe-5bf4-4ffe-a371-714e2411e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: work/wadi_A1\n",
      "Raw:     work/wadi_A1/data/raw\n",
      "Staged:  work/wadi_A1/data/staged\n",
      "Ref:     work/wadi_A1/data/reference\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# ── Paths ─────────────────────────────────────────────────────────────────────\n",
    "WORK_DIR    = Path(\"work\")\n",
    "PROJECT_DIR = WORK_DIR / \"wadi_A1\"\n",
    "DATA_DIR    = PROJECT_DIR / \"data\"\n",
    "RAW_DIR     = DATA_DIR / \"raw\"\n",
    "STAGED_DIR  = DATA_DIR / \"staged\"\n",
    "REF_DIR     = DATA_DIR / \"reference\"\n",
    "RUN_DIR     = REF_DIR / \"pipeline_runs\"\n",
    "\n",
    "for p in [RAW_DIR, STAGED_DIR, REF_DIR, RUN_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project:\", PROJECT_DIR)\n",
    "print(\"Raw:    \", RAW_DIR)\n",
    "print(\"Staged: \", STAGED_DIR)\n",
    "print(\"Ref:    \", REF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01bb934-d14f-4843-94cc-c1873e0ae858",
   "metadata": {},
   "source": [
    "## 0.2 Helper Utilities  \n",
    "Reusable functions used throughout the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b24fa1-c564-4832-9075-65c563570706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready.\n"
     ]
    }
   ],
   "source": [
    "class PipelineError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def sha16(x: str) -> str:\n",
    "    return hashlib.sha256(x.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def write_json(path: Path, obj: dict) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, default=str))\n",
    "\n",
    "def read_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "def require_columns(df: pd.DataFrame, cols: list[str], context: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise PipelineError(f\"[{context}] Missing required columns: {missing}\")\n",
    "\n",
    "print(\"Helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e4cf6-0b2b-42ba-8272-ab33f45a3846",
   "metadata": {},
   "source": [
    "## 0.3 - Configuration  \n",
    "Pipeline constants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b3e1e6-3875-4cef-bded-c65c0c56e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  WaDi.A1_9 Oct 2017\n",
      "Source:   iTrust Labs, SUTD\n",
      "Run ID:   20260223_140105_utc\n",
      "Split:    normal operation → train | attack period → test\n"
     ]
    }
   ],
   "source": [
    "# Dataset Identity\n",
    "DATASET_NAME   = \"WaDi.A1_9 Oct 2017\"\n",
    "DATA_SOURCE    = \"iTrust Labs, SUTD\"\n",
    "DATASET_URL    = \"https://itrust.sutd.edu.sg/itrust-labs_datasets/dataset_info/\"\n",
    "DOWNLOAD_DATE  = \"2026-02-06\"\n",
    "\n",
    "# Expected raw files\n",
    "EXPECTED_FILES = [\n",
    "    \"WADI_14days.csv\",\n",
    "    \"WADI_attackdata.csv\",\n",
    "    \"attack_description.xlsx\",\n",
    "    \"table_WADI.pdf\",\n",
    "]\n",
    "\n",
    "# Run ID\n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S_utc\")\n",
    "\n",
    "print(f\"Dataset:  {DATASET_NAME}\")\n",
    "print(f\"Source:   {DATA_SOURCE}\")\n",
    "print(f\"Run ID:   {RUN_ID}\")\n",
    "print(f\"Split:    normal operation → train | attack period → test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8db640-1ac9-480f-8fdd-5f6ea02d380f",
   "metadata": {},
   "source": [
    "# Stage 1 - Ingest: Acquire Raw Data  \n",
    "Verifies all expected raw files are present and documents their provenance.  \n",
    "The raw files are not modified - this stage is read-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d54a9-ec94-4626-bc85-f7a262d457c2",
   "metadata": {},
   "source": [
    "## 1.1 - Verify Raw Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc6c592-bc11-4ee7-98ca-38d1613e7335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required files present:\n",
      "\n",
      "  WADI_14days.csv                         777.5 MB\n",
      "  WADI_attackdata.csv                     111.2 MB\n",
      "  attack_description.xlsx                   0.0 MB\n",
      "  table_WADI.pdf                            0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Verify all expected raw files are present\n",
    "missing = [f for f in EXPECTED_FILES if not (RAW_DIR / f).exists()]\n",
    "\n",
    "if missing:\n",
    "    raise PipelineError(f\"Missing required files in {RAW_DIR}: {missing}\")\n",
    "\n",
    "print(\"All required files present:\\n\")\n",
    "for fname in EXPECTED_FILES:\n",
    "    fpath = RAW_DIR / fname\n",
    "    size_mb = fpath.stat().st_size / 1e6\n",
    "    print(f\"  {fname:<35s}  {size_mb:>8.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00dea7-f705-418e-8c5a-34f21985cf57",
   "metadata": {},
   "source": [
    "## 1.2 - Write Raw Metadata  \n",
    "Documents the source, download date, and file fingerprints for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0f0c3b-3f1c-4cb4-afaf-f3557bb1e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata written: work/wadi_A1/data/raw/metadata_raw_20260223_140105_utc.json\n"
     ]
    }
   ],
   "source": [
    "file_meta = {}\n",
    "for fname in EXPECTED_FILES:\n",
    "    fpath = RAW_DIR / fname\n",
    "    file_meta[fname] = {\n",
    "        \"size_bytes\": fpath.stat().st_size,\n",
    "        \"sha16\":      sha16(fpath.read_bytes().hex()),\n",
    "        \"path\":       str(fpath),\n",
    "    }\n",
    "\n",
    "metadata_raw = {\n",
    "    \"run_id\":        RUN_ID,\n",
    "    \"created_at_utc\": utc_now_iso(),\n",
    "    \"dataset_name\":  DATASET_NAME,\n",
    "    \"source\":        DATA_SOURCE,\n",
    "    \"source_url\":    DATASET_URL,\n",
    "    \"download_date\": DOWNLOAD_DATE,\n",
    "    \"files\":         file_meta,\n",
    "    \"notes\": [\n",
    "        \"Dataset requires registration with iTrust Labs.\",\n",
    "        \"Files manually downloaded and placed in RAW_DIR.\",\n",
    "        \"Normal operations: 14 days. Attack scenarios: 2 days.\",\n",
    "        \"Attack labels in WADI_attackdata.csv cover Oct 9–11 2017.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "meta_path = RAW_DIR / f\"metadata_raw_{RUN_ID}.json\"\n",
    "write_json(meta_path, metadata_raw)\n",
    "print(f\"Metadata written: {meta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e928d8-45e7-4ef6-820e-a73db8a12947",
   "metadata": {},
   "source": [
    "# Stage 2 - Stage: Parse and Normalize  \n",
    "* Loads raw CSVs\n",
    "* cleans column names\n",
    "* parses timestamps\n",
    "* combines normal and attack data into a single DataFrame\n",
    "* drops uninformative sensor columns\n",
    "* defines the SENSOR_COLS list\n",
    "* adds time features\n",
    "* assigns the stratified train/val/test split\n",
    "* writes the staged parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee642398-2d1a-4265-8a9a-863355edd8f1",
   "metadata": {},
   "source": [
    "## 2.1 - Define Staging Function  \n",
    "Cleans the raw WaDi CSV structure:  \n",
    "* Strips the windows path prefix from column names\n",
    "* Parses separate Date + Time columns into a single UTC timestamp\n",
    "* Drops the row-number column\n",
    "* Casts all sensor columns to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce403ba8-3b67-4434-8cbd-e30894821367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging function defined.\n"
     ]
    }
   ],
   "source": [
    "# Windows path prefix present on all sensor column names\n",
    "WADI_COL_PREFIX = \"\\\\\\\\WIN-25J4RO10SBF\\\\LOG_DATA\\\\SUTD_WADI\\\\LOG_DATA\\\\\"\n",
    "\n",
    "def stage_wadi_data(df_raw: pd.DataFrame, dataset_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse and normalize one raw WaDi CSV (normal or attack).\n",
    "    Returns a clean, typed DataFrame with UTC timestamps.\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # ── Strip Windows path prefix from column names ───────────────────────────\n",
    "    df.columns = [\n",
    "        c.replace(WADI_COL_PREFIX, \"\").strip()\n",
    "        if c.startswith(\"\\\\\\\\\") else c.strip()\n",
    "        for c in df.columns\n",
    "    ]\n",
    "\n",
    "    # ── Drop row-number column (unnamed or 'Row') ─────────────────────────────\n",
    "    drop_candidates = [c for c in df.columns if c.strip() in (\"\", \"Row\") or\n",
    "                       c.startswith(\"Unnamed\")]\n",
    "    df = df.drop(columns=drop_candidates, errors=\"ignore\")\n",
    "\n",
    "    # ── Parse timestamp ───────────────────────────────────────────────────────\n",
    "    # Normal file has 'Date' + 'Time', attack file has the same structure\n",
    "    df[\"timestamp\"] = pd.to_datetime(\n",
    "        df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str),\n",
    "        format=\"mixed\",\n",
    "        dayfirst=False,\n",
    "    ).dt.tz_localize(\"UTC\")\n",
    "\n",
    "    df = df.drop(columns=[\"Date\", \"Time\"], errors=\"ignore\")\n",
    "\n",
    "    # ── Add source identifier ─────────────────────────────────────────────────\n",
    "    df[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # ── Sort by timestamp ─────────────────────────────────────────────────────\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # ── Cast sensor columns to float32 ────────────────────────────────────────\n",
    "    meta_cols = {\"timestamp\", \"dataset_id\"}\n",
    "    sensor_cols = [c for c in df.columns if c not in meta_cols]\n",
    "    for col in sensor_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Staging function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7f29d-c64f-4e88-8cc5-ab210f981266",
   "metadata": {},
   "source": [
    "## 2.2 - Load and Stage Raw Data  \n",
    "Loads both CSV files and applies the staging function. The normal operations file has metadata rows at the top that must be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20debd8a-2e05-472c-855b-2d051cdcc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0: 'Created: 10/9/2017 6:05:57.359 PM Malay Peninsula Standard Time                       \\n'\n",
      "Line 1: 'Number of rows: 1.2096E+6\\n'\n",
      "Line 2: 'Interpolation interval: 1 seconds\\n'\n",
      "Line 3: '\\n'\n",
      "Line 4: 'Row,Date,Time,\\\\\\\\WIN-25J4RO10SBF\\\\LOG_DATA\\\\SUTD_WADI\\\\LOG_DATA\\\\1_AIT_001_PV,\\\\\\\\WIN-25J4RO10SBF\\\\LOG_DATA\\\\SUTD_WADI\\\\LOG_DATA\\\\1'\n",
      "Line 5: '1,9/25/2017,6:00:00.000 PM,171.155,0.619473,11.5759,504.645,0.318319,0.00115685,0,0,47.8911,1,1,1,1,1,1,1,1,2,1,2464.88,'\n",
      "Line 6: '2,9/25/2017,6:00:01.000 PM,171.155,0.619473,11.5759,504.645,0.318319,0.00115685,0,0,47.8911,1,1,1,1,1,1,1,1,2,1,2464.88,'\n",
      "Line 7: '3,9/25/2017,6:00:02.000 PM,171.155,0.619473,11.5759,504.645,0.318319,0.00115685,0,0,47.8911,1,1,1,1,1,1,1,1,2,1,2464.88,'\n",
      "Line 8: '4,9/25/2017,6:00:03.000 PM,171.155,0.607477,11.5725,504.673,0.318438,0.00120685,0,0,47.7503,1,1,1,1,1,1,1,1,2,1,2477.67,'\n",
      "Line 9: '5,9/25/2017,6:00:04.000 PM,171.155,0.607477,11.5725,504.673,0.318438,0.00120685,0,0,47.7503,1,1,1,1,1,1,1,1,2,1,2477.67,'\n"
     ]
    }
   ],
   "source": [
    "# Peek at the first 10 raw lines to understand the file structure\n",
    "with open(RAW_DIR / \"WADI_14days.csv\", \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(f\"Line {i}: {line[:120]!r}\")\n",
    "        if i >= 9:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0cfc893-f0f1-43dd-b8e2-1ca0c9e27999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading normal operations data...\n",
      "  Raw shape: (1209601, 130)\n",
      "\n",
      "Loading attack data...\n",
      "  Raw shape: (172801, 130)\n",
      "\n",
      "Staging normal data...\n",
      "  Staged shape: (1209601, 129)\n",
      "  Time range: 2017-09-25 18:00:00+00:00 → 2017-10-09 18:00:00+00:00\n",
      "\n",
      "Staging attack data...\n",
      "  Staged shape: (172801, 129)\n",
      "  Time range: 2017-10-09 18:00:00+00:00 → 2017-10-11 18:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# 1. Load normal operations CSV \n",
    "print(\"Loading normal operations data...\")\n",
    "df_raw_normal = pd.read_csv(RAW_DIR / \"WADI_14days.csv\", skiprows=4, header=0, low_memory=False)\n",
    "print(f\"  Raw shape: {df_raw_normal.shape}\")\n",
    "\n",
    "# 2. Load attack data CSV\n",
    "print(\"\\nLoading attack data...\")\n",
    "df_raw_attack = pd.read_csv(RAW_DIR / \"WADI_attackdata.csv\", low_memory=False)\n",
    "print(f\"  Raw shape: {df_raw_attack.shape}\")\n",
    "\n",
    "# 3. Stage both \n",
    "print(\"\\nStaging normal data...\")\n",
    "df_normal = stage_wadi_data(df_raw_normal, dataset_id=\"normal\")\n",
    "print(f\"  Staged shape: {df_normal.shape}\")\n",
    "print(f\"  Time range: {df_normal['timestamp'].min()} → {df_normal['timestamp'].max()}\")\n",
    "\n",
    "print(\"\\nStaging attack data...\")\n",
    "df_attack = stage_wadi_data(df_raw_attack, dataset_id=\"attack\")\n",
    "print(f\"  Staged shape: {df_attack.shape}\")\n",
    "print(f\"  Time range: {df_attack['timestamp'].min()} → {df_attack['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31345319-2b54-48f3-9e83-63b8675a1266",
   "metadata": {},
   "source": [
    "## 2.3 - Combine and Assign Labels  \n",
    "* Concatenates normal and attack DataFrames into a single dataset\n",
    "* Assigns the three-class label column: 0=normal, 1=attack.\n",
    "* Drops the dataset_id column. It directly encodes the label and would be data leakage if carried into the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5863e6f-0aa7-4f35-a716-266da950f3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (1382402, 129)\n",
      "Time range:     2017-09-25 18:00:00+00:00 → 2017-10-11 18:00:00+00:00\n",
      "\n",
      "Label counts:\n",
      "  Normal (0): 1,209,601\n",
      "  Attack (1):   172,801\n"
     ]
    }
   ],
   "source": [
    "# 1. Defragment both DataFrames before combining\n",
    "df_normal = df_normal.copy()\n",
    "df_attack = df_attack.copy()\n",
    "\n",
    "df_normal[\"label\"] = 0\n",
    "df_attack[\"label\"] = 1\n",
    "\n",
    "# 2. Combine \n",
    "df = pd.concat([df_normal, df_attack], ignore_index=True)\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# 3. Drop dataset_id — encodes the label, would be direct leakage \n",
    "df = df.drop(columns=[\"dataset_id\"])\n",
    "\n",
    "# 4. Cast label to int8 \n",
    "df[\"label\"] = df[\"label\"].astype(\"int8\")\n",
    "\n",
    "print(f\"Combined shape: {df.shape}\")\n",
    "print(f\"Time range:     {df['timestamp'].min()} → {df['timestamp'].max()}\")\n",
    "print(f\"\\nLabel counts:\")\n",
    "print(f\"  Normal (0): {(df['label']==0).sum():>9,}\")\n",
    "print(f\"  Attack (1): {(df['label']==1).sum():>9,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1296bff-a396-4f37-ab7b-5cee7a2be80a",
   "metadata": {},
   "source": [
    "## 2.4 - Drop Uninformative Sensor Columns  \n",
    "Identifies and removes sensor columns that carry no predictive signal:  \n",
    "* **100% NaN** - sensor was never recorded or permanently offline\n",
    "* **Constant value** - no variation across the entire dataset, zero discriminative power\n",
    "\n",
    "These columns are documented explicitly before removal so the decision is reproducible and citable. They will be excluded from SENSOR_COLS permanently.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55433bdb-e746-4e82-89d9-5770903a9518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% NaN columns (4):\n",
      "  2_LS_001_AL\n",
      "  2_LS_002_AL\n",
      "  2_P_001_STATUS\n",
      "  2_P_002_STATUS\n",
      "\n",
      "Constant-value columns (25):\n",
      "  1_LS_001_AL                          value=0.0\n",
      "  1_LS_002_AL                          value=0.0\n",
      "  1_P_002_STATUS                       value=1.0\n",
      "  1_P_004_STATUS                       value=1.0\n",
      "  2_MV_001_STATUS                      value=1.0\n",
      "  2_MV_002_STATUS                      value=1.0\n",
      "  2_MV_004_STATUS                      value=2.0\n",
      "  2_MV_005_STATUS                      value=2.0\n",
      "  2_MV_009_STATUS                      value=2.0\n",
      "  2_P_004_STATUS                       value=1.0\n",
      "  2_SV_101_STATUS                      value=1.0\n",
      "  2_SV_201_STATUS                      value=1.0\n",
      "  2_SV_301_STATUS                      value=1.0\n",
      "  2_SV_401_STATUS                      value=1.0\n",
      "  2_SV_501_STATUS                      value=1.0\n",
      "  2_SV_601_STATUS                      value=1.0\n",
      "  3_LS_001_AL                          value=1.0\n",
      "  3_MV_001_STATUS                      value=1.0\n",
      "  3_MV_002_STATUS                      value=1.0\n",
      "  3_MV_003_STATUS                      value=1.0\n",
      "  3_P_001_STATUS                       value=1.0\n",
      "  3_P_002_STATUS                       value=1.0\n",
      "  3_P_003_STATUS                       value=1.0\n",
      "  3_P_004_STATUS                       value=1.0\n",
      "  PLANT_START_STOP_LOG                 value=1.0\n",
      "\n",
      "Total to drop: 29\n"
     ]
    }
   ],
   "source": [
    "# Identify all sensor columns (everything except timestamp and label)\n",
    "meta_cols = {\"timestamp\", \"label\"}\n",
    "all_sensor_candidates = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "# Find 100% NaN columns \n",
    "null_counts = df[all_sensor_candidates].isnull().sum()\n",
    "fully_null = null_counts[null_counts == len(df)].index.tolist()\n",
    "\n",
    "# Find constant columns (zero variance)\n",
    "constant = [\n",
    "    c for c in all_sensor_candidates\n",
    "    if c not in fully_null and df[c].nunique(dropna=True) <= 1\n",
    "]\n",
    "\n",
    "print(f\"100% NaN columns ({len(fully_null)}):\")\n",
    "for c in fully_null:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "print(f\"\\nConstant-value columns ({len(constant)}):\")\n",
    "for c in constant:\n",
    "    val = df[c].dropna().unique()\n",
    "    print(f\"  {c:<35s}  value={val[0] if len(val) else 'NaN'}\")\n",
    "\n",
    "print(f\"\\nTotal to drop: {len(fully_null) + len(constant)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bb8ce7-95c0-497b-a4df-dff5302f9690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before drop: 129\n",
      "Columns dropped:     29\n",
      "Columns remaining:   100\n",
      "\n",
      "Remaining columns: timestamp, label + 98 sensor columns\n"
     ]
    }
   ],
   "source": [
    "# Drop uninformative columns \n",
    "cols_to_drop = fully_null + constant\n",
    "\n",
    "# Document what we're dropping before removing\n",
    "drop_log = {\n",
    "    \"fully_null\":  fully_null,\n",
    "    \"constant\":    constant,\n",
    "    \"total_dropped\": len(cols_to_drop),\n",
    "}\n",
    "\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "n_before = len(df.columns) + len(cols_to_drop)\n",
    "print(f\"Columns before drop: {n_before}\")\n",
    "print(f\"Columns dropped:     {len(cols_to_drop)}\")\n",
    "print(f\"Columns remaining:   {len(df.columns)}\")\n",
    "print(f\"\\nRemaining columns: timestamp, label + {len(df.columns) - 2} sensor columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee6217-eadd-4810-8789-04c22dc64080",
   "metadata": {},
   "source": [
    "## 2.5 - Define and Freeze SENSOR_COLS  \n",
    "Defines the canonical list of sensor columns used by all downstream modules.  \n",
    "Written to a reference JSON so fault injection and curation modules load the same column list without hardcoding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c470313a-76d7-4022-9d2d-3dc9332591e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENSOR_COLS count: 98\n",
      "\n",
      "First 10: ['1_AIT_001_PV', '1_AIT_002_PV', '1_AIT_003_PV', '1_AIT_004_PV', '1_AIT_005_PV', '1_FIT_001_PV', '1_LT_001_PV', '1_MV_001_STATUS', '1_MV_002_STATUS', '1_MV_003_STATUS']\n",
      "Last 10:  ['2B_AIT_004_PV', '3_AIT_001_PV', '3_AIT_002_PV', '3_AIT_003_PV', '3_AIT_004_PV', '3_AIT_005_PV', '3_FIT_001_PV', '3_LT_001_PV', 'LEAK_DIFF_PRESSURE', 'TOTAL_CONS_REQUIRED_FLOW']\n",
      "\n",
      "SENSOR_COLS written to: work/wadi_A1/data/reference/sensor_cols.json\n"
     ]
    }
   ],
   "source": [
    "# Define canonical sensor column list \n",
    "SENSOR_COLS = [c for c in df.columns if c not in {\"timestamp\", \"label\"}]\n",
    "\n",
    "print(f\"SENSOR_COLS count: {len(SENSOR_COLS)}\")\n",
    "print(f\"\\nFirst 10: {SENSOR_COLS[:10]}\")\n",
    "print(f\"Last 10:  {SENSOR_COLS[-10:]}\")\n",
    "\n",
    "# Write to reference JSON for downstream notebooks \n",
    "sensor_cols_path = REF_DIR / \"sensor_cols.json\"\n",
    "write_json(sensor_cols_path, {\n",
    "    \"run_id\":       RUN_ID,\n",
    "    \"dataset\":      DATASET_NAME,\n",
    "    \"sensor_cols\":  SENSOR_COLS,\n",
    "    \"n_sensors\":    len(SENSOR_COLS),\n",
    "    \"dropped_fully_null\": drop_log[\"fully_null\"],\n",
    "    \"dropped_constant\":   drop_log[\"constant\"],\n",
    "    \"notes\": [\n",
    "        \"100% NaN columns dropped — sensor never recorded or permanently offline.\",\n",
    "        \"Constant-value columns dropped — no variation, zero discriminative power.\",\n",
    "        \"This list is the canonical feature set for all downstream notebooks.\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"\\nSENSOR_COLS written to: {sensor_cols_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12998fce-d35d-4c59-bc3a-d22e46257f4b",
   "metadata": {},
   "source": [
    "## 2.6 - Add Time Features  \n",
    "Adds two time-derived columns:  \n",
    "* `observation_day` - calendar date, used for canary checks and daily row counts\n",
    "* `seconds_since_start` - ordinal position from dataset start, captures startup vs steady-state behavior without encoding clock time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a11b0b52-d290-4411-b8bc-4843f9ded024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_day range:     2017-09-25 → 2017-10-11\n",
      "seconds_since_start range: 0 → 1382400\n",
      "  (16.0 days)\n"
     ]
    }
   ],
   "source": [
    "# Add time features \n",
    "df[\"observation_day\"] = df[\"timestamp\"].dt.date\n",
    "\n",
    "t0 = df[\"timestamp\"].min()\n",
    "df[\"seconds_since_start\"] = (\n",
    "    (df[\"timestamp\"] - t0).dt.total_seconds().astype(\"float32\")\n",
    ")\n",
    "\n",
    "print(f\"observation_day range:     {df['observation_day'].min()} → {df['observation_day'].max()}\")\n",
    "print(f\"seconds_since_start range: {df['seconds_since_start'].min():.0f} → {df['seconds_since_start'].max():.0f}\")\n",
    "print(f\"  ({df['seconds_since_start'].max() / 86400:.1f} days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5711d-c40e-4d5f-b106-9be32d2b8cef",
   "metadata": {},
   "source": [
    "## 2.7 - Train/Test Split\n",
    "\n",
    "Normal operation rows (label=0) are assigned to train.\n",
    "Attack period rows (label=1) are assigned to test.\n",
    "\n",
    "This matches the standard WaDi evaluation protocol used in the literature:\n",
    "train on normal behavior, evaluate on the attack period. It also avoids\n",
    "the temporal distribution shift that arises from splitting within the\n",
    "normal operation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6784e5-9b04-486a-b3b4-76ae5d6cb73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split distribution:\n",
      "  train : 1,209,601\n",
      "  test  :   172,801\n",
      "\n",
      "Split distribution by class:\n",
      "\n",
      "  normal (label=0):\n",
      "    train : 1,209,601\n",
      "\n",
      "  attack (label=1):\n",
      "    test  :   172,801\n"
     ]
    }
   ],
   "source": [
    "# Assign split: normal -> train, attack -> test\n",
    "df[\"split\"] = df[\"label\"].map({0: \"train\", 1: \"test\"})\n",
    "\n",
    "print(\"Split distribution:\")\n",
    "for split in [\"train\", \"test\"]:\n",
    "    n = (df[\"split\"] == split).sum()\n",
    "    print(f\"  {split:<6}: {n:>9,}\")\n",
    "\n",
    "print(\"\\nSplit distribution by class:\")\n",
    "for label_val, label_name in [(0, \"normal\"), (1, \"attack\")]:\n",
    "    counts = df[df[\"label\"] == label_val][\"split\"].value_counts()\n",
    "    print(f\"\\n  {label_name} (label={label_val}):\")\n",
    "    for split, n in counts.items():\n",
    "        print(f\"    {split:<6}: {n:>9,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4188fd-cf80-439e-bdfb-adf82e7e8288",
   "metadata": {},
   "source": [
    "## 2.8 - Validate Split  \n",
    "Confirms the split assignment is correct before saving.  \n",
    "Checks that all classes appear in all splits and that temporal order is preserved within each class-split combination  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef10543-ef8f-4aed-987a-7de875125904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split validation PASSED — all checks clean.\n",
      "\n",
      "Temporal boundary:\n",
      "  Train: 2017-09-25 → 2017-10-09\n",
      "  Test:  2017-10-09 → 2017-10-11\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "\n",
    "# Check all rows assigned\n",
    "n_unassigned = df[\"split\"].isna().sum()\n",
    "if n_unassigned > 0:\n",
    "    errors.append(f\"  {n_unassigned} rows have no split assignment\")\n",
    "\n",
    "# Check expected splits exist\n",
    "for expected_split in [\"train\", \"test\"]:\n",
    "    if expected_split not in df[\"split\"].values:\n",
    "        errors.append(f\"  Split '{expected_split}' is missing entirely\")\n",
    "\n",
    "# Check normal rows only in train\n",
    "normal_in_test = ((df[\"label\"] == 0) & (df[\"split\"] == \"test\")).sum()\n",
    "if normal_in_test > 0:\n",
    "    errors.append(f\"  {normal_in_test} normal rows found in test split\")\n",
    "\n",
    "# Check attack rows only in test\n",
    "attack_in_train = ((df[\"label\"] == 1) & (df[\"split\"] == \"train\")).sum()\n",
    "if attack_in_train > 0:\n",
    "    errors.append(f\"  {attack_in_train} attack rows found in train split\")\n",
    "\n",
    "# Check temporal order: all train timestamps precede test timestamps\n",
    "train_max = df[df[\"split\"] == \"train\"][\"timestamp\"].max()\n",
    "test_min  = df[df[\"split\"] == \"test\"][\"timestamp\"].min()\n",
    "if train_max > test_min:\n",
    "    errors.append(f\"  Train/test temporal boundary violated: \"\n",
    "                  f\"train_max={train_max}, test_min={test_min}\")\n",
    "\n",
    "# Report\n",
    "if errors:\n",
    "    print(\"VALIDATION FAILED:\")\n",
    "    for e in errors:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"Split validation PASSED — all checks clean.\")\n",
    "\n",
    "print(f\"\\nTemporal boundary:\")\n",
    "print(f\"  Train: {df[df['split']=='train']['timestamp'].min().date()} → \"\n",
    "      f\"{df[df['split']=='train']['timestamp'].max().date()}\")\n",
    "print(f\"  Test:  {df[df['split']=='test']['timestamp'].min().date()} → \"\n",
    "      f\"{df[df['split']=='test']['timestamp'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcf2de-bee3-4861-bd63-fb4e92ad7a94",
   "metadata": {},
   "source": [
    "## 2.9 - Write Staged Parquet and Run Log  \n",
    "Writes the staged dataset to disk and documents the pipeline run.  \n",
    "The staged parquet is the input to the fault injection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acdd064e-96b8-48da-a661-3a433316fef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged parquet written: work/wadi_A1/data/staged/wadi_staged_20260223_140105_utc.parquet\n",
      "Size:                   100.2 MB\n",
      "Shape:                  (1382402, 103)\n",
      "\n",
      "Run log written: work/wadi_A1/data/reference/pipeline_runs/run_20260223_140105_utc.json\n"
     ]
    }
   ],
   "source": [
    "# Final column ordering \n",
    "ordered_cols = [\"timestamp\", \"observation_day\", \"seconds_since_start\",\n",
    "                \"split\", \"label\"] + SENSOR_COLS\n",
    "df_staged = df[ordered_cols].copy()\n",
    "\n",
    "# Write staged parquet \n",
    "staged_path = STAGED_DIR / f\"wadi_staged_{RUN_ID}.parquet\"\n",
    "df_staged.to_parquet(staged_path, index=False)\n",
    "\n",
    "size_mb = staged_path.stat().st_size / 1e6\n",
    "print(f\"Staged parquet written: {staged_path}\")\n",
    "print(f\"Size:                   {size_mb:.1f} MB\")\n",
    "print(f\"Shape:                  {df_staged.shape}\")\n",
    "\n",
    "# Write run log \n",
    "run_log = {\n",
    "    \"run_id\":           RUN_ID,\n",
    "    \"created_at_utc\":   utc_now_iso(),\n",
    "    \"stage\":            \"Notebook 1 — Ingest and Stage\",\n",
    "    \"dataset\":          DATASET_NAME,\n",
    "    \"source\":           DATA_SOURCE,\n",
    "    \"download_date\":    DOWNLOAD_DATE,\n",
    "    \"inputs\": {\n",
    "        \"normal_csv\":   str(RAW_DIR / \"WADI_14days.csv\"),\n",
    "        \"attack_csv\":   str(RAW_DIR / \"WADI_attackdata.csv\"),\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"staged_parquet\":  str(staged_path),\n",
    "        \"sensor_cols_ref\": str(sensor_cols_path),\n",
    "    },\n",
    "    \"dataset_summary\": {\n",
    "        \"total_rows\":    len(df_staged),\n",
    "        \"total_cols\":    len(df_staged.columns),\n",
    "        \"n_sensor_cols\": len(SENSOR_COLS),\n",
    "        \"label_counts\":  df_staged[\"label\"].value_counts().sort_index().to_dict(),\n",
    "        \"split_counts\":  df_staged[\"split\"].value_counts().sort_index().to_dict(),\n",
    "        \"time_start\":    str(df_staged[\"timestamp\"].min()),\n",
    "        \"time_end\":      str(df_staged[\"timestamp\"].max()),\n",
    "    },\n",
    "    \"columns_dropped\": {\n",
    "        \"fully_null\":  drop_log[\"fully_null\"],\n",
    "        \"constant\":    drop_log[\"constant\"],\n",
    "        \"total\":       drop_log[\"total_dropped\"],\n",
    "    },\n",
    "    \"split_strategy\": \"normal_operation=train, attack_period=test\",\n",
    "    \n",
    "    \"notes\": [\n",
    "        \"dataset_id dropped — directly encodes label, would be leakage.\",\n",
    "        \"observation_hour, dayofweek, is_weekend excluded — no physical relationship to faults or attacks.\",\n",
    "        \"Split follows standard WaDi protocol: normal operation → train, attack period → test.\",\n",
    "        \"Matches literature convention for direct comparability with prior WaDi work.\",\n",
    "        \"Fault injection notebook reads this parquet and injects label=2 rows into train and test splits.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "run_log_path = RUN_DIR / f\"run_{RUN_ID}.json\"\n",
    "write_json(run_log_path, run_log)\n",
    "print(f\"\\nRun log written: {run_log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ebd5aa-6fc8-4c90-bde4-f128fa462eaf",
   "metadata": {},
   "source": [
    "# Stage 3 - Pipeline Reflection\n",
    "Documents key decisions, assumptions, and risks from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eee14da-cc68-462c-a5b5-a5dfb4fd9307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Reflection\n",
      "============================================================\n",
      "\n",
      "[Row definition]\n",
      "  Each row represents one second of sensor readings from the WaDi water distribution testbed. Label 0=normal operation, 1=cyber attack (original WaDi labels). Label 2=injected sensor fault will be added by the fault injection notebook.\n",
      "\n",
      "[Columns dropped]\n",
      "  29 of 128 sensor columns removed before defining SENSOR_COLS: 4 columns were 100% NaN across the entire dataset (sensors never recorded), and 25 were constant-valued (no variation, zero discriminative power). All are documented in the run log and sensor_cols.json.\n",
      "\n",
      "[Time features]\n",
      "  Only observation_day and seconds_since_start are retained. Hour-of-day, day-of-week, and weekend flags were deliberately excluded — they have no physical relationship to sensor faults or cyber attacks in an ICS environment and would add spurious signal.\n",
      "\n",
      "[dataset_id dropped]\n",
      "  The dataset_id column (normal/attack) directly encodes the label and was dropped immediately after label assignment. Retaining it would be direct label leakage into the feature matrix.\n",
      "\n",
      "[Train/test split]\n",
      "  Normal operation rows (label=0) assigned to train; attack period rows (label=1) assigned to test. This follows the standard WaDi evaluation protocol used in the ICS anomaly detection literature and avoids temporal distribution shift. Train covers 2017-09-25 to 2017-10-09 (14 days); test covers 2017-10-09 to 2017-10-11 (2 days). No validation split is used at this stage — cross-validation is handled in the modeling notebook.\n",
      "\n",
      "[Known limitation]\n",
      "  Attack train/val/test rows all come from the same two-day window (Oct 9–11). Train and test attacks represent similar system conditions, which may slightly inflate attack detection performance. This is documented as a limitation of the WaDi dataset structure, not of the methodology.\n",
      "\n",
      "[Leakage risks]\n",
      "  Normalization stats must be fit on train split only — deferred to feature engineering notebook. Rolling window features must use only past observations — deferred to feature engineering notebook. Fault injection must not use information from attack labels — enforced in injection notebook.\n",
      "\n",
      "[Next step]\n",
      "  Fault injection notebook reads wadi_staged_*.parquet and injects synthetic sensor failures into normal rows within the train split only, producing a three-class dataset with label 2=fault. The curate/validate notebook then picks up from the injected parquet.\n"
     ]
    }
   ],
   "source": [
    "reflection = [\n",
    "    (\"Row definition\",\n",
    "     \"Each row represents one second of sensor readings from the WaDi water \"\n",
    "     \"distribution testbed. Label 0=normal operation, 1=cyber attack (original \"\n",
    "     \"WaDi labels). Label 2=injected sensor fault will be added by the fault \"\n",
    "     \"injection notebook.\"),\n",
    "\n",
    "    (\"Columns dropped\",\n",
    "     \"29 of 128 sensor columns removed before defining SENSOR_COLS: 4 columns \"\n",
    "     \"were 100% NaN across the entire dataset (sensors never recorded), and 25 \"\n",
    "     \"were constant-valued (no variation, zero discriminative power). All are \"\n",
    "     \"documented in the run log and sensor_cols.json.\"),\n",
    "\n",
    "    (\"Time features\",\n",
    "     \"Only observation_day and seconds_since_start are retained. Hour-of-day, \"\n",
    "     \"day-of-week, and weekend flags were deliberately excluded — they have no \"\n",
    "     \"physical relationship to sensor faults or cyber attacks in an ICS \"\n",
    "     \"environment and would add spurious signal.\"),\n",
    "\n",
    "    (\"dataset_id dropped\",\n",
    "     \"The dataset_id column (normal/attack) directly encodes the label and was \"\n",
    "     \"dropped immediately after label assignment. Retaining it would be direct \"\n",
    "     \"label leakage into the feature matrix.\"),\n",
    "\n",
    "    (\"Train/test split\",\n",
    "     \"Normal operation rows (label=0) assigned to train; attack period rows \"\n",
    "     \"(label=1) assigned to test. This follows the standard WaDi evaluation \"\n",
    "     \"protocol used in the ICS anomaly detection literature and avoids temporal \"\n",
    "     \"distribution shift. Train covers 2017-09-25 to 2017-10-09 (14 days); \"\n",
    "     \"test covers 2017-10-09 to 2017-10-11 (2 days). No validation split is \"\n",
    "     \"used at this stage — cross-validation is handled in the modeling notebook.\"),\n",
    "\n",
    "    (\"Known limitation\",\n",
    "     \"Attack train/val/test rows all come from the same two-day window (Oct 9–11). \"\n",
    "     \"Train and test attacks represent similar system conditions, which may \"\n",
    "     \"slightly inflate attack detection performance. This is documented as a \"\n",
    "     \"limitation of the WaDi dataset structure, not of the methodology.\"),\n",
    "\n",
    "    (\"Leakage risks\",\n",
    "     \"Normalization stats must be fit on train split only — deferred to feature \"\n",
    "     \"engineering notebook. Rolling window features must use only past \"\n",
    "     \"observations — deferred to feature engineering notebook. Fault injection \"\n",
    "     \"must not use information from attack labels — enforced in injection notebook.\"),\n",
    "\n",
    "    (\"Next step\",\n",
    "     \"Fault injection notebook reads wadi_staged_*.parquet and injects synthetic \"\n",
    "     \"sensor failures into normal rows within the train split only, producing a \"\n",
    "     \"three-class dataset with label 2=fault. The curate/validate notebook then \"\n",
    "     \"picks up from the injected parquet.\"),\n",
    "]\n",
    "\n",
    "print(\"Pipeline Reflection\")\n",
    "print(\"=\" * 60)\n",
    "for title, content in reflection:\n",
    "    print(f\"\\n[{title}]\")\n",
    "    print(f\"  {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befabd1a-1db6-4240-82c8-35a072bc06ce",
   "metadata": {},
   "source": [
    "# This Continues with WaDi A2 Pipeline Notebook 2 - Fault Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56561152-6821-4d4c-9e66-af5d262ade56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
