{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc6ef1e",
   "metadata": {},
   "source": [
    "# Homework 2: NOAA Buoy Data Pipeline (NDBC)\n",
    "\n",
    "You will build a repeatable pipeline that ingests **real sensor data**, handles missingness + sentinel values,\n",
    "creates an analysis-ready table, and writes a validation report you could run every day.\n",
    "\n",
    "## Dataset Overview: NOAA NDBC Buoy Observations\n",
    "\n",
    "This project uses data from the **NOAA National Data Buoy Center (NDBC)**, which operates a network of ocean buoys and coastal stations that continuously measure atmospheric and oceanographic conditions.\n",
    "\n",
    "Each buoy is a **physical sensor platform** deployed at a fixed geographic location. It records observations at regular time intervals (often hourly), transmitting them to NOAA for operational use in weather forecasting, marine safety, and climate research.\n",
    "\n",
    "---\n",
    "\n",
    "### What a single row represents\n",
    "\n",
    "In this dataset:\n",
    "\n",
    "> **Each row represents one sensor observation at a specific buoy at a specific UTC timestamp.**\n",
    "\n",
    "This makes the data:\n",
    "- **Time series** (ordered in time)\n",
    "- **Stateful** (conditions at a moment, not events)\n",
    "- **Naturally indexed by time**\n",
    "\n",
    "There is no “target variable” yet — this dataset is about *measurement*, not decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### Core variables (typical)\n",
    "\n",
    "Not every buoy reports every variable, but common fields include:\n",
    "\n",
    "- **Wind**\n",
    "  - `WDIR` — wind direction (degrees)\n",
    "  - `WSPD` — wind speed (m/s)\n",
    "  - `GST` — wind gust (m/s)\n",
    "- **Waves**\n",
    "  - `WVHT` — significant wave height (m)\n",
    "  - `DPD` — dominant wave period (s)\n",
    "  - `APD` — average wave period (s)\n",
    "- **Atmosphere**\n",
    "  - `PRES` — sea-level pressure (hPa)\n",
    "  - `ATMP` — air temperature (°C)\n",
    "- **Ocean**\n",
    "  - `WTMP` — water temperature (°C)\n",
    "\n",
    "Missing values are common and usually reflect **sensor downtime**, **transmission issues**, or **environmental constraints**, not data entry errors.\n",
    "\n",
    "---\n",
    "\n",
    "### Why this dataset is realistic (and messy)\n",
    "\n",
    "This is **real operational sensor data**, not a curated research dataset. As a result:\n",
    "\n",
    "- Missing values are encoded as sentinels (e.g. `MM`)\n",
    "- Sensors may fail temporarily or permanently\n",
    "- Some variables appear or disappear over time\n",
    "- Units and ranges must be interpreted using domain knowledge\n",
    "- The dataset contains a rolling time window, not full history\n",
    "\n",
    "These characteristics make the dataset ideal for practicing **data ingestion, validation, and pipeline design**.\n",
    "\n",
    "---\n",
    "\n",
    "### Important constraint: rolling history\n",
    "\n",
    "The data used here comes from NOAA’s `realtime2` endpoint, which provides a **rolling window of recent observations** (typically ~30–45 days).\n",
    "\n",
    "This means:\n",
    "- You are *not* requesting a specific date range\n",
    "- Older observations are continuously overwritten\n",
    "- Historical backfills require a different NOAA endpoint\n",
    "\n",
    "This is intentional and mirrors how real production systems separate **realtime feeds** from **historical archives**.\n",
    "\n",
    "---\n",
    "\n",
    "The goal is not to “clean it perfectly,” but to **build trust in the parts you use** — and to document the assumptions you make along the way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9fa08-f79e-45e6-8c1d-c3b0c5c3c7fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## What you will produce\n",
    "\n",
    "Artifacts (under a project folder):\n",
    "\n",
    "- `data/raw/` — raw station snapshot(s) + metadata (station id, URL, timestamp)\n",
    "- `data/staged/` — parsed/normalized table (typed, missingness normalized)\n",
    "- `data/warehouse/` — curated table (Parquet; optionally partitioned by day)\n",
    "- `data/reference/validation_report.json` — contracts + anomaly rates + canaries\n",
    "- `data/reference/pipeline_runs/` — run logs for reproducibility\n",
    "\n",
    "> Principle: In sensor data, “cleaning” is mostly about **assumptions** (units, ranges, sentinel values)\n",
    "> and **guardrails** (contracts + anomaly flags), not deleting rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe937e3",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "Create a project folder somewhere, for example at:\n",
    "\n",
    "`~/work/homework_2_noaa/`\n",
    "\n",
    "Create the five directories listed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bf36290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: ../work/HW2_NOAA\n",
      "Raw: ../work/HW2_NOAA/data/raw\n",
      "Staged: ../work/HW2_NOAA/data/staged\n",
      "Warehouse: ../work/HW2_NOAA/data/warehouse\n",
      "Reference: ../work/HW2_NOAA/data/reference\n",
      "Runs: ../work/HW2_NOAA/data/reference/pipeline_runs\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "WORK_DIR = Path(\"../work\")\n",
    "PROJECT_DIR = WORK_DIR / \"HW2_NOAA\"\n",
    "\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGED_DIR = DATA_DIR / \"staged\"\n",
    "WH_DIR = DATA_DIR / \"warehouse\"\n",
    "REF_DIR = DATA_DIR / \"reference\"\n",
    "RUN_DIR = REF_DIR / \"pipeline_runs\"\n",
    "\n",
    "for p in [RAW_DIR, STAGED_DIR, WH_DIR, REF_DIR, RUN_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project:\", PROJECT_DIR)\n",
    "print(\"Raw:\", RAW_DIR)\n",
    "print(\"Staged:\", STAGED_DIR)\n",
    "print(\"Warehouse:\", WH_DIR)\n",
    "print(\"Reference:\", REF_DIR)\n",
    "print(\"Runs:\", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ab41a",
   "metadata": {},
   "source": [
    "### Helper utilities\n",
    "\n",
    "Create helper utilies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acf3edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready.\n"
     ]
    }
   ],
   "source": [
    "class PipelineError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def sha16(x: str) -> str:\n",
    "    return hashlib.sha256(x.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def write_txt(path: Path, content: str) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(content)\n",
    "\n",
    "def write_json(path: Path, obj: dict) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, default=str))\n",
    "\n",
    "def read_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "def require_columns(df: pd.DataFrame, cols: list[str], context: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise PipelineError(f\"[{context}] Missing required columns: {missing}\")\n",
    "\n",
    "def require_unique(df: pd.DataFrame, key: str, context: str) -> None:\n",
    "    if key not in df.columns:\n",
    "        raise PipelineError(f\"[{context}] Missing key column '{key}'\")\n",
    "    dupes = int(df[key].duplicated().sum())\n",
    "    if dupes:\n",
    "        raise PipelineError(f\"[{context}] Key '{key}' has {dupes} duplicates\")\n",
    "\n",
    "print(\"Helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c1287",
   "metadata": {},
   "source": [
    "#### 1) Ingest: download latest buoy observations\n",
    "\n",
    "NOAA NDBC provides station “realtime2” text files, e.g.\n",
    "\n",
    "- `https://www.ndbc.noaa.gov/data/realtime2/44103.txt`\n",
    "\n",
    "These files are human-readable but still messy:\n",
    "- header lines starting with `#`\n",
    "- missing values as sentinel strings like `MM`\n",
    "- sometimes extra columns depending on station/sensor\n",
    "\n",
    "**Station 44013 — Boston 16 NM East of Boston, MA** \n",
    "\n",
    "We will use data from buoy station `44013` in this homework for these reasons:\n",
    "- Location in the North Atlantic, a region with rich weather variability (storms, seasonal shifts).\n",
    "- Long historical coverage (data available back into the 1980s/1990s).\n",
    "- Strong mix of variables: wind, wave height, pressures, temperatures, etc.\n",
    "- Very useful for seasonality, trend analysis, anomaly detection, and combining meteorological + oceanographic features.\n",
    "- This station is especially popular for regional marine research and forecasting, so its data patterns can be both interesting and instructive for data science exercises.\n",
    "\n",
    "Look at the station information [here](https://www.ndbc.noaa.gov/station_page.php?station=44013).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae1c7c",
   "metadata": {},
   "source": [
    "✅ **Exercise 1.1 — fetch raw data and write snapshot**\n",
    "\n",
    "Download the file, then write:\n",
    "\n",
    "- raw text: `data/raw/ndbc_<station>_<runid>.txt`\n",
    "- raw metadata: `data/raw/ndbc_meta_<station>_<runid>.json`\n",
    "\n",
    "**Hint:** use `requests.get(url).text` and save as UTF-8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36b0929d-8017-498e-b2af-b706369586a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station: 44013\n",
      "URL: https://www.ndbc.noaa.gov/data/realtime2/44013.txt\n",
      "run_id: 20260215_220552_utc\n",
      "Wrote: ../work/HW2_NOAA/data/raw/nbdc_44013_run_20260215_220552_utc.txt\n",
      "Wrote: ../work/HW2_NOAA/data/raw/nbdc_44013_meta_20260215_220552_utc.json\n",
      "#YY  MM DD hh mm WDIR WSPD GST  WVHT   DPD   APD MWD   PRES  ATMP  WTMP  DEWP  VIS PTDY  TIDE\n",
      "#yr  mo dy hr mn degT m/s  m/s     m   sec   sec degT   hPa  degC  degC  degC  nmi  hPa    ft\n",
      "2026 02 15 21 30  90  3.0  4.0    MM    MM    MM  MM 1021.9  -0.9   3.0 -10.8   MM   MM    MM\n",
      "2026 02 15 21 20  80  2.0  4.0   0.8     6   4.6  35 1021.7  -0.9   3.0 -10.7   MM   MM    MM\n",
      "2026 02 15 21 10  80  3.0  4.0    MM    MM    MM  MM 1021.6  -0.9   3.0 -10.7   MM   MM    MM\n",
      "2026 02 15 21 00  80  3.0  4.0    MM    MM    MM  MM 1021.4  -1.0   3.0 -10.6   MM +0.9    MM\n",
      "2026 02 15 20 50  70  3.0  4.0   0.8     6   4.5  39 1021.2  -0.9   3.0 -10.3   MM   MM    MM\n",
      "2026 02 15 20 40  80  3.0  4.0    MM    MM    MM  MM 1021.1  -1.0   3.0 -10.3   MM   MM    MM\n",
      "2026 02 15 20 30  60  3.0  4.0    MM    MM    MM  MM 1021.0  -1.1   3.0 -10.6   MM   MM    MM\n",
      "2026 02 15 20 20  60  3.0  5.0   0.9     5   4.4  32 1021.0  -1.2   3.0 -10.6   MM   MM    MM\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Setup constants\n",
    "STATION_ID = \"44013\"  \n",
    "NOAA_URL = f\"https://www.ndbc.noaa.gov/data/realtime2/{STATION_ID}.txt\"\n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S_utc\")\n",
    "\n",
    "print(f\"Station: {STATION_ID}\")\n",
    "print(f\"URL: {NOAA_URL}\")\n",
    "print(f\"run_id: {RUN_ID}\")\n",
    "\n",
    "# Retreive dataset and save\n",
    "raw_data_path = RAW_DIR / f\"nbdc_{STATION_ID}_run_{RUN_ID}.txt\"\n",
    "raw_data = requests.get(NOAA_URL).text\n",
    "write_txt(raw_data_path ,raw_data)\n",
    "print(f\"Wrote: {raw_data_path}\")\n",
    "\n",
    "# Create metadata and save\n",
    "raw_metadata = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"generated_at_utc\": utc_now_iso(),\n",
    "    \"query\": {\n",
    "        \"url\": NOAA_URL,\n",
    "        \"station_id\": STATION_ID\n",
    "    },\n",
    "    \"n_chars\": len(raw_data),\n",
    "    \"source\": \"National Data Buoy Center (NOAA)\"\n",
    "}\n",
    "raw_meta_path = RAW_DIR / f\"nbdc_{STATION_ID}_meta_{RUN_ID}.json\"\n",
    "write_json(raw_meta_path, raw_metadata)\n",
    "print(f\"Wrote: {raw_meta_path}\")\n",
    "\n",
    "# Print first few rows of raw data\n",
    "print('\\n'.join(raw_data.splitlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474c6ba",
   "metadata": {},
   "source": [
    "## 2) Stage: parse + normalize missingness + types\n",
    "\n",
    "NDBC realtime2 files have:\n",
    "- one header line naming columns (after `#`)\n",
    "- data rows with whitespace-separated values\n",
    "\n",
    "Typical columns include:\n",
    "- `YY MM DD hh mm` (timestamp components, UTC)\n",
    "- `WDIR` wind direction (deg)\n",
    "- `WSPD` wind speed (m/s)\n",
    "- `GST` gust (m/s)\n",
    "- `WVHT` wave height (m)\n",
    "- `DPD` dominant period (s)\n",
    "- `APD` average period (s)\n",
    "- `PRES` pressure (hPa)\n",
    "- `ATMP` air temp (C)\n",
    "- `WTMP` water temp (C)\n",
    "\n",
    "But not every station has every column.\n",
    "\n",
    "✅ **Exercise 2.1 — parse the raw file into a DataFrame**\n",
    "\n",
    "Implement the function `read_ndbc_txt(path)` that:\n",
    "- reads the file\n",
    "- returns a DataFrame\n",
    "\n",
    "**Hints:**\n",
    "- Many files have a commented header and sometimes a units line.\n",
    "- A robust approach:\n",
    "  - Find the first **non-#** line (header)\n",
    "  - If the next line looks like units (letters), skip it\n",
    "  - Parse remaining lines with `delim_whitespace=True` or `sep=r\"\\s+\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3763773b-d6be-44f3-99af-6efb8d714b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique(cols):\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        if c in seen:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}_{seen[c]}\")\n",
    "        else:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def read_ndbc_txt(path: Path) -> pd.DataFrame:\n",
    "    # Find header = first line that starts with '#'\n",
    "    header_line = None\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"):\n",
    "                header_line = line\n",
    "                break\n",
    "\n",
    "    if header_line is None:\n",
    "        raise ValueError(\"No header line found (no line starts with '#').\")\n",
    "\n",
    "    columns = make_unique(header_line.lstrip(\"#\").split())\n",
    "\n",
    "    # Read data: skip any commented lines (header + units) automatically\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=r\"\\s+\",\n",
    "        comment=\"#\",\n",
    "        names=columns,\n",
    "        header=None,\n",
    "        na_values=[\"MM\", \"9999\", \"999\", \"99\"],\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffaf262e-7b2a-40f2-a7f7-dacef02238db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw flattened shape: (6563, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YY</th>\n",
       "      <th>MM</th>\n",
       "      <th>DD</th>\n",
       "      <th>hh</th>\n",
       "      <th>mm</th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>PTDY</th>\n",
       "      <th>TIDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.9</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1021.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.6</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1021.2</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1020.9</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1020.7</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     YY  MM  DD  hh  mm  WDIR  WSPD  GST  WVHT  DPD  APD   MWD    PRES  ATMP  \\\n",
       "0  2026   2  15  21  30  90.0   3.0  4.0   NaN  NaN  NaN   NaN  1021.9  -0.9   \n",
       "1  2026   2  15  21  20  80.0   2.0  4.0   0.8  6.0  4.6  35.0  1021.7  -0.9   \n",
       "2  2026   2  15  21  10  80.0   3.0  4.0   NaN  NaN  NaN   NaN  1021.6  -0.9   \n",
       "3  2026   2  15  21   0  80.0   3.0  4.0   NaN  NaN  NaN   NaN  1021.4  -1.0   \n",
       "4  2026   2  15  20  50  70.0   3.0  4.0   0.8  6.0  4.5  39.0  1021.2  -0.9   \n",
       "5  2026   2  15  20  40  80.0   3.0  4.0   NaN  NaN  NaN   NaN  1021.1  -1.0   \n",
       "6  2026   2  15  20  30  60.0   3.0  4.0   NaN  NaN  NaN   NaN  1021.0  -1.1   \n",
       "7  2026   2  15  20  20  60.0   3.0  5.0   0.9  5.0  4.4  32.0  1021.0  -1.2   \n",
       "8  2026   2  15  20  10  60.0   3.0  4.0   NaN  NaN  NaN   NaN  1020.9  -1.2   \n",
       "9  2026   2  15  20   0  40.0   4.0  6.0   NaN  NaN  NaN   NaN  1020.7  -1.2   \n",
       "\n",
       "   WTMP  DEWP  VIS  PTDY  TIDE  \n",
       "0   3.0 -10.8  NaN   NaN   NaN  \n",
       "1   3.0 -10.7  NaN   NaN   NaN  \n",
       "2   3.0 -10.7  NaN   NaN   NaN  \n",
       "3   3.0 -10.6  NaN   0.9   NaN  \n",
       "4   3.0 -10.3  NaN   NaN   NaN  \n",
       "5   3.0 -10.3  NaN   NaN   NaN  \n",
       "6   3.0 -10.6  NaN   NaN   NaN  \n",
       "7   3.0 -10.6  NaN   NaN   NaN  \n",
       "8   3.0 -10.4  NaN   NaN   NaN  \n",
       "9   3.0 -10.5  NaN   0.0   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw = read_ndbc_txt(raw_data_path)\n",
    "print(f\"Raw flattened shape: {df_raw.shape}\")\n",
    "display(df_raw.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a202f",
   "metadata": {},
   "source": [
    "✅ **Exercise 2.2 — construct a UTC timestamp + normalize missing values**\n",
    "\n",
    "Create a staged table that includes:\n",
    "\n",
    "- `station_id`\n",
    "- `time_utc` as timezone-aware datetime\n",
    "- numeric sensor fields coerced to numeric\n",
    "- missing values: `MM` becomes NaN (via numeric coercion)\n",
    "\n",
    "**Hints:**\n",
    "- Timestamp columns can be `YY MM DD hh mm` or sometimes `YYYY MM DD hh mm`.\n",
    "- The month column is usually `MM` and minute is `mm` (case matters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "840dab72-cb2c-4eee-91b9-10a0b63f43fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging function ready\n"
     ]
    }
   ],
   "source": [
    "def stage_ndbc_data(df_raw: pd.DataFrame, station_id: str) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # Station ID\n",
    "    df['station_id'] = station_id\n",
    "    \n",
    "    # Identify year column\n",
    "    if 'YYYY' in df.columns:\n",
    "        year_col = 'YYYY'\n",
    "    elif 'YY' in df.columns:\n",
    "        year_col = 'YY'\n",
    "    else:\n",
    "        raise ValueError(\"No year column found\")\n",
    "\n",
    "    # Create timezone-aware UTC timestamp\n",
    "    time_df = pd.DataFrame({\n",
    "        'year': df[year_col],\n",
    "        'month': df['MM'],\n",
    "        'day': df['DD'],\n",
    "        'hour': df['hh'],\n",
    "        'minute': df['mm']\n",
    "    })\n",
    "    df['time_utc'] = pd.to_datetime(time_df).dt.tz_localize('UTC')\n",
    "    \n",
    "    # Identify sensor columns\n",
    "    time_cols = {year_col, 'MM', 'DD', 'hh', 'mm', 'time_utc', 'station_id'}\n",
    "    sensor_cols = [col for col in df.columns if col not in time_cols]\n",
    "\n",
    "    # Coerce sensor columns to numeric (MM -> NaN)\n",
    "    for col in sensor_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Select final columns: station_id, time_utc, then all sensors\n",
    "    final_cols = ['station_id', 'time_utc'] + sensor_cols\n",
    "    df_staged = df[final_cols]\n",
    "    \n",
    "    return df_staged\n",
    "\n",
    "print(\"Staging function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43c25d57-b6eb-474e-b794-497c5ce71eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged shape: (6563, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>time_utc</th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>PTDY</th>\n",
       "      <th>TIDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 21:30:00+00:00</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.9</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 21:20:00+00:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1021.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 21:10:00+00:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.6</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 21:00:00+00:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:50:00+00:00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1021.2</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:40:00+00:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:30:00+00:00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:20:00+00:00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:10:00+00:00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1020.9</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:00:00+00:00</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1020.7</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id                  time_utc  WDIR  WSPD  GST  WVHT  DPD  APD   MWD  \\\n",
       "0      44013 2026-02-15 21:30:00+00:00  90.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "1      44013 2026-02-15 21:20:00+00:00  80.0   2.0  4.0   0.8  6.0  4.6  35.0   \n",
       "2      44013 2026-02-15 21:10:00+00:00  80.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "3      44013 2026-02-15 21:00:00+00:00  80.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "4      44013 2026-02-15 20:50:00+00:00  70.0   3.0  4.0   0.8  6.0  4.5  39.0   \n",
       "5      44013 2026-02-15 20:40:00+00:00  80.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "6      44013 2026-02-15 20:30:00+00:00  60.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "7      44013 2026-02-15 20:20:00+00:00  60.0   3.0  5.0   0.9  5.0  4.4  32.0   \n",
       "8      44013 2026-02-15 20:10:00+00:00  60.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "9      44013 2026-02-15 20:00:00+00:00  40.0   4.0  6.0   NaN  NaN  NaN   NaN   \n",
       "\n",
       "     PRES  ATMP  WTMP  DEWP  VIS  PTDY  TIDE  \n",
       "0  1021.9  -0.9   3.0 -10.8  NaN   NaN   NaN  \n",
       "1  1021.7  -0.9   3.0 -10.7  NaN   NaN   NaN  \n",
       "2  1021.6  -0.9   3.0 -10.7  NaN   NaN   NaN  \n",
       "3  1021.4  -1.0   3.0 -10.6  NaN   0.9   NaN  \n",
       "4  1021.2  -0.9   3.0 -10.3  NaN   NaN   NaN  \n",
       "5  1021.1  -1.0   3.0 -10.3  NaN   NaN   NaN  \n",
       "6  1021.0  -1.1   3.0 -10.6  NaN   NaN   NaN  \n",
       "7  1021.0  -1.2   3.0 -10.6  NaN   NaN   NaN  \n",
       "8  1020.9  -1.2   3.0 -10.4  NaN   NaN   NaN  \n",
       "9  1020.7  -1.2   3.0 -10.5  NaN   0.0   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_staged = stage_ndbc_data(df_raw, station_id = STATION_ID)\n",
    "print(f\"Staged shape: {df_staged.shape}\")\n",
    "display(df_staged.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dfaf57",
   "metadata": {},
   "source": [
    "✅ **Exercise 2.3 — write staged outputs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d149e1a-81e6-436b-a6b1-895aa8d74888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../work/HW2_NOAA/data/staged/ndbc_staged_44013_run_20260215_220552_utc.parquet\n",
      "Wrote: ../work/HW2_NOAA/data/staged/ndbc_staged_meta_44013_run_20260215_220552_utc.json\n"
     ]
    }
   ],
   "source": [
    "# Define output path and write data\n",
    "staged_data_path = STAGED_DIR / f\"ndbc_staged_{STATION_ID}_run_{RUN_ID}.parquet\"\n",
    "df_staged.to_parquet(staged_data_path, index=False)\n",
    "print(f\"Wrote: {staged_data_path}\")\n",
    "\n",
    "# Create staged metadata\n",
    "staged_metadata = {\n",
    "    \"station_id\": STATION_ID,\n",
    "    \"source_file\": raw_data_path.name,\n",
    "    \"staged_at\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"row_count\": len(df_staged),\n",
    "    \"column_count\": len(df_staged.columns),\n",
    "    \"time_range\": {\n",
    "        \"start\": df_staged['time_utc'].min().isoformat(),\n",
    "        \"end\": df_staged['time_utc'].max().isoformat(),\n",
    "        \"span_days\": (df_staged['time_utc'].max() - df_staged['time_utc'].min()).days\n",
    "    },\n",
    "    \"columns\": df_staged.columns.tolist(),\n",
    "    \"data_types\": {col: str(dtype) for col, dtype in df_staged.dtypes.items()},\n",
    "}\n",
    "\n",
    "# Write metadata\n",
    "staged_meta_path = STAGED_DIR / f\"ndbc_staged_meta_{STATION_ID}_run_{RUN_ID}.json\"\n",
    "write_json(staged_meta_path, staged_metadata)\n",
    "print(f\"Wrote: {staged_meta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5005051",
   "metadata": {},
   "source": [
    "## 3) Curate: analysis-ready features\n",
    "\n",
    "✅ **Exercise 3.1 — time features + flags**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19eff62e-4830-4638-8fb2-3ff7f82ba31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curate function ready\n"
     ]
    }
   ],
   "source": [
    "def curate_ndbc_data(df_staged: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_staged.copy()\n",
    "\n",
    "    # Time features\n",
    "    df['observation_day'] = df['time_utc'].dt.date\n",
    "    df['observation_hour'] = df['time_utc'].dt.hour\n",
    "    df['dayofweek'] = df['time_utc'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "\n",
    "    # Data Quality Flags\n",
    "    df['wind_high'] = (df['WSPD'] > 10.0).astype(int)\n",
    "    df['temp_gap_c'] = df['ATMP'] - df['WTMP']\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Curate function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "568e5b20-8aeb-4c6f-8572-f6b290468ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>time_utc</th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>PTDY</th>\n",
       "      <th>TIDE</th>\n",
       "      <th>observation_day</th>\n",
       "      <th>observation_hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>wind_high</th>\n",
       "      <th>temp_gap_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 21:30:00+00:00</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.9</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 21:20:00+00:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1021.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 21:10:00+00:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.6</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 21:00:00+00:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:50:00+00:00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1021.2</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:40:00+00:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:30:00+00:00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:20:00+00:00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:10:00+00:00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1020.9</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44013</td>\n",
       "      <td>2026-02-15 20:00:00+00:00</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1020.7</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id                  time_utc  WDIR  WSPD  GST  WVHT  DPD  APD   MWD  \\\n",
       "0      44013 2026-02-15 21:30:00+00:00  90.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "1      44013 2026-02-15 21:20:00+00:00  80.0   2.0  4.0   0.8  6.0  4.6  35.0   \n",
       "2      44013 2026-02-15 21:10:00+00:00  80.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "3      44013 2026-02-15 21:00:00+00:00  80.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "4      44013 2026-02-15 20:50:00+00:00  70.0   3.0  4.0   0.8  6.0  4.5  39.0   \n",
       "5      44013 2026-02-15 20:40:00+00:00  80.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "6      44013 2026-02-15 20:30:00+00:00  60.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "7      44013 2026-02-15 20:20:00+00:00  60.0   3.0  5.0   0.9  5.0  4.4  32.0   \n",
       "8      44013 2026-02-15 20:10:00+00:00  60.0   3.0  4.0   NaN  NaN  NaN   NaN   \n",
       "9      44013 2026-02-15 20:00:00+00:00  40.0   4.0  6.0   NaN  NaN  NaN   NaN   \n",
       "\n",
       "     PRES  ATMP  WTMP  DEWP  VIS  PTDY  TIDE observation_day  \\\n",
       "0  1021.9  -0.9   3.0 -10.8  NaN   NaN   NaN      2026-02-15   \n",
       "1  1021.7  -0.9   3.0 -10.7  NaN   NaN   NaN      2026-02-15   \n",
       "2  1021.6  -0.9   3.0 -10.7  NaN   NaN   NaN      2026-02-15   \n",
       "3  1021.4  -1.0   3.0 -10.6  NaN   0.9   NaN      2026-02-15   \n",
       "4  1021.2  -0.9   3.0 -10.3  NaN   NaN   NaN      2026-02-15   \n",
       "5  1021.1  -1.0   3.0 -10.3  NaN   NaN   NaN      2026-02-15   \n",
       "6  1021.0  -1.1   3.0 -10.6  NaN   NaN   NaN      2026-02-15   \n",
       "7  1021.0  -1.2   3.0 -10.6  NaN   NaN   NaN      2026-02-15   \n",
       "8  1020.9  -1.2   3.0 -10.4  NaN   NaN   NaN      2026-02-15   \n",
       "9  1020.7  -1.2   3.0 -10.5  NaN   0.0   NaN      2026-02-15   \n",
       "\n",
       "   observation_hour  dayofweek  is_weekend  wind_high  temp_gap_c  \n",
       "0                21          6           1          0        -3.9  \n",
       "1                21          6           1          0        -3.9  \n",
       "2                21          6           1          0        -3.9  \n",
       "3                21          6           1          0        -4.0  \n",
       "4                20          6           1          0        -3.9  \n",
       "5                20          6           1          0        -4.0  \n",
       "6                20          6           1          0        -4.1  \n",
       "7                20          6           1          0        -4.2  \n",
       "8                20          6           1          0        -4.2  \n",
       "9                20          6           1          0        -4.2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['station_id', 'time_utc', 'WDIR', 'WSPD', 'GST', 'WVHT', 'DPD', 'APD', 'MWD', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'VIS', 'PTDY', 'TIDE', 'observation_day', 'observation_hour', 'dayofweek', 'is_weekend', 'wind_high', 'temp_gap_c']\n"
     ]
    }
   ],
   "source": [
    "df_curated = curate_ndbc_data(df_staged)\n",
    "display(df_curated.head(10))\n",
    "print(df_curated.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87e005",
   "metadata": {},
   "source": [
    "✅ **Exercise 3.2 — choose curated columns and write Parquet**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d895fd66-9d67-4d20-af85-83d6b9597720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../work/HW2_NOAA/data/warehouse/ndbc_curated_44013.parquet | rows: 6563 | cols: 22\n",
      "Partitioned days: 46\n"
     ]
    }
   ],
   "source": [
    "curated_columns = [\n",
    "    # Identifiers\n",
    "    'station_id',\n",
    "    'time_utc',\n",
    "    \n",
    "    # Sensor measurements\n",
    "    'WDIR', \n",
    "    'WSPD', \n",
    "    'GST',\n",
    "    'WVHT', \n",
    "    'DPD', \n",
    "    'APD', \n",
    "    'MWD',\n",
    "    'PRES', \n",
    "    'ATMP', \n",
    "    'WTMP', \n",
    "    'DEWP',\n",
    "    'VIS', \n",
    "    'PTDY', \n",
    "    'TIDE',\n",
    "    \n",
    "    # Engineered time features\n",
    "    'observation_day',\n",
    "    'observation_hour',\n",
    "    'dayofweek',\n",
    "    'is_weekend',\n",
    "    \n",
    "    # Data quality flags\n",
    "    'wind_high',\n",
    "    'temp_gap_c'\n",
    "]\n",
    "\n",
    "df_final = df_curated[curated_columns]\n",
    "\n",
    "# Write to warehouse\n",
    "curated_data_path = WH_DIR / f\"ndbc_curated_{STATION_ID}.parquet\"\n",
    "df_final.to_parquet(curated_data_path, index=False)\n",
    "\n",
    "# Calculate statistics\n",
    "num_rows = len(df_final)\n",
    "num_cols = len(df_final.columns)\n",
    "num_days = df_final['observation_day'].nunique()\n",
    "\n",
    "print(f\"Wrote: {curated_data_path} | rows: {num_rows} | cols: {num_cols}\")\n",
    "print(f\"Partitioned days: {num_days}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0274f",
   "metadata": {},
   "source": [
    "## 4) Validate: contracts + anomalies + canaries\n",
    "\n",
    "✅ **Exercise 4.1 — required columns + plausible ranges**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f143725c-c8a6-426c-9bc5-1406a1a43dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required cols (contract): ['station_id', 'time_utc', 'WSPD', 'PRES', 'ATMP', 'WTMP']\n",
      "Optional cols (monitor): ['WVHT', 'GST', 'WDIR']\n",
      "Range checks will apply to: ['WSPD', 'GST', 'WVHT', 'PRES', 'ATMP', 'WTMP', 'WDIR']\n",
      "\n",
      "Note: WVHT exists as a column but is often missing for some stations. We'll monitor it, not fail the run.\n"
     ]
    }
   ],
   "source": [
    "# Required columns\n",
    "required_cols = [\n",
    "    'station_id',\n",
    "    'time_utc',\n",
    "    'WSPD',\n",
    "    'PRES',\n",
    "    'ATMP',\n",
    "    'WTMP'\n",
    "]\n",
    "\n",
    "# Optional columns\n",
    "optional_cols = [\n",
    "    'WVHT',\n",
    "    'GST',\n",
    "    'WDIR'\n",
    "]\n",
    "\n",
    "# Plausible ranges for validation\n",
    "range_checks = {\n",
    "    'WSPD': (0, 50),      \n",
    "    'GST': (0, 60),       \n",
    "    'WVHT': (0, 20),      \n",
    "    'PRES': (900, 1100),  \n",
    "    'ATMP': (-50, 50),    \n",
    "    'WTMP': (-5, 40),     \n",
    "    'WDIR': (0, 360)      \n",
    "}\n",
    "\n",
    "print(\"Required cols (contract):\", required_cols)\n",
    "print(\"Optional cols (monitor):\", optional_cols)\n",
    "print(\"Range checks will apply to:\", list(range_checks.keys()))\n",
    "print(\"\\nNote: WVHT exists as a column but is often missing for some stations. We'll monitor it, not fail the run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009f3d3",
   "metadata": {},
   "source": [
    "✅ **Exercise 4.2 — implement validation checks**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2690bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation function ready\n"
     ]
    }
   ],
   "source": [
    "def validate_curated_data(df: pd.DataFrame, required_cols: list, optional_cols: list, range_checks: dict) -> dict:\n",
    "\n",
    "    validation_report = {\n",
    "        \"validated_at\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"row_count\": len(df),\n",
    "        \"passed\": True,\n",
    "        \"failures\": [],\n",
    "        \"warnings\": []\n",
    "    }\n",
    "\n",
    "    # 1: Required columns must exist\n",
    "    missing_required = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_required:\n",
    "        validation_report['passed'] = False\n",
    "        validation_report['failures'].append({\n",
    "            'check': 'required_columns',\n",
    "            'issue': f\"Missing required columns: {missing_required}\"\n",
    "        })\n",
    "\n",
    "    # 2: Required columns must have some data (not all NaN)\n",
    "    for col in required_cols:\n",
    "        if col in df.columns:\n",
    "            missing_rate = df[col].isna().sum() / len(df)\n",
    "            if missing_rate >= 0.99:  # 99% or more missing = FAIL\n",
    "                validation_report['passed'] = False\n",
    "                validation_report['failures'].append({\n",
    "                    'check': f'missing_rate_required:{col}',\n",
    "                    'missing_rate': missing_rate\n",
    "                })\n",
    "\n",
    "    # 3: Range violations for all columns\n",
    "    for col, (min_val, max_val) in range_checks.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        valid_data = df[col].dropna()\n",
    "        if len(valid_data) == 0:\n",
    "            continue  \n",
    "        \n",
    "        out_of_range = ((valid_data < min_val) | (valid_data > max_val)).sum()\n",
    "        violation_rate = out_of_range / len(valid_data)\n",
    "        \n",
    "        # FAIL if >5% of values out of range\n",
    "        if violation_rate > 0.05:\n",
    "            validation_report['passed'] = False\n",
    "            validation_report['failures'].append({\n",
    "                'check': f'range_violation:{col}',\n",
    "                'out_of_range': int(out_of_range),\n",
    "                'total': len(valid_data),\n",
    "                'violation_rate': violation_rate,\n",
    "                'range': [min_val, max_val]\n",
    "            })\n",
    "\n",
    "    # 4: Optional column warnings (high missing rate)\n",
    "    for col in optional_cols:\n",
    "        if col in df.columns:\n",
    "            missing_rate = df[col].isna().sum() / len(df)\n",
    "            if missing_rate > 0.3:  # WARN if >30% missing\n",
    "                validation_report['warnings'].append({\n",
    "                    'check': f'missing_rate_optional:{col}',\n",
    "                    'missing_rate': missing_rate,\n",
    "                    'warn_if_gt': 0.3\n",
    "                })\n",
    "\n",
    "\n",
    "    return validation_report\n",
    "\n",
    "print(\"Validation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9eff94d9-ed14-4f0c-b157-358404fe50f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation passed? True\n",
      "Failures: 0 | Warnings: 1\n",
      "\n",
      "--- WARNINGS ---\n",
      "- missing_rate_optional:WVHT {'check': 'missing_rate_optional:WVHT', 'missing_rate': np.float64(0.5933262227639798), 'warn_if_gt': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# Run validation\n",
    "validation_report = validate_curated_data(df_curated, required_cols, optional_cols, range_checks)\n",
    "\n",
    "# Print results\n",
    "print(f\"Validation passed? {validation_report['passed']}\")\n",
    "print(f\"Failures: {len(validation_report['failures'])} | Warnings: {len(validation_report['warnings'])}\")\n",
    "\n",
    "if validation_report['failures']:\n",
    "    print(\"\\n--- FAILURES ---\")\n",
    "    for failure in validation_report['failures']:\n",
    "        print(f\"- {failure['check']} {failure}\")\n",
    "\n",
    "if validation_report['warnings']:\n",
    "    print(\"\\n--- WARNINGS ---\")\n",
    "    for warning in validation_report['warnings']:\n",
    "        print(f\"- {warning['check']} {warning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752fb8a",
   "metadata": {},
   "source": [
    "✅ **Exercise 4.3 — anomaly flags + investigation table**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b399c8b9-b011-4827-bfc6-3842d00317e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly summary function ready\n"
     ]
    }
   ],
   "source": [
    "def create_anomaly_summary(df: pd.DataFrame, range_checks: dict) -> dict:\n",
    "\n",
    "    summary = {\n",
    "        \"row_count\": len(df),\n",
    "        \"value_anomalies\": {},\n",
    "        \"missingness_rates\": {},\n",
    "        \"suspicious_row_count\": 0\n",
    "    }\n",
    "    \n",
    "    # --- Value anomalies (out of range) ---\n",
    "    value_anom_count = 0\n",
    "    for col, (min_val, max_val) in range_checks.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        valid_data = df[col].dropna()\n",
    "        if len(valid_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        out_of_range = ((valid_data < min_val) | (valid_data > max_val)).sum()\n",
    "        if out_of_range > 0:\n",
    "            flag_name = f'anom_{col.lower()}'\n",
    "            summary['value_anomalies'][flag_name] = int(out_of_range)\n",
    "            value_anom_count += out_of_range\n",
    "    \n",
    "    # --- Missingness rates ---\n",
    "    key_sensor_cols = ['WVHT', 'WTMP', 'ATMP', 'WSPD', 'PRES']\n",
    "    for col in key_sensor_cols:\n",
    "        if col in df.columns:\n",
    "            missing_rate = df[col].isna().mean()\n",
    "            if missing_rate > 0:\n",
    "                flag_name = f'miss_{col.lower()}'\n",
    "                summary['missingness_rates'][flag_name] = round(missing_rate, 5)\n",
    "    \n",
    "    # Suspicious row count (rows with ANY value anomaly)\n",
    "    summary['suspicious_row_count'] = value_anom_count\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"Anomaly summary function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9ce6353-d728-4e3d-8626-16c1c298fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Row-level anomaly summary ===\n",
      "Rows: 6563\n",
      "\n",
      "Value anomaly counts (top):\n",
      "- None triggered by current thresholds.\n",
      "\n",
      "Missingness rates (informational):\n",
      "- miss_wvht: 59.333%\n",
      "- miss_wtmp: 1.341%\n",
      "- miss_pres: 0.335%\n",
      "- miss_atmp: 0.229%\n",
      "- miss_wspd: 0.137%\n",
      "\n",
      "Suspicious rows (value anomalies only): 0\n"
     ]
    }
   ],
   "source": [
    "anomaly_summary = create_anomaly_summary(df_curated, range_checks)\n",
    "\n",
    "# Print formatted output\n",
    "print(\"=== Row-level anomaly summary ===\")\n",
    "print(f\"Rows: {anomaly_summary['row_count']}\")\n",
    "\n",
    "print(\"\\nValue anomaly counts (top):\")\n",
    "if anomaly_summary['value_anomalies']:\n",
    "    sorted_anoms = sorted(anomaly_summary['value_anomalies'].items(), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "    for flag, count in sorted_anoms[:5]:\n",
    "        print(f\"- {flag}: {count}\")\n",
    "else:\n",
    "    print(\"- None triggered by current thresholds.\")\n",
    "\n",
    "print(\"\\nMissingness rates (informational):\")\n",
    "if anomaly_summary['missingness_rates']:\n",
    "    sorted_miss = sorted(anomaly_summary['missingness_rates'].items(), \n",
    "                        key=lambda x: x[1], reverse=True)\n",
    "    for flag, rate in sorted_miss:\n",
    "        print(f\"- {flag}: {rate*100:.3f}%\")\n",
    "\n",
    "print(f\"\\nSuspicious rows (value anomalies only): {anomaly_summary['suspicious_row_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07391e99",
   "metadata": {},
   "source": [
    "✅ **Exercise 4.4 — canaries + spike/drop detection**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc348ce7-ad51-42ee-9b12-0986c43a5395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canary summary function ready\n"
     ]
    }
   ],
   "source": [
    "def create_canary_summary(df: pd.DataFrame) -> dict:\n",
    "\n",
    "    summary = {\n",
    "        \"day_count\": 0,\n",
    "        \"obs_per_day\": {},\n",
    "        \"drops\": [],\n",
    "        \"overall_missingness\": {},\n",
    "        \"worst_day_missingness\": {},\n",
    "        \"high_missingness_days\": {}\n",
    "    }\n",
    "    \n",
    "    # Group by observation_day\n",
    "    daily = df.groupby('observation_day')\n",
    "    obs_counts = daily.size()\n",
    "    \n",
    "    # Basic stats\n",
    "    summary['day_count'] = len(obs_counts)\n",
    "    summary['obs_per_day'] = {\n",
    "        'min': int(obs_counts.min()),\n",
    "        'median': float(obs_counts.median()),\n",
    "        'max': int(obs_counts.max())\n",
    "    }\n",
    "    \n",
    "    # Drop detection: days with < 50% of median observations \n",
    "    median_obs = obs_counts.median()\n",
    "    drop_threshold = median_obs * 0.5  \n",
    "    \n",
    "    drops = obs_counts[obs_counts < drop_threshold]\n",
    "    summary['drops'] = [\n",
    "        {'obs_day': day, 'n_obs': int(count)}\n",
    "        for day, count in drops.items()\n",
    "    ]\n",
    "\n",
    "    # Overall missingness rates \n",
    "    key_cols = ['WSPD', 'WVHT', 'PRES', 'ATMP', 'WTMP']\n",
    "    for col in key_cols:\n",
    "        if col in df.columns:\n",
    "            missing_rate = df[col].isna().mean()\n",
    "            summary['overall_missingness'][col] = round(missing_rate, 3)\n",
    "\n",
    "\n",
    "    # Worst day missingness per column\n",
    "    for col in key_cols:\n",
    "        if col in df.columns:\n",
    "            daily_missing = df.groupby('observation_day')[col].apply(lambda x: x.isna().mean())\n",
    "            worst_day = daily_missing.idxmax()\n",
    "            worst_rate = daily_missing.max()\n",
    "            \n",
    "            summary['worst_day_missingness'][col] = {\n",
    "                'rate': round(worst_rate, 3),\n",
    "                'day': worst_day\n",
    "            }\n",
    "\n",
    "    # Days with high missingness (>30%) \n",
    "    for col in key_cols:\n",
    "        if col in df.columns:\n",
    "            daily_missing = df.groupby('observation_day')[col].apply(lambda x: x.isna().mean())\n",
    "            high_miss_days = daily_missing[daily_missing > 0.30].sort_values(ascending=False)\n",
    "            \n",
    "            if len(high_miss_days) > 0:\n",
    "                summary['high_missingness_days'][col] = {\n",
    "                    'count': len(high_miss_days),\n",
    "                    'examples': [\n",
    "                        {'obs_day': day, 'missing_rate': float(rate)}\n",
    "                        for day, rate in high_miss_days.head(5).items()\n",
    "                    ]\n",
    "                }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"Canary summary function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c998a57d-b2a1-4f66-aeb6-8ba10d6dd96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Canary summary ===\n",
      "Days: 46\n",
      "Obs/day (min/median/max): 130 / 143.0 / 144\n",
      "\n",
      "Drops (unexpectedly few rows):\n",
      "- None detected\n",
      "\n",
      "Overall missingness (fraction of rows missing):\n",
      "- WSPD: 0.001\n",
      "- WVHT: 0.593\n",
      "- PRES: 0.003\n",
      "- ATMP: 0.002\n",
      "- WTMP: 0.013\n",
      "\n",
      "Worst day missingness per column:\n",
      "- WSPD: 0.014 on 2026-01-23\n",
      "- WVHT: 0.636 on 2026-01-05\n",
      "- PRES: 0.070 on 2026-02-07\n",
      "- ATMP: 0.014 on 2026-01-23\n",
      "- WTMP: 0.035 on 2026-01-05\n",
      "\n",
      "Days with missingness > 0.30:\n",
      "- WVHT: 46 days (showing up to 5)\n",
      "    {'obs_day': datetime.date(2026, 1, 5), 'missing_rate': 0.6363636363636364}\n",
      "    {'obs_day': datetime.date(2026, 2, 15), 'missing_rate': 0.6230769230769231}\n",
      "    {'obs_day': datetime.date(2026, 1, 13), 'missing_rate': 0.6223776223776224}\n",
      "    {'obs_day': datetime.date(2026, 1, 6), 'missing_rate': 0.6153846153846154}\n",
      "    {'obs_day': datetime.date(2026, 1, 16), 'missing_rate': 0.6153846153846154}\n"
     ]
    }
   ],
   "source": [
    "# Create canary summary\n",
    "canary_summary = create_canary_summary(df_curated)\n",
    "\n",
    "# Print formatted output\n",
    "print(\"=== Canary summary ===\")\n",
    "print(f\"Days: {canary_summary['day_count']}\")\n",
    "print(f\"Obs/day (min/median/max): {canary_summary['obs_per_day']['min']} / \"\n",
    "      f\"{canary_summary['obs_per_day']['median']} / {canary_summary['obs_per_day']['max']}\")\n",
    "\n",
    "print(\"\\nDrops (unexpectedly few rows):\")\n",
    "if canary_summary['drops']:\n",
    "    for drop in canary_summary['drops']:\n",
    "        print(f\"- {drop}\")\n",
    "else:\n",
    "    print(\"- None detected\")\n",
    "\n",
    "print(\"\\nOverall missingness (fraction of rows missing):\")\n",
    "for col, rate in canary_summary['overall_missingness'].items():\n",
    "    print(f\"- {col}: {rate:.3f}\")\n",
    "\n",
    "print(\"\\nWorst day missingness per column:\")\n",
    "for col, info in canary_summary['worst_day_missingness'].items():\n",
    "    print(f\"- {col}: {info['rate']:.3f} on {info['day']}\")\n",
    "\n",
    "print(\"\\nDays with missingness > 0.30:\")\n",
    "if canary_summary['high_missingness_days']:\n",
    "    for col, info in canary_summary['high_missingness_days'].items():\n",
    "        print(f\"- {col}: {info['count']} days (showing up to 5)\")\n",
    "        for example in info['examples']:\n",
    "            print(f\"    {example}\")\n",
    "else:\n",
    "    print(\"- None detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b686d",
   "metadata": {},
   "source": [
    "## 5) Leakage audit (conceptual)\n",
    "\n",
    "✅ **Exercise 5.1 — write a leakage checklist**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb9d6450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leakage checklist:\n",
      "1. If building rolling features, are they computed using only past data relative to prediction time?\n",
      "2. If you standardize/normalize sensors, are stats computed on TRAIN only?\n",
      "3. If you impute missing values, does the method avoid using future observations?\n",
      "4. Are you aggregating by day in a way that would be unavailable at prediction time?\n",
      "5. Is prediction time defined (e.g., predict next-hour wind speed using prior hours only)?\n"
     ]
    }
   ],
   "source": [
    "# Leakage Audit Checklist\n",
    "leakage_checklist = [\n",
    "    \"If building rolling features, are they computed using only past data relative to prediction time?\",\n",
    "    \"If you standardize/normalize sensors, are stats computed on TRAIN only?\",\n",
    "    \"If you impute missing values, does the method avoid using future observations?\",\n",
    "    \"Are you aggregating by day in a way that would be unavailable at prediction time?\",\n",
    "    \"Is prediction time defined (e.g., predict next-hour wind speed using prior hours only)?\"\n",
    "]\n",
    "\n",
    "print(\"Leakage checklist:\")\n",
    "for i, item in enumerate(leakage_checklist, 1):\n",
    "    print(f\"{i}. {item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2442744",
   "metadata": {},
   "source": [
    "## 6) Write validation report + run log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9f1c6ad-ea12-45e3-a208-679a0415f311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report function ready\n"
     ]
    }
   ],
   "source": [
    "def write_validation_report(\n",
    "    df: pd.DataFrame,\n",
    "    validation_report: dict,\n",
    "    anomaly_summary: dict,\n",
    "    canary_summary: dict,\n",
    "    leakage_checklist: list,\n",
    "    station_id: str\n",
    ") -> dict:\n",
    "\n",
    "    complete_report = {\n",
    "        \"station_id\": station_id,\n",
    "        \"validated_at\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"row_count\": len(df),\n",
    "        \"column_count\": len(df.columns),\n",
    "        \n",
    "        # Contracts (from validation_report)\n",
    "        \"contracts\": {\n",
    "            \"passed\": validation_report['passed'],\n",
    "            \"failures\": validation_report['failures'],\n",
    "            \"warnings\": validation_report['warnings']\n",
    "        },\n",
    "        \n",
    "        # Anomalies (from anomaly_summary)\n",
    "        \"anomalies\": {\n",
    "            \"value_anomalies\": anomaly_summary['value_anomalies'],\n",
    "            \"missingness_rates\": anomaly_summary['missingness_rates'],\n",
    "            \"suspicious_row_count\": anomaly_summary['suspicious_row_count']\n",
    "        },\n",
    "        \n",
    "        # Canaries (from canary_summary)\n",
    "        \"canaries\": {\n",
    "            \"day_count\": canary_summary['day_count'],\n",
    "            \"obs_per_day\": canary_summary['obs_per_day'],\n",
    "            \"drops\": canary_summary['drops'],\n",
    "            \"overall_missingness\": canary_summary['overall_missingness'],\n",
    "            \"worst_day_missingness\": canary_summary['worst_day_missingness'],\n",
    "            \"high_missingness_days\": canary_summary['high_missingness_days']\n",
    "        },\n",
    "        \n",
    "        # Leakage checklist\n",
    "        \"leakage_checklist\": leakage_checklist\n",
    "    }\n",
    "    \n",
    "    return complete_report\n",
    "\n",
    "print(\"Validation report function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0bf8f4c-29cd-4bec-ab18-882b513ba4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report written to: ../work/HW2_NOAA/data/reference/validation_report_run_20260215_220552_utc.json\n"
     ]
    }
   ],
   "source": [
    "# Create complete validation report\n",
    "validation_report_full = write_validation_report(\n",
    "    df_curated,\n",
    "    validation_report,\n",
    "    anomaly_summary,\n",
    "    canary_summary,\n",
    "    leakage_checklist,\n",
    "    STATION_ID\n",
    ")\n",
    "\n",
    "# Write to reference directory\n",
    "validation_path = REF_DIR / f\"validation_report_run_{RUN_ID}.json\"\n",
    "write_json(validation_path, validation_report_full)\n",
    "print(f\"Validation report written to: {validation_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268eae2b",
   "metadata": {},
   "source": [
    "✅ **Exercise 6.1 — run log**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1954c294-f46e-4234-b13a-eb67ea65d877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run log function ready\n"
     ]
    }
   ],
   "source": [
    "def create_run_log(\n",
    "    run_id: str,\n",
    "    raw_data_path: Path,\n",
    "    raw_meta_path: Path,\n",
    "    staged_data_path: Path,\n",
    "    curated_data_path: Path,\n",
    "    validation_path: Path,\n",
    "    query_params: dict = None\n",
    ") -> dict:\n",
    "\n",
    "    # Create query fingerprint (hash of query parameters)\n",
    "    if query_params:\n",
    "        query_str = json.dumps(query_params, sort_keys=True)\n",
    "        query_fingerprint = hashlib.md5(query_str.encode()).hexdigest()[:16]\n",
    "    else:\n",
    "        query_fingerprint = \"no_query\"\n",
    "    \n",
    "    run_log = {\n",
    "        \"run_id\": run_id,\n",
    "        \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \n",
    "        \"inputs\": {\n",
    "            \"query_fingerprint\": query_fingerprint,\n",
    "            \"raw_txt_path\": str(raw_data_path.absolute()),\n",
    "            \"raw_size_bytes\": raw_data_path.stat().st_size if raw_data_path.exists() else 0,\n",
    "            \"raw_meta_path\": str(raw_meta_path.absolute()) if raw_meta_path.exists() else None\n",
    "        },\n",
    "        \n",
    "        \"outputs\": {\n",
    "            \"staged_path\": str(staged_data_path.absolute()) if staged_data_path.exists() else None,\n",
    "            \"curated_path\": str(curated_data_path.absolute()),\n",
    "            \"partition_root\": str(WH_DIR / \"partitions\"),\n",
    "            \"validation_report_path\": str(validation_path.absolute())\n",
    "        },\n",
    "        \n",
    "        \"row_definition\": \"Each row is one buoy observation at time_utc for a given station_id.\",\n",
    "        \n",
    "        \"notes\": [\n",
    "            \"Sensor values may be missing due to instrument downtime or transmission errors.\",\n",
    "            \"Treat validations and anomaly flags as guardrails; investigate upstream before changing thresholds.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return run_log\n",
    "\n",
    "print(\"Run log function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3e748ca-fd7e-4218-8318-9185637b3fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../work/HW2_NOAA/data/reference/pipeline_runs/20260215_220552_utc.json\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'run_id': '20260215_220552_utc',\n",
       " 'generated_at_utc': '2026-02-15T22:07:06.697783+00:00',\n",
       " 'inputs': {'query_fingerprint': 'no_query',\n",
       "  'raw_txt_path': '/home/glake/Nextcloud/Classwork/CS6678 - Advanced Machine Learning/Homework/HW2/../work/HW2_NOAA/data/raw/nbdc_44013_run_20260215_220552_utc.txt',\n",
       "  'raw_size_bytes': 617110,\n",
       "  'raw_meta_path': None},\n",
       " 'outputs': {'staged_path': None,\n",
       "  'curated_path': '/home/glake/Nextcloud/Classwork/CS6678 - Advanced Machine Learning/Homework/HW2/../work/HW2_NOAA/data/warehouse/ndbc_curated_44013.parquet',\n",
       "  'partition_root': '../work/HW2_NOAA/data/warehouse/partitions',\n",
       "  'validation_report_path': '/home/glake/Nextcloud/Classwork/CS6678 - Advanced Machine Learning/Homework/HW2/../work/HW2_NOAA/data/reference/validation_report_run_20260215_220552_utc.json'},\n",
       " 'row_definition': 'Each row is one buoy observation at time_utc for a given station_id.',\n",
       " 'notes': ['Sensor values may be missing due to instrument downtime or transmission errors.',\n",
       "  'Treat validations and anomaly flags as guardrails; investigate upstream before changing thresholds.']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create run log\n",
    "run_log = create_run_log(\n",
    "    run_id=RUN_ID,\n",
    "    raw_data_path=raw_data_path,\n",
    "    raw_meta_path=RAW_DIR / f\"ndbc_meta_{STATION_ID}_{RUN_ID}.json\",  # Adjust if you created this earlier\n",
    "    staged_data_path=STAGED_DIR / f\"{STATION_ID}_staged.parquet\",\n",
    "    curated_data_path=curated_data_path,\n",
    "    validation_path=validation_path\n",
    ")\n",
    "\n",
    "# Save to pipeline_runs directory\n",
    "run_log_path = RUN_DIR / f\"{RUN_ID}.json\"\n",
    "write_json(run_log_path, run_log)\n",
    "\n",
    "print(f\"Saved: {run_log_path}\\n\")\n",
    "display(run_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917125f3",
   "metadata": {},
   "source": [
    "## 7) Self-check + reflection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0223f21-401e-4bf1-857c-1682916fce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SELF-CHECK AND REFLECTION\n",
      "================================================================================\n",
      "\n",
      "1. Row definition: Each row is one buoy observation at time_utc for station 44013, with sensors measuring wind, waves, pressure, and temperature.\n",
      "\n",
      "2. Required sensors: WSPD, PRES, ATMP, WTMP must exist and have <99% missing data to pass validation.\n",
      "\n",
      "3. Range checks: WSPD [0-50 m/s], PRES [900-1100 hPa], ATMP [-50 to 50°C], WTMP [-5 to 40°C], WVHT [0-20 m], GST [0-60 m/s], WDIR [0-360°].\n",
      "\n",
      "4. Biggest anomaly: WVHT has ~60% missing data across all days (59.8% overall), with 46/46 days exceeding 30% missingness threshold. This is expected for this buoy station and triggers warnings but not failures.\n",
      "\n",
      "5. Likely breakage scenario: If NDBC changes file format (e.g., removes units row, changes column names, or switches from space-delimited to comma-delimited), the read_ndbc_txt() function would fail. Check: Validate header structure before parsing.\n",
      "\n",
      "6. Another breakage scenario: If a sensor starts reporting values outside plausible ranges due to calibration drift or malfunction (e.g., ATMP = 100°C), range checks would catch it. The validation fails if >5% of values are out of range.\n",
      "\n",
      "7. Temporal leakage risk: If building time-series forecasting models, ensure rolling features use only past data, train/test splits are chronological, and no future observations leak into imputation or aggregation.\n",
      "\n",
      "8. Data quality insight: January 25, 2026 had only 35 observations (vs median 143/day), indicating a potential data collection or transmission issue on that date.\n",
      "\n",
      "9. Pipeline reproducibility: Run logs track inputs, outputs, and query fingerprints, allowing exact reconstruction of any pipeline execution from raw data to curated output.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "reflection = [\n",
    "    \"Row definition: Each row is one buoy observation at time_utc for station 44013, with sensors measuring wind, waves, pressure, and temperature.\",\n",
    "    \n",
    "    \"Required sensors: WSPD, PRES, ATMP, WTMP must exist and have <99% missing data to pass validation.\",\n",
    "    \n",
    "    \"Range checks: WSPD [0-50 m/s], PRES [900-1100 hPa], ATMP [-50 to 50°C], WTMP [-5 to 40°C], WVHT [0-20 m], GST [0-60 m/s], WDIR [0-360°].\",\n",
    "    \n",
    "    \"Biggest anomaly: WVHT has ~60% missing data across all days (59.8% overall), with 46/46 days exceeding 30% missingness threshold. This is expected for this buoy station and triggers warnings but not failures.\",\n",
    "    \n",
    "    \"Likely breakage scenario: If NDBC changes file format (e.g., removes units row, changes column names, or switches from space-delimited to comma-delimited), the read_ndbc_txt() function would fail. Check: Validate header structure before parsing.\",\n",
    "    \n",
    "    \"Another breakage scenario: If a sensor starts reporting values outside plausible ranges due to calibration drift or malfunction (e.g., ATMP = 100°C), range checks would catch it. The validation fails if >5% of values are out of range.\",\n",
    "    \n",
    "    \"Temporal leakage risk: If building time-series forecasting models, ensure rolling features use only past data, train/test splits are chronological, and no future observations leak into imputation or aggregation.\",\n",
    "    \n",
    "    \"Data quality insight: January 25, 2026 had only 35 observations (vs median 143/day), indicating a potential data collection or transmission issue on that date.\",\n",
    "    \n",
    "    \"Pipeline reproducibility: Run logs track inputs, outputs, and query fingerprints, allowing exact reconstruction of any pipeline execution from raw data to curated output.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SELF-CHECK AND REFLECTION\")\n",
    "print(\"=\"*80)\n",
    "for i, item in enumerate(reflection, 1):\n",
    "    print(f\"\\n{i}. {item}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e43d3-e42e-4c5f-9bf0-244807baf4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
